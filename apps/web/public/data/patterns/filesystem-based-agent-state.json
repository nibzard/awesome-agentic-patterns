{
  "title": "Filesystem-Based Agent State",
  "status": "established",
  "authors": ["Nikola Balic (@nibzard)"],
  "based_on": ["Anthropic Engineering Team"],
  "category": "Context & Memory",
  "source": "https://www.anthropic.com/engineering/code-execution-with-mcp",
  "tags": ["state-management", "persistence", "resumption", "long-running-tasks"],
  "slug": "filesystem-based-agent-state",
  "id": "filesystem-based-agent-state",
  "summary": "Agents persist intermediate results and working state to files, creating durable checkpoints that enable workflow resumption, recovery from failures, and support for long-running tasks.",
  "updated_at": "2026-01-05",
  "body": "\n## Problem\n\nMany agent workflows are long-running or may be interrupted (by errors, timeouts, or user intervention). Keeping all intermediate state in the model's context window is fragile and doesn't persist across sessions. When failures occur or when agents hit context limits, work is lost and must restart from scratch.\n\n## Solution\n\nAgents persist intermediate results and working state to files in the execution environment. This creates durable checkpoints that enable workflow resumption, recovery from failures, and support for tasks that exceed single-session context limits.\n\n**Core pattern:**\n\n```python\n# Agent writes intermediate state to files\ndef multi_step_workflow():\n    # Check if previous work exists\n    if os.path.exists(\"state/step1_results.json\"):\n        print(\"Resuming from step 1...\")\n        step1_data = json.load(open(\"state/step1_results.json\"))\n    else:\n        print(\"Starting from beginning...\")\n        step1_data = perform_step1()\n        with open(\"state/step1_results.json\", \"w\") as f:\n            json.dump(step1_data, f)\n\n    # Continue with step 2\n    if os.path.exists(\"state/step2_results.json\"):\n        print(\"Resuming from step 2...\")\n        step2_data = json.load(open(\"state/step2_results.json\"))\n    else:\n        step2_data = perform_step2(step1_data)\n        with open(\"state/step2_results.json\", \"w\") as f:\n            json.dump(step2_data, f)\n\n    # Final step\n    return perform_step3(step2_data)\n```\n\n**State organization:**\n\n```\nworkspace/\n├── state/\n│   ├── step1_results.json\n│   ├── step2_results.json\n│   └── progress.txt\n├── data/\n│   ├── input.csv\n│   └── processed.csv\n└── logs/\n    └── execution.log\n```\n\n## How to use it\n\n**Best for:**\n\n- Multi-step workflows with expensive operations (API calls, data processing)\n- Long-running tasks that may exceed session limits\n- Workflows that need recovery from transient failures\n- Collaborative tasks where multiple agents or sessions build on previous work\n- Batch processing jobs with checkpointing\n\n**Implementation patterns:**\n\n1. **Checkpoint after expensive operations:**\n\n   ```python\n   def process_large_dataset():\n       checkpoint_file = \"state/processed_rows.json\"\n\n       # Load progress if exists\n       if os.path.exists(checkpoint_file):\n           processed = json.load(open(checkpoint_file))\n           start_row = len(processed)\n       else:\n           processed = []\n           start_row = 0\n\n       # Process from checkpoint\n       for i, row in enumerate(data[start_row:]):\n           result = expensive_operation(row)\n           processed.append(result)\n\n           # Checkpoint every 100 rows\n           if (i + 1) % 100 == 0:\n               with open(checkpoint_file, \"w\") as f:\n                   json.dump(processed, f)\n\n       return processed\n   ```\n\n2. **State file with metadata:**\n\n   ```json\n   {\n     \"workflow_id\": \"abc-123\",\n     \"current_step\": \"data_processing\",\n     \"completed_steps\": [\"data_fetch\", \"validation\"],\n     \"last_update\": \"2024-01-15T10:30:00Z\",\n     \"data\": {\n       \"records_processed\": 1500,\n       \"errors_encountered\": 3\n     }\n   }\n   ```\n\n3. **Progress logging for visibility:**\n\n   ```python\n   def log_progress(step, status, details=None):\n       with open(\"logs/progress.log\", \"a\") as f:\n           timestamp = datetime.now().isoformat()\n           log_entry = f\"{timestamp} | {step} | {status}\"\n           if details:\n               log_entry += f\" | {json.dumps(details)}\"\n           f.write(log_entry + \"\\n\")\n           print(log_entry)  # Also show in agent context\n   ```\n\n## Trade-offs\n\n**Pros:**\n\n- Enables workflow resumption after interruption\n- Protects against data loss from transient failures\n- Supports long-running tasks beyond single-session limits\n- Allows inspection of intermediate results\n- Facilitates debugging (can examine state at each checkpoint)\n- Multiple agents can collaborate by reading/writing shared state\n\n**Cons:**\n\n- Agents must write checkpoint/recovery logic\n- File I/O adds overhead to workflow execution\n- Requires discipline around state naming and organization\n- Stale state files can cause confusion if not cleaned up\n- Concurrent access needs coordination (file locking, atomic writes)\n- Execution environment needs persistent storage\n\n**Operational considerations:**\n\n- Define state file cleanup policies (retention period, automatic cleanup)\n- Use atomic writes to prevent corruption (write to temp, then rename)\n- Include timestamps and version info in state files\n- Consider state file size limits (don't checkpoint massive datasets)\n- Secure state files if they contain sensitive data\n\n## References\n\n* Anthropic Engineering: Code Execution with MCP (2024)\n* Related: Episodic Memory pattern (for conversation-level persistence)\n"
}
