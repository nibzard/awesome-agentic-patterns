{
  "title": "Context-Minimization Pattern",
  "status": "emerging",
  "authors": ["Nikola Balic (@nibzard)"],
  "based_on": ["Luca Beurer-Kellner et al. (2025)"],
  "category": "Context & Memory",
  "source": "https://arxiv.org/abs/2506.08837",
  "tags": ["context-hygiene", "taint-removal", "prompt-injection"],
  "slug": "context-minimization-pattern",
  "id": "context-minimization-pattern",
  "summary": "User-supplied or tainted text lingers in the conversation, enabling it to influence later generations.",
  "updated_at": "2026-01-05",
  "body": "\n## Problem\nUser-supplied or tainted text lingers in the conversation, enabling it to influence later generations.\n\n## Solution\n**Purge or redact** untrusted segments once they've served their purpose:\n\n- After transforming input into a safe intermediate (query, structured object), strip the original prompt from context.  \n- Subsequent reasoning sees **only trusted data**, eliminating latent injections.\n\n```pseudo\nsql = LLM(\"to SQL\", user_prompt)\nremove(user_prompt)              # tainted tokens gone\nrows = db.query(sql)\nanswer = LLM(\"summarize rows\", rows)\n```\n\n## How to use it\n\nCustomer-service chat, medical Q&A, any multi-turn flow where initial text shouldn't steer later steps.\n\n## Trade-offs\n\n* **Pros:** Simple; no extra models needed; helps prevent [context window anxiety](context-window-anxiety-management.md) by reducing overall context usage.\n* **Cons:** Later turns lose conversational nuance; may hurt UX; overly aggressive minimization can remove useful context.\n\n## References\n\n* Beurer-Kellner et al., ยง3.1 (6) Context-Minimization.\n* [Building Companies with Claude Code](https://claude.com/blog/building-companies-with-claude-code) - Emphasizes importance of eliminating context contradictions: \"if there's any contradictions in your prompt, you're going to receive lower quality output\"\n"
}
