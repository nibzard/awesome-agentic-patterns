{
  "title": "Agent Reinforcement Fine-Tuning (Agent RFT)",
  "status": "emerging",
  "authors": [
    "Nikola Balic (@nibzard)"
  ],
  "based_on": [
    "Will Brown (OpenAI)",
    "Theo (OpenAI Solutions Architect)"
  ],
  "category": "Learning & Adaptation",
  "source": "https://youtu.be/1s_7RMG4O4U",
  "tags": [
    "reinforcement-learning",
    "fine-tuning",
    "tool-use",
    "multi-step-rl",
    "agent-training",
    "exploration"
  ],
  "slug": "agent-reinforcement-fine-tuning",
  "id": "agent-reinforcement-fine-tuning-agent-rft",
  "summary": "Train model weights end-to-end on agentic tasks via reinforcement learning with real tool calls and custom reward signals, optimizing for domain-specific tool use efficiency and multi-step reasoning performance.",
  "updated_at": "2026-01-05",
  "body": "\n## Problem\n\nAfter optimizing prompts and task design, agents may still underperform on your specific business tasks because:\n\n- **Domain shift**: Your tools and business context differ from what the base model was trained on\n- **Inefficient tool use**: Agents make too many tool calls or use wrong tools, leading to high latency\n- **Suboptimal reasoning**: The model doesn't reason well across your specific tool outputs\n- **Sample scarcity**: Some domains (e.g., new GPU hardware, specialized finance) lack training data\n\nTraditional fine-tuning approaches don't work well because they can't train the agent end-to-end on multi-step tool interactions with your environment.\n\n## Solution\n\n**Agent Reinforcement Fine-Tuning (Agent RFT)** trains the model weights end-to-end on agentic tasks by allowing the model to:\n\n1. **Explore via actual tool calls**: During training rollouts, the agent calls your real tool endpoints, learning from actual responses\n2. **Receive custom reward signals**: You define what \"good\" looks like via flexible graders (model-based, endpoint-based, or string-based)\n3. **Learn multi-step reasoning**: The agent learns to reason across tool outputs in the context window\n4. **Optimize for your metrics**: Reduce tool calls, improve accuracy, or balance both based on your reward function\n\n**Key Components:**\n\n- **Tool Endpoints**: Host your tools (same as production) that the model calls during training\n- **Grader Endpoint**: Define custom reward logic that evaluates final answers and/or tool call traces\n- **Unique Rollout IDs**: Each training rollout gets a unique ID for state management across tool calls\n- **Compute Multiplier**: Controls exploration breadth (higher = more rollouts per sample)\n\n```python\n# Agent RFT Training Setup\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n# 1. Define your tools with hosted endpoints\ntools = [\n    {\n        \"name\": \"search\",\n        \"url\": \"https://your-tools.modal.run/search\",\n        \"headers\": {\"Authorization\": \"Bearer YOUR_TOKEN\"}\n    },\n    {\n        \"name\": \"read_file\",\n        \"url\": \"https://your-tools.modal.run/read_file\",\n        \"headers\": {\"Authorization\": \"Bearer YOUR_TOKEN\"}\n    }\n]\n\n# 2. Define your grader (model-based or endpoint-based)\ngrader = {\n    \"type\": \"model\",  # or \"endpoint\" for custom grading logic\n    \"model\": \"gpt-4o\",\n    \"response_format\": {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"grader_response\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"score\": {\"type\": \"number\"},  # 0.0 to 1.0\n                    \"reasoning\": {\"type\": \"string\"}\n                }\n            }\n        }\n    },\n    \"prompt\": \"\"\"\n    Evaluate the agent's answer based on:\n    1. Correctness vs ground truth\n    2. Completeness of reasoning\n\n    Ground truth: {ground_truth}\n    Agent answer: {final_answer}\n\n    Provide score (0-1) and reasoning.\n    \"\"\"\n}\n\n# 3. Start fine-tuning job\njob = client.fine_tuning.jobs.create(\n    training_file=\"file-abc123\",\n    model=\"gpt-4o-2024-08-06\",\n    method=\"rft\",\n    rft={\n        \"tools\": tools,\n        \"grader\": grader,\n        \"hyperparameters\": {\n            \"n_epochs\": 3,\n            \"batch_size\": 16,\n            \"compute_multiplier\": 1  # Exploration factor\n        }\n    }\n)\n```\n\n## How to use it\n\n**Prerequisites:**\n\n- Well-specified, constrained task with consensus on correct answers\n- Non-zero baseline performance (model sometimes gets it right)\n- Quality training data (100-1000 samples, quality over quantity)\n- Hosted tool endpoints that mirror production behavior\n\n**Training Process:**\n\n1. **Baseline evaluation**: Run your base model multiple times per sample to measure variance\n2. **Host tools**: Deploy tool endpoints (FastAPI, Modal, etc.) that handle bursty traffic\n3. **Design grader**: Create reward function that's hard to game, provides gradient (not just binary)\n4. **Monitor training**: Watch reward curves, tool call distributions, and reasoning token counts\n5. **Evaluate results**: Compare fine-tuned model on validation set for accuracy and latency\n\n**What Agent RFT Optimizes:**\n\n- **ML Performance**: Better final answer quality through improved reasoning and tool use\n- **Latency**: Fewer tool calls and reasoning tokens (e.g., 50% reduction common)\n- **Sample Efficiency**: Can achieve strong results with as few as 100 quality samples\n\n```mermaid\ngraph TD\n    A[Training Sample] --> B[Model Generates Rollout]\n    B --> C{Tool Call?}\n    C -->|Yes| D[Call Your Tool Endpoint]\n    D --> E[Tool Response]\n    E --> F[Add to Context]\n    F --> B\n    C -->|No| G[Final Answer]\n    G --> H[Call Your Grader]\n    H --> I[Reward Signal]\n    I --> J[Update Model Weights]\n    J --> K[Next Sample]\n\n    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style J fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n```\n\n## Real-World Examples\n\n**Cognition (Devon AI)**\n\n- Task: File planning agent to identify which files to edit\n- Tools: `read_file`, `shell` (grep, find)\n- Results:\n  \n- Reduced planning time by 50% (8-10 tool calls → 4 tool calls)\n  - Learned to parallelize tool calls automatically\n  - Improved F1 score on file identification\n\n**Ambience Healthcare**\n\n- Task: ICD-10 medical coding from transcripts\n- Tools: Semantic search over 70K medical codes\n- Results:\n  \n- F1 score: 0.52 → 0.57 (significant given 0.75 human ceiling)\n  - 18% latency reduction\n  - 50% reduction in samples exceeding latency threshold\n\n**Rogo Finance**\n\n- Task: Financial reasoning and summarization from filings\n- Tools: Document retrieval, analysis tools\n- Results:\n  \n- 21% ML performance improvement\n  - Reduced hallucinations and missing citations\n  - Required hardening grader against reward hacking\n\n**Modular (Mojo GPU Kernels)**\n\n- Task: Write performant GPU kernels for new hardware\n- Tools: Compiler, kernel execution environment\n- Results:\n  \n- 72% improvement in correct + performant kernels\n  - Only 100 PyTorch prompts needed (sample efficient)\n  - No code examples required in training data\n\n## Trade-offs\n\n**Pros:**\n\n- **End-to-end optimization**: Trains the entire agent loop, not just final outputs\n- **Sample efficient**: Can work with 100-1000 samples vs millions for pre-training\n- **Flexible rewards**: Support for complex, multi-criteria grading logic\n- **Natural speedups**: Models learn to use fewer tokens and tool calls organically\n- **Domain adaptation**: Closes distribution gap between base model and your business context\n\n**Cons:**\n\n- **Infrastructure complexity**: Must host robust tool and grader endpoints\n- **Bursty traffic**: Training sends 100s of simultaneous requests at training step boundaries\n- **Grader design effort**: Requires careful reward engineering to avoid gaming\n- **Training cost**: More expensive than supervised fine-tuning due to exploration\n- **Debugging difficulty**: Hard to trace why model learned certain behaviors\n\n## References\n\n- [OpenAI Build Hour: Agent RFT (November 2025)](https://youtu.be/1s_7RMG4O4U)\n- [OpenAI Fine-tuning Guide](https://platform.openai.com/docs/guides/fine-tuning)\n- [Cognition Devon Case Study](https://www.cognition-labs.com/)\n- Related patterns: RLAIF, Tool Use Incentivization via Reward Shaping, Inference-Healed Code Review Reward\n"
}