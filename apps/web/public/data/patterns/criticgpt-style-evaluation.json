{
  "title": "CriticGPT-Style Code Review",
  "status": "validated-in-production",
  "authors": ["Nikola Balic (@nibzard)"],
  "based_on": ["OpenAI"],
  "category": "Reliability & Eval",
  "source": "https://openai.com/research/criticgpt",
  "tags": ["evaluation", "code-review", "critique", "quality-assurance", "bug-detection", "gpt-4"],
  "slug": "criticgpt-style-evaluation",
  "id": "criticgpt-style-code-review",
  "summary": "TODO: Add a concise summary for \"CriticGPT-Style Code Review\" describing the pattern's purpose and key benefits.",
  "updated_at": "2026-01-05",
  "body": "\n## Problem\n\nAs AI-generated code becomes more sophisticated, it becomes increasingly difficult for human reviewers to catch subtle bugs, security issues, or quality problems. Traditional code review processes may miss issues in AI-generated code because:\n\n- The volume of generated code can overwhelm human reviewers\n- Subtle bugs may appear correct at first glance\n- Security vulnerabilities may be non-obvious\n- Style and best practice violations may be inconsistent\n\n## Solution\n\nDeploy specialized AI models trained specifically for code critique and evaluation. These models act as automated code reviewers that can:\n\n1. **Identify bugs** that human reviewers might miss\n2. **Detect security vulnerabilities** in generated code\n3. **Suggest improvements** for code quality and efficiency\n4. **Verify correctness** of implemented solutions\n5. **Check adherence** to coding standards and best practices\n\nThe critic model works alongside code generation models, providing an additional layer of quality assurance before code reaches human review or production.\n\n## Example\n\n```python\nclass CriticGPTReviewer:\n    def __init__(self, critic_model, severity_threshold=0.7):\n        self.critic = critic_model\n        self.severity_threshold = severity_threshold\n        \n    def review_code(self, code, context=None, language=\"python\"):\n        \"\"\"Comprehensive code review using specialized critic model\"\"\"\n        \n        reviews = {\n            'bugs': self.check_for_bugs(code, context, language),\n            'security': self.security_audit(code, language),\n            'quality': self.quality_review(code, language),\n            'performance': self.performance_analysis(code, language),\n            'best_practices': self.best_practices_check(code, language)\n        }\n        \n        # Aggregate findings\n        all_issues = []\n        for category, findings in reviews.items():\n            for issue in findings:\n                issue['category'] = category\n                all_issues.append(issue)\n        \n        # Sort by severity\n        all_issues.sort(key=lambda x: x['severity'], reverse=True)\n        \n        return {\n            'issues': all_issues,\n            'summary': self.generate_summary(all_issues),\n            'recommended_action': self.recommend_action(all_issues)\n        }\n    \n    def check_for_bugs(self, code, context, language):\n        prompt = f\"\"\"\n        Review this {language} code for bugs:\n        \n        Context: {context or 'General purpose code'}\n        \n        Code:\n        ```{language}\n        {code}\n        ```\n        \n        Identify any bugs including:\n        \n- Logic errors\n        - Off-by-one errors  \n        - Null/undefined reference errors\n        - Type mismatches\n        - Resource leaks\n        - Race conditions\n        - Edge case handling\n        \n        For each bug found, provide:\n        1. Line number(s)\n        2. Description of the bug\n        3. Severity (0-1)\n        4. Suggested fix\n        \"\"\"\n        \n        response = self.critic.analyze(prompt)\n        return self.parse_bug_findings(response)\n    \n    def security_audit(self, code, language):\n        prompt = f\"\"\"\n        Perform security analysis on this {language} code:\n        \n        ```{language}\n        {code}\n        ```\n        \n        Check for vulnerabilities including:\n        \n- SQL injection\n        - XSS vulnerabilities\n        - Command injection\n        - Path traversal\n        - Insecure cryptography\n        - Hardcoded secrets\n        - Authentication/authorization issues\n        - Input validation problems\n        \n        Report format: [line, vulnerability type, severity, fix]\n        \"\"\"\n        \n        response = self.critic.analyze(prompt)\n        return self.parse_security_findings(response)\n    \n    def quality_review(self, code, language):\n        prompt = f\"\"\"\n        Review code quality for this {language} code:\n        \n        ```{language}\n        {code}\n        ```\n        \n        Evaluate:\n        \n- Code clarity and readability\n        - Function/variable naming\n        - Code organization\n        - Documentation completeness\n        - DRY principle violations\n        - Coupling and cohesion\n        - Error handling quality\n        \"\"\"\n        \n        response = self.critic.analyze(prompt)\n        return self.parse_quality_findings(response)\n    \n    def suggest_fixes(self, code, issue):\n        \"\"\"Generate specific fix for identified issue\"\"\"\n        prompt = f\"\"\"\n        Code with issue:\n        ```\n        {code}\n        ```\n        \n        Issue: {issue['description']}\n        Location: Line {issue['line']}\n        \n        Provide a corrected version of the relevant code section.\n        \"\"\"\n        \n        return self.critic.generate(prompt)\n\nclass IntegratedCodeGeneration:\n    \"\"\"Example of critic integration with code generation\"\"\"\n    \n    def __init__(self, generator, critic):\n        self.generator = generator\n        self.critic = CriticGPTReviewer(critic)\n        \n    def generate_and_review(self, task_description, max_iterations=3):\n        # Initial generation\n        code = self.generator.generate_code(task_description)\n        \n        for i in range(max_iterations):\n            # Review generated code\n            review = self.critic.review_code(\n                code, \n                context=task_description\n            )\n            \n            # If no critical issues, we're done\n            critical_issues = [\n                issue for issue in review['issues'] \n                if issue['severity'] > 0.8\n            ]\n            \n            if not critical_issues:\n                break\n                \n            # Otherwise, regenerate with feedback\n            feedback = self.format_feedback(critical_issues)\n            refinement_prompt = f\"\"\"\n            Original task: {task_description}\n            \n            Generated code has these issues:\n            {feedback}\n            \n            Generate improved code addressing these issues:\n            \"\"\"\n            \n            code = self.generator.generate_code(refinement_prompt)\n        \n        return {\n            'code': code,\n            'final_review': review,\n            'iterations': i + 1\n        }\n```\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Generator as Code Generator\n    participant Critic as CriticGPT\n    participant Human as Human Reviewer\n    \n    User->>Generator: Request code for task\n    Generator->>Generator: Generate initial code\n    Generator->>Critic: Submit code for review\n    \n    loop Until code passes or max iterations\n        Critic->>Critic: Analyze for bugs\n        Critic->>Critic: Security audit  \n        Critic->>Critic: Quality review\n        Critic->>Generator: Return issues found\n        \n        alt Critical issues found\n            Generator->>Generator: Refine code based on feedback\n            Generator->>Critic: Submit revised code\n        else No critical issues\n            Generator->>Human: Present code with review\n        end\n    end\n    \n    Human->>User: Approve/modify final code\n```\n\n## Benefits\n\n- **Catches More Bugs**: Specialized training helps identify subtle issues\n- **Consistent Reviews**: No fatigue or oversight like human reviewers\n- **Fast Feedback**: Near-instantaneous review of generated code\n- **Learning Tool**: Helps developers understand potential issues\n- **Reduces Security Risks**: Proactive vulnerability detection\n\n## Trade-offs\n\n**Pros:**\n- Scalable code review process\n- Consistent quality standards\n- Catches issues early in development\n- Can review code 24/7 without breaks\n- Improves over time with more training\n\n**Cons:**\n- May have false positives requiring human verification\n- Training specialized critic models is resource-intensive\n- Cannot understand full business context like humans\n- May miss novel vulnerability types\n- Requires integration into existing workflows\n\n## References\n\n- [OpenAI's CriticGPT Announcement (July 2024)](https://openai.com/research/criticgpt)\n- [Using LLMs for Code Review - Microsoft Research](https://www.microsoft.com/en-us/research/)\n- [Automated Code Review with AI - Google Research](https://research.google/)\n"
}
