{
  "title": "Workflow Evals with Mocked Tools",
  "status": "emerging",
  "authors": [
    "Nikola Balic (@nibzard)"
  ],
  "based_on": [
    "Will Larson (lethain.com)",
    "Sierra (chat/voice platform)"
  ],
  "category": "Reliability & Eval",
  "source": "https://lethain.com/agents-evals/",
  "tags": [
    "evals",
    "testing",
    "ci-cd",
    "mocked-tools",
    "simulations",
    "workflow-validation",
    "end-to-end-testing"
  ],
  "slug": "workflow-evals-with-mocked-tools",
  "id": "workflow-evals-with-mocked-tools",
  "summary": "TODO: Add a concise summary for \"Workflow Evals with Mocked Tools\" describing the pattern's purpose and key benefits.",
  "updated_at": "2026-01-13",
  "body": "\n## Problem\n\nUnit tests, linters, and typecheckers validate individual components but don't test agent workflows end-to-end. It's easy to create prompts that don't work well despite all underlying pieces being correct.\n\nYou need to validate that prompts and tools work together effectively as a system.\n\n## Solution\n\nImplement **workflow evals (simulations)** that test complete agent workflows with mocked tools.\n\n**Core components (Sierra-inspired approach):**\n\n1. **Dual tool implementations**: Every tool has both `true` and `mock` versions\n\n   ```python\n   # True implementation - calls real APIs\n   def search_knowledge_base_true(query: str) -> str:\n       return kb_api.search(query)\n\n   # Mock implementation - returns static/test data\n   def search_knowledge_base_mock(query: str) -> str:\n       return TEST_KB_RESULTS.get(query, DEFAULT_RESULT)\n   ```\n\n2. **Simulation configuration**: Each eval defines:\n   - **Initial prompt**: What the agent receives\n   - **Metadata**: Situation context available to harness\n   - **Evaluation criteria**: Success/failure determination\n\n   ```yaml\n   evals:\n     - name: slack_reaction_jira_workflow\n       initial_prompt: \"Add a smiley reaction to the JIRA ticket in this Slack message\"\n       metadata:\n         situation: \"slack_message_with_jira_link\"\n       expected_tools:\n         - slack_get_message\n         - jira_get_ticket\n         - slack_add_reaction\n       evaluation_criteria:\n         objective:\n           - tools_called: [\"slack_get_message\", \"jira_get_ticket\", \"slack_add_reaction\"]\n           - tools_not_called: [\"slack_send_message\"]\n         subjective:\n           - agent_judge: \"Response was helpful and accurate\"\n   ```\n\n3. **Dual evaluation criteria**:\n\n   **Objective criteria**:\n   - Which tools were called\n   - Which tools were NOT called\n   - Tags/states added to conversation (if applicable)\n\n   **Subjective criteria**:\n   - Agent-as-judge assessments (e.g., \"Was response friendly?\")\n   - LLM evaluations of qualitative outcomes\n\n4. **CI/CD integration**: Run evals automatically on every PR\n\n   ```pseudo\n   # GitHub Actions workflow\n   on: pull_request\n   steps:\n     - run: python scripts/run_agent_evals.py\n       # Posts results as PR comment\n   ```\n\n```pseudo\n# Eval execution flow\n1. Load eval configuration\n2. Swap in mock implementations for all tools\n3. Run agent with initial prompt + metadata\n4. Track which tools agent calls\n5. Evaluate against objective criteria (tool usage)\n6. Run agent-as-judge for subjective criteria\n7. Report pass/fail with details\n```\n\n## How to use it\n\n**Best for:**\n\n- Agent workflows where tools have side effects (APIs, databases)\n- CI/CD pipelines requiring workflow validation\n- Prompt engineering and optimization\n- Regression testing for agent behavior changes\n\n**Implementation approach:**\n\n**1. Create mock layer for tools:**\n\n```python\nclass MockToolRegistry:\n    def __init__(self, mode: str = \"mock\"):\n        self.mode = mode\n\n    def get_tool(self, tool_name: str):\n        if self.mode == \"mock\":\n            return self.mocks[tool_name]\n        return self.real_tools[tool_name]\n\n    # Register mock implementations\n    mocks = {\n        \"slack_send_message\": mock_slack_send_message,\n        \"jira_create_ticket\": mock_jira_create_ticket,\n        # ...\n    }\n```\n\n**2. Define eval cases:**\n\n```python\nevals = [\n    {\n        \"name\": \"login_support_flow\",\n        \"prompt\": \"User can't log in, help them\",\n        \"expected_tools\": [\"user_lookup\", \"password_reset\"],\n        \"forbidden_tools\": [\"account_delete\"],\n        \"subjective_criteria\": \"Response was empathetic and helpful\"\n    },\n    # ... more evals\n]\n```\n\n**3. Run and evaluate:**\n\n```python\ndef run_eval(eval_config):\n    # Run agent with mocked tools\n    result = agent.run(\n        prompt=eval_config[\"prompt\"],\n        tools=mock_registry\n    )\n\n    # Check objective criteria\n    tools_called = result.tools_used\n    passed = all(t in tools_called for t in eval_config[\"expected_tools\"])\n    passed &= all(t not in tools_called for t in eval_config[\"forbidden_tools\"])\n\n    # Check subjective criteria\n    if passed:\n        judge_prompt = f\"\"\"\n        Evaluate this agent response: {result.response}\n        Criteria: {eval_config['subjective_criteria']}\n        Pass/fail?\n        \"\"\"\n        passed = llm_evaluator(judge_prompt) == \"PASS\"\n\n    return {\"passed\": passed, \"details\": result}\n```\n\n**4. Integrate with CI/CD:**\n\n```yaml\n# .github/workflows/agent_evals.yml\nname: Agent Evals\non: pull_request\njobs:\n  evals:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - run: python scripts/run_evals.py --format github\n      - uses: actions/github-script@v6\n        with:\n          script: |\n            const results = require('./eval_results.json');\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: formatResults(results)\n            });\n```\n\n**Handling non-determinism:**\n\nThe article notes evals are \"not nearly as well as I hoped\" due to non-determinism:\n\n- **Strong signal**: All pass or all fail\n- **Weak signal**: Mixed results\n- **Mitigation**: Retry failed evals (e.g., \"at least once in three tries\")\n\n## Trade-offs\n\n**Pros:**\n\n- **End-to-end validation**: Tests prompts + tools together as a system\n- **Fast feedback**: Catch regressions before they reach production\n- **Safe testing**: Mocked tools avoid side effects during testing\n- **Clear criteria**: Both objective (tool calls) and subjective (quality) measures\n- **CI/CD integration**: Automated validation on every PR\n\n**Cons:**\n\n- **Non-deterministic**: LLM variability makes flaky tests common\n- **Mock maintenance**: Need to keep mocks synced with real tool behavior\n- **Prompt-driven fragility**: Prompt-dependent workflows (vs code-driven) more flaky\n- **Not blocking-ready**: Hard to use as CI gate due to variability\n- **Tuning overhead**: Need continuous adjustment of prompts and mock responses\n- **Limited signal**: Mixed pass/fail results provide ambiguous guidance\n\n**Operational challenges:**\n\n> \"This is working well, but not nearly as well as I had hoped... there's very strong signal when they all fail, and strong signal when they all pass, but most runs are in between.\"\n\n> \"Our reliance on prompt-driven workflows rather than code-driven workflows introduces a lot of non-determinism, which I don't have a way to solve without... prompt and mock tuning.\"\n\n**Improvement strategies:**\n\n1. **Retry logic**: \"At least once in three tries\" to reduce flakiness\n2. **Tune prompts**: Make eval prompts more precise and deterministic\n3. **Tune mocks**: Improve mock responses to be more realistic\n4. **Code over prompts**: Move complex workflows from prompt-driven to code-driven\n5. **Directional vs blocking**: Use for context rather than CI gates\n\n## References\n\n* [Building an internal agent: Evals to validate workflows](https://lethain.com/agents-evals/) - Will Larson (2025)\n* Sierra platform: Simulations approach for agent testing\n* Related: [Stop Hook Auto-Continue Pattern](stop-hook-auto-continue-pattern.md) - Post-execution testing\n* Related: [Agent Reinforcement Fine-Tuning](agent-reinforcement-fine-tuning.md) - Training on agent workflows\n"
}