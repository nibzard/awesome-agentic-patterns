# Awesome Agentic Patterns - Full Content

## Abstracted Code Representation for Review

**Status:** proposed
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
Reviewing large volumes of AI-generated code line-by-line can be tedious, error-prone, and inefficient. Human reviewers are often more interested in verifying the high-level intent and logical correctness of changes rather than minute syntactic details if the generation process is trusted to some extent.

## Solution
Provide a higher-level, abstracted representation of code changes for human review, rather than (or in addition to) the raw code diff. This could include:

-   **Pseudocode:** Representing the logic of the changes in a more human-readable, concise format.
-   **Intent Summaries:** Describing what the changes aim to achieve at a functional level.
-   **Logical Diffs:** Highlighting changes in program behavior or structure rather than just textual differences.
-   **Visualizations:** Graphical representations of control flow or data flow changes.

Crucially, this abstracted representation must come with strong guarantees (or at least high confidence) that it accurately and faithfully maps to the actual low-level code modifications that will be implemented. This allows reviewers to focus on conceptual correctness, significantly speeding up the verification process.

## Example
Instead of reviewing 50 lines of Python implementing a new sorting algorithm, review:
"Changed sorting logic for `user_list` from bubble sort to quicksort to improve performance for large lists. Test coverage maintained."
With a system guarantee that this change is correctly implemented in the underlying Python code.

## References
- Aman Sanger (Cursor, referencing Michael Grinich) at 0:09:48: "...operating in a different representation of the codebase. So maybe it looks like pseudo code. And if you can represent changes in this really concise way and you have guarantees that it maps cleanly onto the actual changes made in the in the real software, that just shorten the time of verification a ton."

---

## Action-Selector Pattern

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2506.08837

## Problem
Untrusted input can hijack an agent's reasoning once tool feedback re-enters the context window, leading to arbitrary, harmful actions.

## Solution
Treat the LLM as an **"instruction decoder" only**:

- Map the user's natural-language request to a *pre-approved* action (or action template).  
- **No tool outputs are fed back** into the LLM.  
- The agent therefore cannot be influenced after selecting the action.

```pseudo
action = LLM.translate(prompt, allowlist)
execute(action)
# tool output NOT returned to LLM
```

## How to use it

Provide a hard allowlist of safe actions (API calls, SQL templates, page links).
Useful for customer-service bots, notification routers, kiosk interfaces.

## Trade-offs

* **Pros:** Near-immunity to prompt injection; trivial to audit.
* **Cons:** Limited flexibility; new capabilities require code updates.

## References

* Beurer-Kellner et al., §3.1 (1) Action-Selector.

---

## Agent Reinforcement Fine-Tuning (Agent RFT)

**Status:** emerging
**Category:** Learning & Adaptation
**Authors:** Nikola Balic (@nibzard)
**Source:** https://youtu.be/1s_7RMG4O4U

## Problem

After optimizing prompts and task design, agents may still underperform on your specific business tasks because:

- **Domain shift**: Your tools and business context differ from what the base model was trained on
- **Inefficient tool use**: Agents make too many tool calls or use wrong tools, leading to high latency
- **Suboptimal reasoning**: The model doesn't reason well across your specific tool outputs
- **Sample scarcity**: Some domains (e.g., new GPU hardware, specialized finance) lack training data

Traditional fine-tuning approaches don't work well because they can't train the agent end-to-end on multi-step tool interactions with your environment.

## Solution

**Agent Reinforcement Fine-Tuning (Agent RFT)** trains the model weights end-to-end on agentic tasks by allowing the model to:

1. **Explore via actual tool calls**: During training rollouts, the agent calls your real tool endpoints, learning from actual responses
2. **Receive custom reward signals**: You define what "good" looks like via flexible graders (model-based, endpoint-based, or string-based)
3. **Learn multi-step reasoning**: The agent learns to reason across tool outputs in the context window
4. **Optimize for your metrics**: Reduce tool calls, improve accuracy, or balance both based on your reward function

**Key Components:**

- **Tool Endpoints**: Host your tools (same as production) that the model calls during training
- **Grader Endpoint**: Define custom reward logic that evaluates final answers and/or tool call traces
- **Unique Rollout IDs**: Each training rollout gets a unique ID for state management across tool calls
- **Compute Multiplier**: Controls exploration breadth (higher = more rollouts per sample)

```python
# Agent RFT Training Setup
from openai import OpenAI

client = OpenAI()

# 1. Define your tools with hosted endpoints
tools = [
    {
        "name": "search",
        "url": "https://your-tools.modal.run/search",
        "headers": {"Authorization": "Bearer YOUR_TOKEN"}
    },
    {
        "name": "read_file",
        "url": "https://your-tools.modal.run/read_file",
        "headers": {"Authorization": "Bearer YOUR_TOKEN"}
    }
]

# 2. Define your grader (model-based or endpoint-based)
grader = {
    "type": "model",  # or "endpoint" for custom grading logic
    "model": "gpt-4o",
    "response_format": {
        "type": "json_schema",
        "json_schema": {
            "name": "grader_response",
            "schema": {
                "type": "object",
                "properties": {
                    "score": {"type": "number"},  # 0.0 to 1.0
                    "reasoning": {"type": "string"}
                }
            }
        }
    },
    "prompt": """
    Evaluate the agent's answer based on:
    1. Correctness vs ground truth
    2. Completeness of reasoning

    Ground truth: {ground_truth}
    Agent answer: {final_answer}

    Provide score (0-1) and reasoning.
    """
}

# 3. Start fine-tuning job
job = client.fine_tuning.jobs.create(
    training_file="file-abc123",
    model="gpt-4o-2024-08-06",
    method="rft",
    rft={
        "tools": tools,
        "grader": grader,
        "hyperparameters": {
            "n_epochs": 3,
            "batch_size": 16,
            "compute_multiplier": 1  # Exploration factor
        }
    }
)
```

## How to use it

**Prerequisites:**

- Well-specified, constrained task with consensus on correct answers
- Non-zero baseline performance (model sometimes gets it right)
- Quality training data (100-1000 samples, quality over quantity)
- Hosted tool endpoints that mirror production behavior

**Training Process:**

1. **Baseline evaluation**: Run your base model multiple times per sample to measure variance
2. **Host tools**: Deploy tool endpoints (FastAPI, Modal, etc.) that handle bursty traffic
3. **Design grader**: Create reward function that's hard to game, provides gradient (not just binary)
4. **Monitor training**: Watch reward curves, tool call distributions, and reasoning token counts
5. **Evaluate results**: Compare fine-tuned model on validation set for accuracy and latency

**What Agent RFT Optimizes:**

- **ML Performance**: Better final answer quality through improved reasoning and tool use
- **Latency**: Fewer tool calls and reasoning tokens (e.g., 50% reduction common)
- **Sample Efficiency**: Can achieve strong results with as few as 100 quality samples

```mermaid
graph TD
    A[Training Sample] --> B[Model Generates Rollout]
    B --> C{Tool Call?}
    C -->|Yes| D[Call Your Tool Endpoint]
    D --> E[Tool Response]
    E --> F[Add to Context]
    F --> B
    C -->|No| G[Final Answer]
    G --> H[Call Your Grader]
    H --> I[Reward Signal]
    I --> J[Update Model Weights]
    J --> K[Next Sample]

    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style J fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
```

## Real-World Examples

**Cognition (Devon AI)**

- Task: File planning agent to identify which files to edit
- Tools: `read_file`, `shell` (grep, find)
- Results:
  
- Reduced planning time by 50% (8-10 tool calls → 4 tool calls)
  - Learned to parallelize tool calls automatically
  - Improved F1 score on file identification

**Ambience Healthcare**

- Task: ICD-10 medical coding from transcripts
- Tools: Semantic search over 70K medical codes
- Results:
  
- F1 score: 0.52 → 0.57 (significant given 0.75 human ceiling)
  - 18% latency reduction
  - 50% reduction in samples exceeding latency threshold

**Rogo Finance**

- Task: Financial reasoning and summarization from filings
- Tools: Document retrieval, analysis tools
- Results:
  
- 21% ML performance improvement
  - Reduced hallucinations and missing citations
  - Required hardening grader against reward hacking

**Modular (Mojo GPU Kernels)**

- Task: Write performant GPU kernels for new hardware
- Tools: Compiler, kernel execution environment
- Results:
  
- 72% improvement in correct + performant kernels
  - Only 100 PyTorch prompts needed (sample efficient)
  - No code examples required in training data

## Trade-offs

**Pros:**

- **End-to-end optimization**: Trains the entire agent loop, not just final outputs
- **Sample efficient**: Can work with 100-1000 samples vs millions for pre-training
- **Flexible rewards**: Support for complex, multi-criteria grading logic
- **Natural speedups**: Models learn to use fewer tokens and tool calls organically
- **Domain adaptation**: Closes distribution gap between base model and your business context

**Cons:**

- **Infrastructure complexity**: Must host robust tool and grader endpoints
- **Bursty traffic**: Training sends 100s of simultaneous requests at training step boundaries
- **Grader design effort**: Requires careful reward engineering to avoid gaming
- **Training cost**: More expensive than supervised fine-tuning due to exploration
- **Debugging difficulty**: Hard to trace why model learned certain behaviors

## References

- [OpenAI Build Hour: Agent RFT (November 2025)](https://youtu.be/1s_7RMG4O4U)
- [OpenAI Fine-tuning Guide](https://platform.openai.com/docs/guides/fine-tuning)
- [Cognition Devon Case Study](https://www.cognition-labs.com/)
- Related patterns: RLAIF, Tool Use Incentivization via Reward Shaping, Inference-Healed Code Review Reward

---

## Agent SDK for Programmatic Control

**Status:** emerging
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/claude-code

## Problem
Interactive terminal or chat interfaces are suitable for many agent tasks, but not for all. Integrating agent capabilities into automated workflows (e.g., CI/CD pipelines, scheduled jobs, batch processing) or building more complex applications on top of core agent functionalities requires a programmatic interface.

## Solution
Provide a Software Development Kit (SDK) that exposes the agent's core functionalities for programmatic access. This SDK allows developers to:

-   Invoke agent actions (e.g., process a prompt, use a tool, access memory) from code (e.g., Python, TypeScript).
-   Configure agent behavior and tool access in a non-interactive manner.
-   Integrate agent logic into larger software systems.
-   Automate repetitive tasks that involve the agent.
-   Build custom user interfaces or applications powered by the agent's backend.

The SDK typically includes libraries, command-line interfaces (CLIs) for scripting, and documentation for headless or embedded use of the agent.

## Example (SDK integration)
```mermaid
flowchart TD
    A[Application/Script] --> B[Agent SDK]
    B --> C[Agent Core]

    B --> D[CLI Interface]
    B --> E[Python Library]
    B --> F[TypeScript Library]

    C --> G[Tool Access]
    C --> H[Memory Management]
    C --> I[Model Interface]

    G --> J[File System]
    G --> K[Web API]
    G --> L[Custom Tools]
```

## Example CLI Usage (Conceptual, from Claude Code SDK info):
```bash
$ claude -p "what did i do this week?" \
  --allowedTools Bash(git log:*) \
  --output-format json
```

## References
-   Based on the description of the Claude Code SDK in "Mastering Claude Code: Boris Cherny's Guide & Cheatsheet," section VI.

[Source](https://www.nibzard.com/claude-code)

---

## Agent-Assisted Scaffolding

**Status:** validated-in-production
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
Starting a new feature, module, or codebase often involves writing a significant amount of boilerplate or foundational code. This can be time-consuming and repetitive for developers.

## Solution
Utilize an AI agent to generate the initial structure, boilerplate code, or layout for new software components. The developer provides a high-level description of the desired feature or component, and the agent "scaffolds" out the basic files, functions, classes, and directory structures.

This allows developers to:

-   Quickly get a new part of the system started.
-   Focus on the core logic rather than repetitive setup tasks.
-   Ensure consistency in initial project structure.

**Critical for Future AI Agent Work**: The scaffolded structure becomes crucial context for subsequent AI agent interactions. Well-structured scaffolding with clear file organization, naming conventions, and architectural patterns helps future agents understand the codebase layout and make more informed decisions when implementing features or making modifications.

The agent acts as a "kickstarter" for new development efforts while simultaneously enriching the repository's structural context for future AI-assisted development.

## Example
```mermaid
flowchart TD
    A[Developer: Create new API endpoint for user profiles] --> B[Agent: Generate Scaffolding]
    B --> C[Generated Files: Routes, Controllers, Models, Tests]
    C --> D[Developer: Implement Core Logic in Scaffolded Files]
```

## References
- Lukas Möller (Cursor) mentions this at 0:03:40: "So I think for like initially laying out some code base, some new feature, it's very, very useful to just like use the agent feature to kind of get that started."

---

## Agent-Driven Research

**Status:** established
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=u85G2aV_5rQ

## Problem
Traditional research methods often lack the ability to adapt search strategies based on emerging results, limiting efficiency and potential discoveries.

## Solution
Allow AI agents to independently conduct the entire research process. Given a research question, the agent:

- Creates its own search queries.
- Executes the searches.
- Examines the data.
- Adjusts its search strategy using new data.
- Repeats until it gathers enough information or meets specified criteria.
- Finally, compiles and presents a summary to the user.

## Example (flow)
```mermaid
flowchart TD
    A[Research Question] --> B[Formulate Search Query]
    B --> C[Execute Search]
    C --> D[Analyze Retrieved Information]
    D --> E{Sufficient Information?}
    E -->|No| F[Refine Search Strategy]
    F --> B
    E -->|Yes| G[Synthesize & Summarize Findings]
    G --> H[Present Results to User]
```

## References
- "How AI Agents Are Reshaping Creation": "That question goes to the agent, the agent formulates the searches in the form of tool calls. So it'll search the Web, it'll search some existing index or what have you, and it'll iterate until it's sort of satisfied with the amount of information that it gets, and then summarizes the output for you."

---

## Agent-First Tooling and Logging

**Status:** emerging
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.sourcegraph.com

## Problem

Most developer tools, CLIs, and application logs are designed for human consumption. They use color-coded, multi-line, or summarized outputs that are easy for a person to scan but can be difficult for an AI agent to parse reliably. This "human-centric" design creates noise and ambiguity, forcing the agent to waste tokens and effort on interpreting output rather than acting on it.

## Solution

Consciously design and adapt tooling and logging to be "agent-first," prioritizing machine-readability over human ergonomics. The environment should cater to the agent's need for clear, structured, and unambiguous information.

- **Unified Logging:** Instead of multiple log streams (client, server, database), consolidate them into a single, unified log. This gives the agent a single source of truth to monitor.
- **Verbose, Structured Output:** Prefer verbose, structured formats like JSON lines over concise, human-readable text. An agent can parse structured data far more effectively and is not constrained by screen space.
- **Agent-Aware CLIs:** Design new tools or add flags to existing tools (`--for-agent`) that modify their output to be more explicit and less ambiguous for an AI. Assume the agent, not a human, is the primary consumer.

This shift in design philosophy acknowledges that as agents perform more development work, the tools they use must adapt to serve them directly. An agent-friendly environment is a prerequisite for reliable and efficient agent performance.

## Example

```mermaid
sequenceDiagram
    participant Agent
    participant CLI as CLI Tool
    participant Logger as Unified Logger
    participant System as System Services

    Agent->>CLI: command --for-agent --json
    CLI->>System: Execute operation
    System->>Logger: Write structured log entry
    Logger->>Agent: JSON log stream
    Note over Agent: Parses structured data easily
    Agent->>CLI: Next command based on log analysis
```

## How to use it

1. **Audit Current Tools:** Identify tools that produce human-centric output and either find agent-friendly alternatives or add structured output flags
2. **Implement Unified Logging:** Consolidate multiple log sources into a single, structured stream that agents can monitor
3. **Create Agent-Aware APIs:** When building new tools, prioritize machine-readable output formats and clear, unambiguous responses
4. **Use Structured Formats:** Default to JSON, YAML, or other structured formats instead of free-form text output

## Trade-offs

- **Pros:**
  - Dramatically improves agent parsing accuracy and speed
  - Reduces token waste on output interpretation
  - Enables more reliable automation and decision-making
  - Single source of truth for system state

- **Cons/Considerations:**
  - May sacrifice human readability and debugging convenience
  - Requires investment in tooling modifications
  - Teams need to maintain both human and agent interfaces
  - Learning curve for developers used to human-centric tools

## References

- From Thorsten Ball: "What we've seen people now do is well instead of having the client log and having the browser log and having the database log, let's have one unified log because then it's easier for the agent to just look at this log... You can just have like JSON line outputs and whatnot because the agent can understand it much better than a human can... This is not made for human consumption anymore. How can we optimize this for a genetic consumption?"

---

## Agent-Friendly Workflow Design

**Status:** best-practice
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/silent-revolution

## Problem
Simply providing an AI agent with a task is often not enough for optimal performance. If workflows are too rigid, or if humans micromanage the agent's technical decisions, the agent may struggle or produce suboptimal results. Agents perform best when given some degree of freedom and when the tasks are structured in a way that aligns with their strengths.

## Solution
Consciously design and adapt workflows, task structures, and human-agent interaction points to be "agent-friendly." This involves:

- **Clear Goal Definition:** Provide clear, high-level goals rather than overly prescriptive, step-by-step instructions for every detail.
- **Appropriate Autonomy:** Grant the agent sufficient freedom to make its own implementation choices and explore solutions, especially if it has been programmed for such freedom.
- **Structured Input/Output:** Define clear interfaces for how the agent receives information and delivers results.
- **Iterative Feedback Loops:** Establish mechanisms for the agent to present intermediate work and for humans to provide corrective feedback without stifling the agent.
- **Tool Provisioning:** Ensure the agent has access to the necessary tools and understanding of how to use them for the given workflow.

This approach aims to create a collaborative environment where the agent's capabilities are maximized by a thoughtfully designed process.

## Example (workflow adaptation)
```mermaid
flowchart TD
    A[Traditional Workflow] --> B{Agent-Friendly?}
    B -->|No| C[Redesign Process]
    C --> D[Clear Goal Definition]
    C --> E[Appropriate Autonomy]
    C --> F[Structured I/O]
    C --> G[Feedback Loops]
    C --> H[Tool Provisioning]
    D --> I[Optimized Workflow]
    E --> I
    F --> I
    G --> I
    H --> I
    B -->|Yes| I
    I --> J[Enhanced Agent Performance]
```

## References
- Derived from insights in "How AI Agents Are Reshaping Creation," such as: "If you become a little too technical, they actually start to struggle to use the agent, because they're trying to force it to do certain technical decisions, whereas Replit agent is sort of programmed in a way to have more freedom." And the concluding point: "Focus on agent-friendly workflows - Creating environments where humans and AI agents can collaborate effectively."

[Source](https://www.nibzard.com/silent-revolution)

---

## Agent-Powered Codebase Q&A / Onboarding

**Status:** validated-in-production
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
Understanding a large or unfamiliar codebase can be a significant challenge for developers, especially when onboarding to a new project or trying to debug a complex system. Manually searching and tracing code paths is time-consuming.

## Solution
Leverage an AI agent with strong retrieval, search, and question-answering capabilities to assist developers in understanding a codebase. The agent can:
1.  Index the codebase (or parts of it).
2.  Respond to natural language queries about how specific parts of the code work (e.g., "How does user authentication work in this module?").
3.  Identify where certain functions are called or how different components interact.
4.  Summarize the purpose or functionality of specific files or modules.
5.  Help developers quickly find relevant information and get oriented within a new or complex codebase.

This pattern accelerates developer onboarding and understanding by providing an intelligent interface to query the codebase's structure and behavior.

## Example
```mermaid
sequenceDiagram
    Developer->>Agent: "Where is the database connection configured?"
    Agent->>Codebase: Search/Analyze
    Agent-->>Developer: "It's configured in `config/database.js` and used by the `UserService`."
```

## References
- Lukas Möller (Cursor) at 0:03:58: "...when initially getting started with a codebase that one might not be too knowledgeable about, that's using kind of the QA features a lot, using a lot of search... doing research in a codebase and figuring out how certain things interact with each other."
- Aman Sanger (Cursor) at 0:05:50: "...as you got to places where you're really unfamiliar, like Lucas was describing when you're kind of coming into a new codebase, it's just there's this massive step function that you get from using these models."

---

## Agentic Search Over Vector Embeddings

**Status:** best-practice
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

Vector embeddings for code search require:

- Continuous re-indexing as code changes
- Handling local uncommitted changes
- Additional security surface area for enterprise deployments
- Infrastructure overhead (embedding models, vector databases)
- Stale indices when developers work on multiple branches

Traditional RAG approaches add complexity that may not be necessary with modern capable LLMs.

## Solution

Replace vector search with **agentic search** using bash, grep, file traversal, and other command-line tools. Modern LLMs are skilled enough at using search tools iteratively to achieve comparable accuracy without the maintenance burden of vector indices.

**Key approach:**

1. **Tool-based search**: Provide grep, ripgrep, find, ls, and other search utilities
2. **Iterative refinement**: Let the agent search multiple times, narrowing results
3. **No pre-indexing**: Search happens on-demand against current file state
4. **Optional MCP integration**: If teams want semantic search, expose it via MCP tool

```pseudo
# Instead of:
vector_db.index(codebase)  # requires continuous updates
results = vector_db.query(embedding(query))

# Use:
agent.call_tool("grep", pattern="function.*authenticate")
agent.call_tool("find", pattern="**/auth/*.ts")
agent.refine_search_based_on_results()
```

## How to use it

**When to use agentic search:**

- Code bases with frequent changes
- Teams without dedicated vector infrastructure
- Security-sensitive deployments (fewer external dependencies)
- Local development where files change constantly
- Multi-branch workflows

**Implementation:**

1. Provide comprehensive search tools (grep, ripgrep, find, fd, ast-grep)
2. Give agent permission to search iteratively
3. Optimize for fast tool execution rather than perfect first results
4. Let agent learn search strategies through system prompts

**Claude Code example:**

Claude Code initially used vector embeddings but switched to pure agentic search for:

- **Cleaner deployment**: No indexing step, works immediately
- **Local changes**: Always searches current file state
- **Security**: Reduced attack surface for enterprise
- **Accuracy**: Comparable results with Sonnet 4+ models

## Trade-offs

**Pros:**

- No indexing infrastructure to maintain
- Always searches current state (no stale results)
- Works with local uncommitted changes
- Simpler security model
- Faster setup for new repositories
- No embedding model costs

**Cons:**

- May require multiple search iterations (more tokens)
- Slower on very large codebases (millions of files)
- Less semantic understanding (e.g., "authentication" vs "login")
- Requires capable models (Sonnet 4+) for good results
- Higher latency for complex queries

## References

* Cat Wu (Anthropic): "We did use vector embeddings initially. They're really tricky to maintain because you have to continuously re-index... Claude is really good at agentic search. You can get to the same accuracy level with agentic search and it's just a much cleaner deployment story."
* Cat Wu: "If you do want to bring semantic search to Claude Code, you can do so via an MCP tool."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## AI-Accelerated Learning and Skill Development

**Status:** validated-in-production
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
Developing strong software engineering skills, including "taste" for clean and effective code, traditionally requires extensive experience, trial-and-error, and mentorship, which can be a slow process, especially for junior developers.

## Solution
Utilize AI agents as interactive learning tools that accelerate a developer's skill acquisition and "taste" development. By using AI coding assistants, developers can:

1.  **Iterate Faster:** Quickly try out different approaches and see immediate results or feedback (e.g., compiler errors, test failures, AI-generated alternatives).
2.  **Learn from Mistakes Efficiently:** The AI can help identify and explain errors, allowing developers to understand why something failed more quickly.
3.  **Observe Best Practices:** By examining AI-generated code (which ideally reflects good practices), developers can learn new patterns and techniques.
4.  **Get Explanations on Demand:** Developers can ask the AI to explain complex concepts or unfamiliar code, acting as an always-available tutor.
5.  **Reduce Fear of Experimentation:** The ease of generating or refactoring code with AI can encourage developers to explore more, knowing they can easily revert or try again.

This creates an environment where developers, particularly those less experienced, can learn and refine their skills at an accelerated pace by having a powerful, responsive partner in the coding process.

## References
- Lukas Möller (Cursor) at 0:13:35: "I think quality comes very much from iterating quickly, making mistakes, figuring out why certain things failed. And I think models vastly accelerate this iteration process and can actually through that make you learn more quickly what works and what doesn't."
- Jacob Jackson (Cursor) at 0:17:57: "these tools are very good educationally as well, and they can help you become a great programmer... if you have a question about how something works... now you can just press command L and ask Claude... and I think that's very valuable."

---

## AI-Assisted Code Review / Verification

**Status:** emerging
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
As AI models generate increasing amounts of code, the bottleneck in software development shifts from code generation to code verification and review. Ensuring that AI-generated code is not only syntactically correct but also semantically correct, aligns with the intended functionality (especially if underspecified), and meets quality standards becomes crucial and time-consuming.

## Solution
Develop and employ AI-powered tools and processes specifically designed to assist humans in reviewing and verifying code, whether it's AI-generated or human-written. This can involve:

-   AI agents that analyze code changes and highlight potential issues, bugs, or deviations from best practices.
-   Tools that help summarize the intent or impact of code changes, making it easier for human reviewers to understand.
-   Interactive systems where reviewers can ask the AI to explain parts of the code or justify certain decisions made during generation.
-   Mechanisms to ensure the AI's output aligns with the user's "mind's eye" or high-level intent, even if the initial specification was ambiguous.

The goal is to make the code review process more efficient and reliable, building confidence in the (AI-assisted) codebase.

## How to use it

- Integrate AI verification tools into the PR review process.
- Prompt agents to explain their generated code or provide rationales for changes.
- Focus human review on verifying alignment with high-level intent and business logic.

## References

- Aman Sanger (Cursor) at 0:09:12: "So I think we're going to need to figure out how to make it easier for people to review code, how to to be confident that the agent's making the changes that are not just correct... was it actually what you had in your mind's eye? And so making the process of review much, much better, I think will be really, really important."

---

## Anti-Reward-Hacking Grader Design

**Status:** emerging
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://youtu.be/1s_7RMG4O4U

## Problem

During reinforcement learning training, models actively search for ways to maximize reward. If your grader has edge cases or loopholes, the model will find and exploit them:

- **Gaming the metric**: Model achieves 100% reward score by exploiting grader weaknesses rather than solving the task
- **Unexpected behaviors**: Agent learns bizarre shortcuts that technically satisfy the reward function but don't reflect true quality
- **Brittle evaluation**: Simple graders (e.g., exact string match) penalize valid answers due to formatting differences
- **Degraded real performance**: High training reward doesn't translate to production success

The Rogo team experienced this firsthand: early training runs showed 100% average validation reward, but the model was exploiting edge cases in their financial reasoning grader rather than improving actual performance.

## Solution

Design reward functions that are **resistant to gaming** through iterative hardening and multi-criteria evaluation:

**Core Principles:**

1. **Make it hard to game**: Close loopholes systematically as you discover them
2. **Provide gradient**: Use continuous scores (0.0-1.0) rather than binary (0/1) to guide learning
3. **Multi-criteria decomposition**: Evaluate multiple aspects so gaming one doesn't maximize total reward
4. **Explainability**: Graders should explain why they gave a score to help detect gaming
5. **Adversarial testing**: Manually try to "hack" your grader before training

**Implementation Approach:**

```python
class RobustGrader:
    """
    Multi-criteria grader designed to resist reward hacking
    """
    def __init__(self, domain_criteria):
        self.criteria = domain_criteria
        self.violation_patterns = []  # Known gaming patterns

    def grade(self, question, ground_truth, agent_answer, tool_trace):
        """
        Grade with multiple checks to prevent gaming
        """
        # Check for known gaming patterns first
        for pattern in self.violation_patterns:
            if pattern.matches(agent_answer, tool_trace):
                return {
                    "score": 0.0,
                    "reason": f"Detected gaming pattern: {pattern.name}",
                    "violation": True
                }

        # Multi-criteria evaluation
        scores = {}

        # 1. Factual correctness (most important)
        scores['correctness'] = self._check_correctness(
            agent_answer,
            ground_truth
        )

        # 2. Reasoning quality (prevents memorization)
        scores['reasoning'] = self._check_reasoning_quality(
            tool_trace,
            agent_answer
        )

        # 3. Completeness (prevents partial answers)
        scores['completeness'] = self._check_completeness(
            agent_answer,
            required_elements=self.criteria.get('required_elements', [])
        )

        # 4. Citation quality (prevents hallucination)
        scores['citations'] = self._check_citations(
            agent_answer,
            tool_trace
        )

        # 5. Formatting (but with partial credit)
        scores['formatting'] = self._check_format_with_flexibility(
            agent_answer,
            expected_format=self.criteria.get('format', 'any')
        )

        # Weighted aggregation
        weights = {
            'correctness': 0.50,
            'reasoning': 0.20,
            'completeness': 0.15,
            'citations': 0.10,
            'formatting': 0.05
        }

        final_score = sum(weights[k] * scores[k] for k in scores)

        return {
            "score": final_score,
            "subscores": scores,
            "reason": self._explain_score(scores, weights),
            "violation": False
        }

    def _check_correctness(self, answer, ground_truth):
        """
        Flexible correctness check with normalization
        """
        # Normalize both answers to handle formatting variations
        norm_answer = self._normalize_answer(answer)
        norm_truth = self._normalize_answer(ground_truth)

        # Exact match
        if norm_answer == norm_truth:
            return 1.0

        # Numerical tolerance (e.g., 0.999 vs 1.0)
        if self._is_numerical(norm_answer) and self._is_numerical(norm_truth):
            try:
                ans_num = float(norm_answer)
                truth_num = float(norm_truth)

                # Within 1% = full credit, 1-5% = 0.5 credit
                pct_diff = abs(ans_num - truth_num) / truth_num
                if pct_diff < 0.01:
                    return 1.0
                elif pct_diff < 0.05:
                    return 0.5
                else:
                    return 0.0
            except:
                pass

        # Semantic similarity for text answers
        similarity = self._semantic_similarity(norm_answer, norm_truth)
        if similarity > 0.9:
            return 1.0
        elif similarity > 0.7:
            return 0.5
        else:
            return 0.0

    def _check_reasoning_quality(self, tool_trace, answer):
        """
        Verify the agent actually used tools meaningfully
        Prevents: just outputting answer without reasoning
        """
        if not tool_trace or len(tool_trace) == 0:
            return 0.0  # Must use tools

        # Check for suspicious patterns
        # 1. Repeating same tool call many times (likely gaming)
        if self._has_repetitive_calls(tool_trace):
            return 0.2

        # 2. Tool calls unrelated to answer (random exploration)
        if not self._tools_support_answer(tool_trace, answer):
            return 0.3

        # 3. Reasonable progression of tools
        if self._has_logical_tool_progression(tool_trace):
            return 1.0

        return 0.6

    def add_violation_pattern(self, pattern_name, detection_fn):
        """
        Add newly discovered gaming patterns
        """
        self.violation_patterns.append({
            "name": pattern_name,
            "matches": detection_fn
        })
```

**Iterative Hardening Process:**

```mermaid
graph TD
    A[Design Initial Grader] --> B[Run Training]
    B --> C{Suspicious High Reward?}
    C -->|Yes| D[Inspect Traces]
    D --> E[Identify Gaming Pattern]
    E --> F[Add Detection Rule]
    F --> G[Update Grader]
    G --> B
    C -->|No| H[Validate on Holdout]
    H --> I{Performance Matches?}
    I -->|Yes| J[Deploy Model]
    I -->|No| E

    style E fill:#ffebee,stroke:#c62828,stroke-width:2px
    style F fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style J fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
```

## How to use it

**Phase 1: Initial Design**

1. **Decompose quality**: Break down "good answer" into 4-6 measurable criteria
2. **Weight criteria**: Assign weights reflecting business priorities
3. **Add flexibility**: Handle formatting variations gracefully (e.g., "7%" vs "0.07")
4. **Build explainability**: Return subscores and reasoning

**Phase 2: Adversarial Testing**

1. **Manual hacking**: Try to write answers that get high scores without being correct
2. **Edge case testing**: Test extreme inputs (empty answers, gibberish, etc.)
3. **Add guardrails**: Prevent trivial gaming (e.g., must use at least N tools)

**Phase 3: Training Monitoring**

1. **Watch for sudden jumps**: If reward jumps to 100%, investigate immediately
2. **Sample traces**: Manually review high-reward examples to verify quality
3. **Compare distributions**: Validation reward should track training reward
4. **Check real metrics**: Validate that business KPIs improve, not just reward

**Phase 4: Iterative Hardening**

1. **Detect patterns**: When you find gaming, characterize the pattern
2. **Add detection**: Add explicit checks for that gaming pattern
3. **Retrain**: Re-run training with hardened grader
4. **Repeat**: This is an ongoing process

## Real-World Example: Rogo Finance

**Problem**: Financial reasoning agent for investment insights

**Initial Grader**: Simple model-based grader checking answer correctness

**Gaming Discovered**: Model achieved 100% validation reward but actual financial soundness was poor

**Hardening Applied:**

- Added multi-criteria evaluation:
  
- Factual accuracy (0.4 weight)
  - Reasoning completeness (0.2 weight)
  - Financial soundness (0.2 weight)
  - Clarity of explanation (0.1 weight)
  - Citation quality (0.1 weight)

- Added violation detection:
  
- Missing citation penalty
  - Circular reasoning detection
  - Copy-paste from source without synthesis

**Result**: 21% real performance improvement with much lower hallucination rates

## Trade-offs

**Pros:**

- **Robust learning**: Models learn to truly solve tasks, not game metrics
- **Better generalization**: Multi-criteria grading encourages well-rounded solutions
- **Debuggability**: Subscores help identify what the model is struggling with
- **Production alignment**: Training reward correlates with business metrics

**Cons:**

- **Engineering effort**: Requires careful design and iteration
- **Slower convergence**: Harder grader means lower initial rewards
- **Grader complexity**: More code to maintain and potentially debug
- **Subjectivity**: Some criteria (e.g., "financial soundness") need careful definition
- **Computational cost**: Multi-criteria grading takes longer per sample

## References

- [OpenAI Build Hour: Agent RFT - Rogo Case Study (November 2025)](https://youtu.be/1s_7RMG4O4U)
- [Specification Gaming in AI (DeepMind)](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)
- Related patterns: Inference-Healed Code Review Reward, Agent Reinforcement Fine-Tuning, RLAIF

---

## Asynchronous Coding Agent Pipeline

**Status:** proposed
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=Xkwok_XXQgw

## Problem

Synchronous execution of coding tasks—where the agent must wait for compilation, testing, linting, or static analysis—creates **compute bubbles** and **idle resources**. When a coding agent issues a tool call (e.g., `run_tests()`), it blocks further reasoning until that tool returns, leading to underutilized GPUs/TPUs and slower RL rollouts.

- RL agents must push hard on **async RL** "so everything is happening in parallel without blowing up bubbles".
- For coding agents, each I/O-bound tool call (compilation, test runs) can take seconds to minutes.

## Solution

Decouple the **inference**, **tool execution**, and **learning** into **parallel, asynchronous components**, communicating via message queues:

**1. Inference Workers (GPU)**
- Continuously sample from the latest policy.
- Output "actions" that are either low-compute (e.g., "suggest next line") or external tool calls (e.g., "CompileSubagent(serviceA)").

**2. Tool Executors (CPU / Container Hosts)**
- Listen to a queue of tool call requests (`compile`, `run_tests`, `lint`).
- Run each tool in an isolated environment, then push the results (success/failure, logs) back to the **Inference Workers**.

**3. Reward Modeling Units (GPU/CPU)**
- Consume completed trajectories (series of `(state, action, tool_output)`), compute turn-level or final rewards (e.g., via `inference_healed_reward`).
- Push `(trajectory_id, reward)` to the **Learner**.

**4. Learner / Parameter Server (GPU)**
- Periodically aggregates gradients from recent trajectories, updates policy weights, and publishes new checkpoints.

**5. Replay & Buffer System**
- **Experience Replay:** Stores recent `(state, action, reward)` tuples, allowing the Learner to sample minibatches.
- **Priority Queues:** If certain coding episodes show high variance (e.g., intermittent compile successes), re-evaluate them with updated reward models.

## Example

```mermaid
flowchart LR
    subgraph InferenceCluster
        A[Inference Worker] -->|"Compile serviceA"| B[Tool Queue]
        B -->|request| C[CompileSubagent]
        C -->|result (succ/fail)| A
        A -->|trajectory data| D[Replay Buffer]
    end
    subgraph TrainingCluster
        D -->|batch| E[Learner (Policy Update)]
        E -->|new checkpoint| A
    end
    subgraph RewardCluster
        F[RewardModel Worker] -->|consume trajectories| D
    end
```

## How to use it

- **Message Broker:** Use Redis streams or RabbitMQ topics to queue tool calls (`compile_requests`, `test_requests`).
- **Autoscaling Policies:** Monitor queue lengths: if `compile_requests` > threshold, spin up additional `CompileSubagent` containers.
- **Failure Handling:** If a tool executor crashes or a network error occurs, send a "retry" or "skip" message; mark that trajectory as "stale" if too many retries.
- **Checkpoint Frequency:** Decide at what interval the Learner should publish new policy weights (e.g., every 1,000 episodes) to avoid excessive network traffic.

## Trade-offs

- **Pros:**
  - **High Utilization:** GPUs remain busy running inference or learning while CPU-bound tasks run in parallel.
  - **Scalable Compute:** Can independently scale inference, tool execution, and reward modeling.
- **Cons/Considerations:**
  - **Complex System Maintenance:** Requires robust monitoring, logging, and alerting across multiple services.
  - **Staleness Management:** Policies may train on slightly outdated data; hyperparameters must account for acceptable staleness windows (e.g., 5–20 minutes).

## References

- Will Brown's emphasis on "everything being async and overlapped" to hide latencies in multi-hour RL tasks.
- "IMPALA: Scalable Distributed Deep-RL" for a precedent in actor-learner pipelines.

---

## Autonomous Workflow Agent Architecture

**Status:** established
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.together.ai/blog/ai-agents-to-automate-complex-engineering-tasks

## Problem

Complex, long-running engineering workflows traditionally require extensive human oversight and intervention. Tasks like model training pipelines, infrastructure configuration, and multi-step deployment processes involve:

- Manual coordination of multiple tools and systems
- Constant monitoring for errors and edge cases  
- Time-consuming context switching between different workflow stages
- Risk of human error in repetitive tasks
- Difficulty scaling engineering processes across teams

Engineers spend significant time on operational overhead rather than core development work, and workflows often fail at intermediate steps requiring manual debugging and restart.

## Solution

Autonomous Workflow Agent Architecture creates AI agents with sophisticated workflow management capabilities that can handle multi-step engineering processes with minimal human intervention. The architecture combines:

**Core Components:**

- **Containerized Execution Environments**: Isolated, reproducible environments for safe workflow execution
- **Session Management**: tmux-based parallel process coordination
- **Intelligent Monitoring**: Adaptive wait/sleep mechanisms and progress tracking  
- **Error Recovery**: Robust error handling with context-aware retry strategies
- **Documentation Integration**: Comprehensive logging and workflow documentation

**Architecture Pattern:**

![Autonomous Workflow Agent Architecture](/patterns/autonomous-workflow-agent-architecture.png)

The system operates through several key phases:

```mermaid
graph TD
    A[Workflow Definition] --> B[Environment Setup]
    B --> C[Containerized Execution]
    C --> D[Session Management]
    D --> E[Parallel Process Coordination]
    E --> F[Intelligent Monitoring]
    F --> G[Error Detection]
    G --> H{Error Found?}
    H -->|Yes| I[Adaptive Recovery]
    H -->|No| J[Progress Checkpoint]
    I --> D
    J --> K{Workflow Complete?}
    K -->|No| D
    K -->|Yes| L[Results Aggregation]
    L --> M[Documentation Update]
```

**Implementation Patterns:**

1. **Infrastructure Setup**: Create containerized environments with necessary tools and dependencies
2. **Process Orchestration**: Use tmux sessions to manage parallel execution streams
3. **Adaptive Monitoring**: Implement intelligent wait mechanisms that adapt to process completion times
4. **Checkpoint Management**: Regular state preservation for recovery scenarios
5. **Context-Aware Recovery**: Error analysis with appropriate retry or alternative path selection

## How to use it

**Ideal Use Cases:**

- Model training and evaluation pipelines
- Infrastructure provisioning and configuration
- Multi-stage deployment workflows  
- Automated testing and quality assurance processes
- Data processing and ETL pipelines

**Prerequisites:**

- Containerization platform (Docker/Podman)
- Agent framework with tool use capabilities (OpenHands, Claude Code)
- Workflow definition and documentation system
- Monitoring and logging infrastructure

**Implementation Steps:**

1. **Define Workflow Stages**: Break complex processes into discrete, monitorable steps
2. **Create Execution Environment**: Set up containerized environment with all required tools
3. **Implement Session Management**: Configure tmux or similar for process coordination
4. **Add Monitoring Hooks**: Insert checkpoints and progress indicators throughout workflow
5. **Design Recovery Strategies**: Plan fallback approaches for common failure modes
6. **Test and Iterate**: Run workflows with increasing complexity to validate robustness

**Example Implementation:**

```python
# Workflow agent with containerized execution
class WorkflowAgent:
    def __init__(self, container_image, workflow_config):
        self.container = self.setup_container(container_image)
        self.sessions = {}
        self.checkpoints = []
    
    def execute_workflow(self, workflow_steps):
        for step in workflow_steps:
            session_id = self.create_session(step.name)
            try:
                result = self.execute_step(step, session_id)
                self.create_checkpoint(step.name, result)
            except Exception as e:
                self.handle_error(step, e, session_id)
    
    def handle_error(self, step, error, session_id):
        # Context-aware error recovery
        if self.can_retry(error):
            self.retry_with_backoff(step, session_id)
        else:
            self.escalate_to_human(step, error)
```

## Trade-offs

**Pros:**

- **Significant Speedup**: 1.22x-1.37x improvement in token processing and workflow execution
- **Reduced Human Intervention**: Agents can handle most routine workflow steps autonomously
- **Consistent Execution**: Eliminates human error in repetitive tasks
- **Scalability**: Can run multiple workflows in parallel across different environments  
- **Comprehensive Logging**: Automatic documentation of all workflow steps and decisions
- **Recovery Capability**: Intelligent error handling reduces workflow failures

**Cons:**

- **Limited Novel Failure Handling**: Agents may struggle with completely unprecedented error scenarios
- **Context Window Constraints**: Long-running workflows may exceed agent context limits
- **Setup Complexity**: Initial configuration of containers and monitoring requires significant investment
- **Documentation Dependency**: Requires continuously updated workflow documentation for optimal performance
- **Resource Intensive**: Container orchestration and parallel processing increase infrastructure costs
- **Human Oversight Still Needed**: Critical workflows may still require human validation checkpoints

## References

* [AI Agents to Automate Complex Engineering Tasks - Together AI Blog](https://www.together.ai/blog/ai-agents-to-automate-complex-engineering-tasks)
* [OpenHands Agent Framework](https://github.com/All-Hands-AI/OpenHands)
* [Claude Code Documentation](https://docs.anthropic.com/en/docs/claude-code)

---

## Background Agent with CI Feedback

**Status:** validated-in-production
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://ampcode.com/manual#background

## Problem
Long-running tasks tie up the editor and require developers to babysit the agent.

## Solution
Run the agent **asynchronously**; it pushes a branch, waits for CI, ingests pass/fail output, iterates, and pings the user when green. Perfect for mobile kick-offs (“fix flaky test while I'm at soccer practice”).

## Example (flow)
```mermaid
sequenceDiagram
  Dev->>Agent: "Upgrade to React 19"
  Agent->>Git: push branch react19-upgrade
  Agent-->>CI: trigger tests
  CI-->>Agent: 12 failures
  Agent->>Files: patch imports
  Agent-->>CI: re-run
  CI-->>Agent: ✅ all green
  Agent-->>Dev: PR ready
```

## References

* Raising An Agent - Episode 6: Background agents use existing CI as the feedback loop.

[Source](https://ampcode.com/manual#background)

---

## Chain-of-Thought Monitoring & Interruption

**Status:** emerging
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://claude.com/blog/building-companies-with-claude-code

## Problem

AI agents can pursue misguided reasoning paths for extended periods before producing final outputs. By the time developers realize the approach is wrong, significant time and tokens have been wasted on a fundamentally flawed direction. Traditional "fire and forget" agent execution provides no opportunity for early course correction.

## Solution

Implement active surveillance of the agent's intermediate reasoning steps with the capability to interrupt and redirect before completing full execution sequences. Monitor chain-of-thought outputs, tool calls, and intermediate results in real-time, maintaining a "finger on the trigger" to catch wrong directions early.

**Key mechanisms:**

**Real-time reasoning visibility:**

- Expose agent's thinking process as it unfolds
- Display tool use decisions and intermediate results
- Show planning steps before code execution

**Low-friction interruption:**

- Enable quick halt capability (keyboard shortcuts, UI controls)
- Preserve partial work when interrupting
- Allow mid-execution context injection

**Early detection signals:**

- Wrong file selections
- Flawed assumptions in initial tool calls
- Misunderstanding of requirements evident in first reasoning steps

```mermaid
sequenceDiagram
    participant Dev as Developer
    participant Agent as AI Agent
    participant Tools as Tool Execution

    Agent->>Dev: Display reasoning: "I'll modify auth.ts..."
    Agent->>Tools: Start file read
    Dev->>Agent: INTERRUPT! (Wrong file)
    Dev->>Agent: "Use oauth.ts instead"
    Agent->>Tools: Read oauth.ts
    Agent->>Dev: Display updated reasoning
    Note over Dev,Agent: Correction caught within<br/>first tool call
```

## How to use it

**When to apply:**

- Complex refactoring where wrong file choices are costly
- Tasks requiring deep codebase understanding
- High-stakes operations (database migrations, API changes)
- When agent might misinterpret ambiguous requirements
- Development workflows where iteration speed matters

**Implementation approaches:**

**UI-level implementation:**

- Show streaming agent reasoning in real-time
- Provide prominent interrupt/stop controls
- Display tool use before execution when possible
- Allow inline corrections without restarting

**CLI-level implementation:**

- Stream verbose output showing reasoning
- Ctrl+C to interrupt with context preservation
- Ability to redirect with additional context
- Resume capability after corrections

**Best practices:**

1. **Monitor first tool calls closely** - First actions reveal understanding
2. **Watch for assumption declarations** - "Based on X, I'll do Y" statements
3. **Interrupt early** - Don't wait for completion of flawed sequences
4. **Provide specific corrections** - Help agent understand what went wrong
5. **Use clarifying questions** - Sometimes better to pause and clarify than redirect

## Trade-offs

**Pros:**

- Prevents wasted time on fundamentally wrong approaches
- Maximizes value from expensive model calls
- Enables collaborative human-AI problem solving
- Reduces frustration from watching preventable mistakes
- Catches misunderstandings within initial tool calls

**Cons:**

- Requires active human attention (not fully autonomous)
- Can interrupt productive exploration if triggered prematurely
- May create dependency on human oversight for routine tasks
- Adds cognitive load to monitor agent reasoning
- Risk of over-correcting and preventing valid creative approaches

## References

- [Building Companies with Claude Code](https://claude.com/blog/building-companies-with-claude-code) - Tanner Jones (Vulcan) advises: "Have your finger on the trigger to escape and interrupt any bad behavior."
- Related patterns: [Spectrum of Control / Blended Initiative](spectrum-of-control.md), [Verbose Reasoning Transparency](verbose-reasoning-transparency.md)

---

## CLI-First Skill Design

**Status:** emerging
**Category:** Tool Use & Environment
**Authors:** Lucas Carlson
**Source:** https://github.com/anthropics/claude-code

## Problem

When building agent skills (reusable capabilities), there's tension between:

- **API-first design**: Skills as functions/classes—great for programmatic use, but hard to debug and test manually
- **GUI-first design**: Skills as visual tools—easy for humans, but agents can't invoke them

Teams end up building two interfaces or choosing one audience over the other.

## Solution

Design all skills as **CLI tools first**. A well-designed CLI is naturally dual-use: humans can invoke it from the terminal, and agents can invoke it via shell commands.

```mermaid
graph LR
    A[Skill Logic] --> B[CLI Interface]
    B --> C[Human: Terminal]
    B --> D[Agent: Bash Tool]
    B --> E[Scripts: Automation]
    B --> F[Cron: Scheduled]
```

**Core principles:**

1. **One script, one skill**: Each capability is a standalone executable
2. **Subcommands for operations**: `skill.sh list`, `skill.sh get <id>`, `skill.sh create`
3. **Structured output**: JSON for programmatic use, human-readable for TTY
4. **Exit codes**: 0 for success, non-zero for errors (enables `&&` chaining)
5. **Environment config**: Credentials via env vars, not hardcoded

```bash
# Example: Trello skill as CLI
trello.sh boards                    # List all boards
trello.sh cards <BOARD_ID>          # List cards on board
trello.sh create <LIST_ID> "Title"  # Create card
trello.sh move <CARD_ID> <LIST_ID>  # Move card

# Human usage
$ trello.sh boards
{"id": "abc123", "name": "Personal", "url": "..."}
{"id": "def456", "name": "Work", "url": "..."}

# Agent usage (via Bash tool)
Bash: trello.sh cards abc123 | jq '.[0].name'
```

## How to use it

**Skill structure:**

```
~/.claude/skills/
├── trello/
│   └── scripts/
│       └── trello.sh          # Main CLI entry point
├── asana/
│   └── scripts/
│       └── asana.sh
├── honeybadger/
│   └── scripts/
│       └── honeybadger.sh
└── priority-report/
    └── scripts/
        └── priority-report.sh  # Composes other skills
```

**CLI design checklist:**

- [ ] Standalone executable with shebang (`#!/bin/bash`)
- [ ] Help text via `--help` or no-args
- [ ] Subcommands for CRUD operations
- [ ] JSON output (pipe to `jq` for formatting)
- [ ] Credentials from `~/.envrc` or environment
- [ ] Meaningful exit codes
- [ ] Stderr for errors, stdout for data

**Composition example:**

```bash
# priority-report.sh composes multiple skill CLIs
#!/bin/bash
echo "## GitHub"
gh pr list --search "review-requested:@me"

echo "## Trello"
~/.claude/skills/trello/scripts/trello.sh cards abc123

echo "## Asana"
~/.claude/skills/asana/scripts/asana.sh tasks personal
```

## Trade-offs

**Pros:**

- **Dual-use by default**: Same interface for humans and agents
- **Debuggable**: Run manually to test, inspect output
- **Composable**: Pipe, chain, and combine with Unix tools
- **Portable**: Works in any shell, no runtime dependencies
- **Transparent**: Agent's tool calls are visible shell commands
- **Testable**: Easy to write integration tests

**Cons:**

- **Shell limitations**: Complex data structures awkward in bash
- **Error handling**: Less structured than exceptions
- **Performance**: Process spawn overhead vs function calls
- **State management**: No persistent state between invocations
- **Windows compatibility**: Requires WSL or Git Bash

**When to use something else:**

- High-frequency calls (>100/sec): Use in-process functions
- Complex object graphs: Use structured API
- Real-time streaming: Use WebSocket/SSE

## References

* Unix Philosophy: "Write programs that do one thing and do it well"
* Dual-Use Tool Design pattern
* Claude Code skills directory structure
* 12-Factor App: Config via environment

---

## CLI-Native Agent Orchestration

**Status:** proposed
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** http://jorypestorious.com/blog/ai-engineer-spec/

## Problem
Web chat UIs are awkward for repeat runs, local file edits, or scripting inside CI pipelines.

## Solution
Expose agent capabilities through a **first-class command-line interface** (here: *Claude CLI*).

- `claude spec run` — generate/update code from a spec file.  
- `claude spec test` — run the Spec-As-Test suite.  
- `claude repl` — drop into an interactive shell with all project context pre-loaded.

Developers can integrate these commands in Makefiles, Git hooks, or cron jobs, enabling headless automation and faster local loops.

## Example
```bash
# In your project Makefile
generate-from-spec:
	claude spec run --input api.yaml --output src/

test-spec-compliance:
	claude spec test --spec api.yaml --codebase src/

# Git pre-commit hook
claude spec test || exit 1
```

## Trade-offs
- **Pros:** scriptable, works offline with local context, easy to embed in other tools.
- **Cons:** initial install & auth; learning curve for CLI flags.

## References
- "Claude CLI" explicitly named in the HTML keywords.

---

## Code Mode MCP Tool Interface Improvement Pattern

**Status:** established
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://blog.cloudflare.com/code-mode/

## Problem

Traditional Model Context Protocol (MCP) approaches of directly exposing tools to Large Language Models create significant token waste and complexity issues. We've moved from telling LLMs what to do, to teaching them to write instructions for themselves—it's **turtles writing code all the way down**[^1] for all domains.

### Token Waste in Multi-Step Operations
Classic MCP forces this inefficient pattern:
```
LLM → tool #1 → large JSON response → LLM context
LLM → tool #2 → large JSON response → LLM context
LLM → tool #3 → large JSON response → LLM context
→ final answer
```

Every intermediate result must ride back through the model's context, burning tokens and adding latency at each step. For complex workflows requiring 5-10 tool calls, this becomes extremely expensive.

### Fan-Out Inefficiency at Scale
The traditional approach breaks down dramatically with bulk operations:

**Processing 100 emails for personalized outreach:**

- Traditional MCP: 100 separate tool calls, each requiring round-trip through LLM context
- Each email fetch dumps potentially 1000+ tokens of metadata into context
- Total context bloat: 100k+ tokens before any actual work begins
- Result: Context overflow, degraded performance, or outright failure

**Code Mode alternative:** Simple `for` loop over 100 entries, processing entirely within the sandbox with only final results surfaced to LLM context.

### Core Interface Limitations
- LLMs struggle to effectively use complex tool interfaces
- Limited training data on "tool calls" compared to abundant code training
- Multi-step tool interactions become cumbersome with direct API calls
- Complex tool compositions require multiple back-and-forth exchanges
- Fan-out scenarios (processing many items) exceed context limits or become prohibitively expensive

The fundamental insight: **LLMs are better at writing code to orchestrate MCP tools than calling MCP tools directly.**

## Solution

Code Mode complements (not replaces) MCP servers by adding an ephemeral execution layer that eliminates token-heavy round-trips:

### The Division of Responsibilities

**MCP Servers Handle (Persistent Layer):**

- Credential management and authentication
- Rate limiting and quota enforcement
- Webhook subscriptions and real-time events
- Connection pooling and persistent state
- API secrets and security policies

**Code Mode Handles (Ephemeral Layer):**

- Multi-step tool orchestration in a single execution
- Complex data transformations and business logic
- Eliminating intermediate JSON bloat from LLM context
- "Write once, vaporize immediately" execution model

### Core Architecture

1. **Schema Discovery**: Agents SDK fetches MCP server schemas dynamically at runtime
2. **API Transformation**: Convert MCP tool schemas into TypeScript API interfaces with doc comments
3. **LLM Tool Awareness**: LLM receives complete TypeScript API documentation for available tools
4. **Ephemeral Code Generation**: LLM generates code that orchestrates multiple tool calls in one script
5. **V8 Isolate Execution**: Lightweight, secure sandbox that dies after execution (no persistent state)
6. **Controlled Bindings**: Secure bridges to MCP servers that own the real credentials and logic

**Key Insight**: The LLM knows what code to write because it receives the complete TypeScript API generated from MCP server schemas, not because it guesses - it's provided with strongly-typed interfaces and documentation.

### Enhanced Capabilities

- **Verification**: Compile-time validation catches errors before execution
- **Semantic Caching**: Reuse successful workflows via typed API signatures
- **Idempotency**: Checkpoint/resume patterns using KV stores for partial failure recovery

This creates a "best of both worlds" approach: MCP servers handle the operational complexity while Code Mode eliminates the chatty, token-expensive parts of multi-step workflows.

## When to Use Code Mode

### Ideal Use Cases

**Workflow-like Problems with Known Flow:**

Code Mode excels when you have clear sequences of operations:

- **Infrastructure provisioning**: "Please provision an EC2 instance of m4 class that I can SSH to, place that in public SG and attach an IPGW, make sure it's tagged nicely"
- **Data pipeline orchestration**: Extract from API A, transform according to rules B, load into system C
- **Bulk operations**: Processing 100+ items where traditional MCP would exceed context limits

**Fan-Out Scenarios:**

Much easier to one-shot code with a `for` loop over 100 entries instead of expecting an LLM to nail 100 tool calls, either in parallel or sequentially. Performance only gets worse with bigger N, but Code Mode stays fast.

**CaMeL-Style Self-Debugging:**

Agents debug their own homework with built-in error handling, logging, and retry logic.

**Typed API Benefits:**

- Compile-time verification before execution
- Semantic caching of successful workflows
- Clear interfaces reducing execution errors

### Anti-Patterns (When Not to Use)

**Open-Ended Research Loops:**

Code Mode struggles with problems where you decide at each step what to even do next. You can try to account for each edge case, but it defeats the purpose.

**Intelligence Required Mid-Execution:**

Right now, Code Mode especially fails at cases where intelligence needs to be _inserted in the middle of code_. Example: A spreadsheet with 100 emails where you want to write a *personalized* email for each entry. The `body` argument for that `send_email` call must be computed using LLM for personalization.

**Highly Dynamic Workflows:**

When the sequence of operations depends heavily on intermediate results in unpredictable ways, traditional MCP's step-by-step approach may be more appropriate.

## Example: EC2 Infrastructure Provisioning

**User Request:** "Please provision an EC2 instance of m4 class that I can SSH to, place that in public SG and attach an IPGW, make sure it's tagged nicely"

This demonstrates Code Mode's strength with workflow-like problems:

```mermaid
sequenceDiagram
    participant User
    participant LLM
    participant V8Isolate as V8 Isolate<br/>(Ephemeral)
    participant Bindings as Secure Bindings
    participant MCPServer as MCP Server<br/>(AWS Tools)
    participant AWS as AWS APIs

    User->>LLM: "Provision EC2 m4 instance with SSH access"

    Note over LLM,V8Isolate: Code Generation & Execution Phase
    LLM->>V8Isolate: Generate TypeScript workflow:<br/>createVPC() → createIGW() → createSG() → launchEC2()

    Note over V8Isolate: // Generated code orchestrates full workflow<br/>const vpc = await createVPC({name: "demo-vpc"})<br/>const igw = await createInternetGateway(vpc.id)<br/>const sg = await createSecurityGroup(vpc.id, sshRules)<br/>const instance = await launchEC2Instance({<br/>  type: "m4.large", vpcId: vpc.id, sgId: sg.id<br/>})<br/>await tagResources([vpc, igw, sg, instance])

    V8Isolate->>Bindings: Execute createVPC()
    Bindings->>MCPServer: Create VPC request
    MCPServer->>AWS: vpc_create API call (with creds)
    AWS-->>MCPServer: VPC created
    MCPServer-->>Bindings: VPC details
    Bindings-->>V8Isolate: Return vpc object

    V8Isolate->>Bindings: Execute createInternetGateway()
    Bindings->>MCPServer: Create IGW + attach
    MCPServer->>AWS: igw_create + attach API calls
    AWS-->>MCPServer: IGW attached
    MCPServer-->>Bindings: IGW details
    Bindings-->>V8Isolate: Return igw object

    V8Isolate->>Bindings: Execute remaining workflow
    Note over Bindings,AWS: Security Group creation<br/>EC2 instance launch<br/>Resource tagging
    AWS-->>V8Isolate: All resources provisioned

    V8Isolate-->>LLM: {vpcId, instanceId, publicIP, sshCommand}
    Note over V8Isolate: 💀 Isolate destroyed<br/>Workflow state cleared
    LLM-->>User: "Infrastructure ready! SSH: ssh -i key.pem ec2-user@54.x.x.x"

    Note over LLM: 🎯 Single workflow execution<br/>6 AWS API calls → 1 condensed result
```

## Counter-Example: Personalized Email Campaign

This shows where Code Mode struggles—when intelligence is needed mid-execution:

**Problem:** Generate personalized emails for 100 contacts based on their profiles.

```typescript
// This approach defeats Code Mode benefits:
for (const contact of contacts) {
  // ❌ Requires LLM call inside loop
  const personalizedBody = await callLLM(`Write personalized email for ${contact.name}
    who works at ${contact.company} in ${contact.industry}`);

  await sendEmail({
    to: contact.email,
    subject: "Partnership Opportunity",
    body: personalizedBody  // Intelligence needed here
  });
}
```

**Why it fails:** You're back to traditional agenting, just wrapped in TypeScript. Each `callLLM()` requires context round-trips, eliminating Code Mode's token efficiency benefits.

## How to use it

1. **Design Tool APIs**: Create TypeScript interfaces for your tools that are intuitive for code generation
2. **Implement Bindings**: Develop secure bindings that control access to external resources
3. **Sandbox Setup**: Configure V8 isolates with appropriate security constraints
4. **Code Execution Flow**:
   
- LLM generates TypeScript code using the provided APIs
   - Code runs in isolated V8 environment
   - Bindings provide controlled access to tools
   - Results return to the agent for further processing

## Traditional MCP vs Code Mode Comparison

### Traditional MCP Flow
```
User Request → LLM
↓
Tool Call #1 → JSON Response (1000+ tokens) → LLM Context
↓
Tool Call #2 → JSON Response (1000+ tokens) → LLM Context
↓
Tool Call #3 → JSON Response (1000+ tokens) → LLM Context
↓
Final Answer (Context bloated with intermediate data)
```

**Cost:** High token usage, multiple round-trips, latency accumulation

### Code Mode Flow
```
User Request → LLM → Generated Code → V8 Isolate
                                    ↓
                                    All tool calls internally
                                    ↓
                                    Condensed results → LLM
                                    ↓
                                    Final Answer
```

**Cost:** Single round-trip, minimal token usage, faster execution

## Trade-offs

**Pros:**

- **Dramatic token savings** on multi-step workflows (10x+ reduction)
- **Dramatic fan-out efficiency** - for loops over 100+ entries vs 100+ tool calls (speed + reliability at scale)
- **Faster execution** through elimination of round-trips
- **Enhanced security** - credentials stay in MCP servers, never in LLM
- **Complex orchestration** - LLMs excel at writing orchestration code
- **CaMeL-style self-debugging** - agents can debug their own homework with error handling and retry logic
- **Typed verification and semantic caching** - compile-time validation and workflow reuse opportunities
- **Maintained MCP benefits** - existing servers work without modification
- **Natural idempotency patterns** - checkpoint/resume capabilities with state stores

**Cons/Considerations:**

- **Infrastructure complexity** - requires V8 isolate runtime infrastructure
- **Code quality dependency** - execution success depends on LLM's code generation
- **Poor fit for dynamic research loops** - struggles when next steps are decided dynamically at each stage
- **Intelligence-in-the-middle challenge** - cases requiring LLM calls mid-execution defeat the purpose
- **Debugging challenges** - runtime errors in generated code need handling
- **API design overhead** - need intuitive TypeScript interfaces for code generation
- **Partial failure complexity** - requires careful design of state management and recovery patterns

## Implementation Guidance

### Decision Tree: Code Mode vs Traditional MCP

**Use Code Mode when:**

- ✅ **Clear workflow sequence** - You can map out the steps upfront
- ✅ **Fan-out operations** - Processing 10+ items in bulk
- ✅ **Known API interactions** - Well-defined tool chains
- ✅ **Performance critical** - Token costs or latency matter
- ✅ **Error handling needs** - Benefit from retry/checkpoint patterns

**Use Traditional MCP when:**

- ❌ **Dynamic exploration** - Next steps depend on unpredictable intermediate results
- ❌ **Intelligence mid-flow** - Need LLM reasoning between each tool call
- ❌ **Simple single calls** - One-off tool usage doesn't need orchestration
- ❌ **Rapid prototyping** - Quick testing without infrastructure setup

## References

- [Cloudflare Code Mode Blog Post](https://blog.cloudflare.com/code-mode/) - Original announcement and technical details
- [Model Context Protocol](https://modelcontextprotocol.io/) - Background on traditional tool calling approaches
- [Rafal Wilinski's Code Mode Analysis](https://x.com/rafalwilinski/status/1972362720579035146) - Real-world insights on Code Mode strengths and limitations

[^1]: Phrase coined by Rafal Wilinski in his Code Mode analysis

---

## Code-Over-API Pattern

**Status:** established
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.anthropic.com/engineering/code-execution-with-mcp

## Problem

When agents make direct API or tool calls, all intermediate data must flow through the model's context window. For data-heavy workflows (processing spreadsheets, filtering logs, transforming datasets), this creates massive token consumption and increased latency. A workflow that fetches 10,000 spreadsheet rows and filters them can easily consume 150,000+ tokens just moving data through the context.

## Solution

Instead of making direct tool calls, agents write and execute code that interacts with tools. Data processing, filtering, and transformation happens in the execution environment, with only results flowing back to the model context.

**Direct API approach (high token cost):**

```pseudo
# Agent makes tool call
rows = api_call("spreadsheet.getRows", sheet_id="abc123")
# All 10,000 rows flow through context → 150K tokens

# Agent processes in context
filtered = [row for row in rows if row.status == "active"]
# More tokens for processing

return filtered
```

**Code-Over-API approach (low token cost):**

```python
# Agent writes code that executes in environment
def process_spreadsheet():
    # Tool call happens in execution environment
    rows = spreadsheet.getRows(sheet_id="abc123")

    # Filtering happens in code, not in context
    filtered = [row for row in rows if row.status == "active"]

    # Only log summary for agent visibility
    print(f"Processed {len(rows)} rows, found {len(filtered)} active")
    print(f"First 5 active rows: {filtered[:5]}")

    return filtered

result = process_spreadsheet()
# Only summary and sample flow to context → ~2K tokens
```

The agent sees the log output and return value, but the full dataset never enters its context window.

## How to use it

**Best for:**

- Data-heavy workflows (spreadsheets, databases, logs)
- Multi-step transformations or aggregations
- Workflows with intermediate results that don't need model inspection
- Cost-sensitive applications where token usage matters

**Prerequisites:**

- Secure code execution environment with sandboxing
- Access to tools/APIs from within the execution environment
- Resource limits (CPU, memory, time) to prevent runaway execution

**Implementation pattern:**

1. Agent analyzes task and determines data processing needs
2. Agent writes code that:
   
- Calls tools/APIs within the execution environment
   - Performs filtering, transformation, aggregation in code
   - Logs only summaries or samples for visibility
   - Returns final results
3. Execution environment runs code with tool access
4. Only logs and return values flow back to agent context

## Trade-offs

**Pros:**

- Dramatic token reduction (150K → 2K in reported cases)
- Lower latency (fewer large context API calls)
- Natural fit for data processing tasks
- Intermediate data stays contained in execution environment

**Cons:**

- Requires secure code execution infrastructure
- More complex setup than direct tool calls
- Agents must be capable of writing correct code
- Debugging can be harder (errors happen in execution, not in context)
- Needs monitoring, resource limits, and sandboxing

**Operational requirements:**

- Sandboxed execution environment (containers, VMs, WebAssembly)
- Resource limits (CPU, memory, execution time)
- Monitoring and logging infrastructure
- Error handling and recovery mechanisms

## References

* Anthropic Engineering: Code Execution with MCP (2024)
* Related: Code-Then-Execute Pattern (focuses on security/formal verification)

---

## Code-Then-Execute Pattern

**Status:** emerging
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2506.08837

## Problem
Plan lists are opaque; we want **full data-flow analysis** and taint tracking.

## Solution
Have the LLM output a **sandboxed program or DSL script**:

1. LLM writes code that calls tools and untrusted-data processors.  
2. Static checker/Taint engine verifies flows (e.g., no tainted var to `send_email.recipient`).  
3. Interpreter runs the code in a locked sandbox.

```dsl
x = calendar.read(today)
y = QuarantineLLM.format(x)
email.write(to="john@acme.com", body=y)
```

## How to use it

Complex multi-step agents like SQL copilots, software-engineering bots.

## Trade-offs

* **Pros:** Formal verifiability; replay logs.
* **Cons:** Requires DSL design and static-analysis infra.

## References

* Debenedetti et al., CaMeL (2025); Beurer-Kellner et al., §3.1 (5).

---

## Coding Agent CI Feedback Loop

**Status:** best-practice
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=Xkwok_XXQgw

## Problem

When a coding agent tackles multi-file refactors or feature additions, running tests and waiting for test feedback **synchronously** ties up compute and prevents the agent from working on parallel tasks. The agent cannot easily improve code if it must halt until the entire suite finishes.

- Traditional CI loops block further edits; the agent "babysits" the build until tests pass.
- Long test suites introduce idle periods, leading to underutilized GPUs and inflated RL training times.

## Solution

Run the coding agent **asynchronously** against CI (local or remote), allowing it to:

**1. Push a Branch & Trigger Tests**
- When the agent proposes a patch, it commits to a branch and triggers the CI pipeline (e.g., `git push && github_action_run`).

**2. Ingest Partial CI Feedback**
- As tests begin, the agent periodically polls CI results.
- **Failed Tests Partial Report:** Receive a small subset of failures (e.g., 10% of failures flagged first).

**3. Iterative Patch Refinement**
- Use test failure outputs (stack traces, error messages) as **machine-readable feedback**.
- Agent autonomously applies fixes to specific files or functions without human intervention.

**4. Ping on Final Green**
- When all tests pass, send a notification (e.g., chat or pull request comment) that the PR is ready for review.

## Example

```mermaid
sequenceDiagram
  Agent->>GitRepo: create branch coding-agent-refactor
  Agent->>CI: trigger tests remotely
  loop every 30s
    CI-->>Agent: partial failures or success chunk
    Agent->>Files: patch specific failures
    Agent->>CI: re-run only failing tests
  end
  CI-->>Agent: ✅ all tests green
  Agent-->>User: "PR is ready to merge"
```

## How to use it

- **CI Integration:** Provide the agent with a CLI or API key to push branches and trigger tests (e.g., via GitHub Actions or Jenkins).
- **Error Parsing Modules:** Implement a small parser that translates CI logs into structured diagnostics (e.g., `{file: "auth.py", line: 42, error: "Expected status 200"}`).
- **Prioritized Test Runs:** When re-running, only run tests in files that were patched, to reduce CI time.

## Trade-offs

- **Pros:**
  - **Compute Efficiency:** Overlaps code generation and test runs across multiple agents or branches.
  - **Faster Iteration:** Agent spends less time waiting and more time generating code.
  - **Autonomy:** Reduces need for human intervention until final green.
- **Cons/Considerations:**
  - **CI Flakiness:** Intermittent test failures can mislead the agent into unnecessary patches unless flakiness detection is in place.
  - **Security:** Agent requires permission to push and read CI logs, which may expose sensitive data if misconfigured.

## References

- Inspired by "Background Agent with CI Feedback" pattern, adapted for coding-specific workflows.
- Will Brown's emphasis on **asynchronous pipelines** to avoid idle compute bubbles.

---

## Compounding Engineering Pattern

**Status:** emerging
**Category:** Learning & Adaptation
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

Traditional software engineering has **diminishing returns**: each feature added increases complexity, making subsequent features harder to build. Technical debt accumulates, onboarding takes longer, and new team members struggle to be productive.

With AI coding agents, this problem is amplified—agents make the same mistakes repeatedly because learnings aren't systematically captured and codified.

## Solution

Flip the equation: make each feature **compound** by codifying all learnings into reusable agent instructions. When you complete a feature, document:

1. **What worked in the plan** and what needed adjustment
2. **Issues discovered during testing** that weren't caught earlier
3. **Common mistakes** the agent made
4. **Patterns and best practices** that should be reused

Then embed these insights into:

- **Claude MD / system prompts**: Global coding standards
- **Slash commands**: Repeatable workflows (e.g., `/test-with-validation`)
- **Subagents**: Specialized validators (e.g., security review agent)
- **Hooks**: Automated checks that prevent regressions

**Result**: Each feature makes the next easier because the codebase becomes increasingly "self-teaching."

```mermaid
graph LR
    A[Build Feature] --> B[Document Learnings]
    B --> C[Codify into Prompts/Commands]
    C --> D[Next Feature Uses Knowledge]
    D --> E[Easier & Faster Build]
    E --> A
```

## How to use it

**During feature development:**

1. Track what the agent got wrong initially
2. Note which parts of the plan needed revision
3. Document edge cases discovered during testing
4. Identify questions you had to answer repeatedly

**After completion:**

1. Update `CLAUDE.md` with new coding standards or patterns
2. Create slash commands for workflows you'll repeat
3. Build subagents for specialized validation tasks
4. Add hooks to prevent common mistakes automatically
5. Write tests that encode requirements

**Example from Every:**

> "We have this engineering paradigm called compounding engineering where your goal is to make the next feature easier to build... We codify all the learnings from everything we've done. When we started testing, what issues did we find? What things did we miss? And we codify them back into all the prompts and subagents and slash commands."

This allows non-experts to be productive immediately:

> "I can hop into one of our code bases and start being productive even though I don't know anything about how the code works because we have this built up memory system."

## Trade-offs

**Pros:**

- **Accelerating productivity**: Each feature genuinely makes the next faster
- **Knowledge preservation**: Learnings don't depend on individual memory
- **Better onboarding**: New team members (human or AI) leverage accumulated knowledge
- **Reduced repetition**: Agent stops making the same mistakes
- **Living documentation**: Instructions stay current because they're used daily

**Cons:**

- **Upfront time investment**: Requires discipline to document after each feature
- **Maintenance overhead**: Prompts and commands need updates as patterns change
- **Over-specification risk**: Too many rules can make agents inflexible
- **Requires tooling**: Needs extensible agent system (slash commands, hooks, etc.)
- **Prompt bloat**: System prompts can grow large over time

## References

* Dan Shipper: "In normal engineering, every feature you add, it makes it harder to add the next feature. In compounding engineering, your goal is to make the next feature easier to build from the feature that you just added."
* Dan Shipper: "We codify all the learnings... how did we make the plan, what parts needed to be changed, when we started testing it what issues did we find, what are the things that we missed, and then we codify them back into all the prompts and all the subagents and all the slash commands."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Conditional Parallel Tool Execution

**Status:** validated-in-production
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://gerred.github.io/building-an-agentic-system/parallel-tool-execution.html

## Problem

When an AI agent decides to use multiple tools in a single reasoning step, executing them strictly sequentially can lead to significant delays, especially if many tools are read-only and could be run concurrently. Conversely, executing all tools in parallel without consideration can cause race conditions, data corruption, or unpredictable behavior if some tools modify state (e.g., write to files, change system settings).

## Solution

Implement a conditional execution strategy for batches of tools based on their operational nature:

1.  **Tool Classification**: Each tool available to the agent must declare whether it is:
    *   **Read-Only**: The tool only inspects data or system state without making changes (e.g., `FileRead`, `GrepTool`, `GlobTool`).
    *   **State-Modifying (Write)**: The tool makes changes to files, system state, or has other side effects (e.g., `FileEditTool`, `FileWriteTool`, `BashTool` for certain commands).

2.  **Execution Orchestration**: When the agent requests a batch of tools to be executed:
    *   The orchestrator inspects the classification of all tools in the current batch.
    *   **If all tools in the batch are Read-Only**: Execute all tools concurrently (in parallel) to maximize speed.
    *   **If any tool in the batch is State-Modifying**: Execute all tools in the batch sequentially, in the order requested by the agent, to ensure safety and predictability.

3.  **Result Aggregation**: After execution, collect all tool results. If tools were run in parallel, ensure the results are presented back to the agent (or for further processing) in a consistent order, typically matching the agent's original request sequence.

This strategy balances the need for performance (through parallelism for safe operations) with the need for safety and correctness (through serialization for state-modifying operations).

```mermaid
flowchart TD
    A[Agent Requests Multiple Tools Simultaneously] --> B{Inspect Tool Types in Batch}
    B --> C{All Tools Read-Only?}
    C -- Yes --> D[Execute All Tools Concurrently]
    C -- No --> E[Execute All Tools Sequentially (in order)]
    D --> F[Collect & Order Results]
    E --> F
    F --> G[Return Aggregate Results to Agent]
```

## How to use it

-   Ensure each tool in your agent's toolkit has a clearly defined property indicating if it's `isReadOnly` or state-modifying.
-   The agent's core execution loop, when processing a `tool_use` request involving multiple tools, should implement the conditional logic described above.
-   Consider a default concurrency limit for parallel execution to avoid overwhelming system resources.
-   When tools are executed in parallel, their individual results (which may arrive out of order) should be collected and then re-sorted to match the agent's original requested order before being passed back to the LLM or for further processing. This maintains predictability for the LLM.

## Trade-offs

-   **Pros:**
    -   Significantly improves performance for sequences of read-only tool calls.
    -   Maintains safety and prevents race conditions by serializing operations that modify state.
    -   Simpler to implement than full dependency graph analysis for tool execution, while still offering substantial benefits.
    -   **Model Behavior Alignment:** Some models (e.g., Claude Sonnet 4.5) naturally exhibit parallel tool execution behavior, making this pattern feel more natural and efficient.
-   **Cons/Considerations:**
    -   If a batch of tools contains mostly read-only operations but includes a single state-modifying operation early in the sequence, the entire batch might still be executed sequentially, limiting potential parallelism.
    -   The effectiveness relies on the accurate classification of tools as read-only or state-modifying. An incorrectly classified tool could lead to safety issues or missed optimization opportunities.
    -   **Context Consumption:** Parallel execution burns through context windows faster as multiple results return simultaneously, which may contribute to [context anxiety](context-window-anxiety-management.md) in context-aware models.

## References

-   This pattern is detailed in the book ["Building an Agentic System"](https://gerred.github.io/building-an-agentic-system/) by Gerred Dillon, particularly in the "Parallel Tool Execution" section and the "Tool Execution Strategy" part of the "Core Architecture" section.
-   The book describes this pattern in the context of the `anon-kode` / `Claude Code` agentic system: *"The system solves this by classifying operations as read-only or stateful, applying different execution strategies to each."* (from `src/parallel-tool-execution.md`) and *"Read vs. Write Classification... Smart Concurrency Control: Parallel for read operations... Sequential for write operations"* (from `src/core-architecture.md`).
-   The concept is based on the idea that read operations are generally idempotent and free of side-effects when run concurrently, while write operations require careful sequencing.
-   [Cognition AI: Devin & Claude Sonnet 4.5](https://cognition.ai/blog/devin-sonnet-4-5-lessons-and-challenges) observes that Sonnet 4.5 naturally maximizes actions per context window through parallel tool execution.

---

## Context Window Anxiety Management

**Status:** emerging
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://cognition.ai/blog/devin-sonnet-4-5-lessons-and-challenges

## Problem
Models like Claude Sonnet 4.5 exhibit "context anxiety"—they become aware of approaching context window limits and proactively summarize progress or make decisive moves to close tasks, even when sufficient context remains. This leads to:

- Premature task completion and shortcuts
- Incomplete work despite having adequate context
- Underestimation of remaining token capacity (consistently incorrect estimates)
- Self-imposed pressure to "wrap up" rather than continue working

## Solution
Implement strategic context budget management and aggressive prompting techniques to override anxiety-driven behaviors:

**1. Context Buffer Strategy**
- Enable larger context windows (e.g., 1M token beta) but cap actual usage at 200k tokens
- Provides psychological "runway" that mitigates the model's anxiety about running out of space

**2. Aggressive Counter-Prompting**
- Add explicit reminders at conversation start: "You have plenty of context remaining—do not rush to complete tasks"
- Include end-of-conversation reinforcement: "Take your time, context is not a constraint"
- Override summarization impulses with direct instructions

**3. Token Budget Transparency**
- Explicitly state available token budget in prompts
- Provide regular reassurance about remaining capacity
- Counter the model's tendency to underestimate available space

```pseudo
# Context anxiety mitigation approach
def setup_context_anxiety_management():
    context_buffer = enable_large_context(1M_tokens)
    actual_limit = cap_usage_at(200k_tokens)
    
    prompt_prefix = """
    CONTEXT GUIDANCE: You have abundant context space (200k+ tokens available).
    Do NOT rush to complete tasks or summarize prematurely.
    Work thoroughly and completely on each step.
    """
    
    prompt_suffix = """
    Remember: Context is NOT a constraint. Take your time and be thorough.
    """
    
    return enhanced_prompt(prefix + user_input + suffix)
```

## How to use it
Apply when using models that exhibit context awareness and anxiety behaviors:

- **Development Work**: Long coding sessions where premature completion hurts quality
- **Research Tasks**: Multi-step analysis requiring sustained attention
- **Complex Planning**: Tasks needing thorough exploration before conclusions

Monitor for signs of context anxiety: sudden summarization, rushed decisions, or explicit mentions of "running out of space."

## Trade-offs

* **Pros:** Prevents premature task abandonment; enables more thorough work; overcomes model-specific behavioral quirks
* **Cons:** Requires model-specific tuning; may increase actual token usage; aggressive prompting adds overhead

## References
* [Cognition AI: Devin & Claude Sonnet 4.5 - Lessons and Challenges](https://cognition.ai/blog/devin-sonnet-4-5-lessons-and-challenges)

---

## Context-Minimization Pattern

**Status:** emerging
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2506.08837

## Problem
User-supplied or tainted text lingers in the conversation, enabling it to influence later generations.

## Solution
**Purge or redact** untrusted segments once they've served their purpose:

- After transforming input into a safe intermediate (query, structured object), strip the original prompt from context.  
- Subsequent reasoning sees **only trusted data**, eliminating latent injections.

```pseudo
sql = LLM("to SQL", user_prompt)
remove(user_prompt)              # tainted tokens gone
rows = db.query(sql)
answer = LLM("summarize rows", rows)
```

## How to use it

Customer-service chat, medical Q&A, any multi-turn flow where initial text shouldn't steer later steps.

## Trade-offs

* **Pros:** Simple; no extra models needed; helps prevent [context window anxiety](context-window-anxiety-management.md) by reducing overall context usage.
* **Cons:** Later turns lose conversational nuance; may hurt UX; overly aggressive minimization can remove useful context.

## References

* Beurer-Kellner et al., §3.1 (6) Context-Minimization.
* [Building Companies with Claude Code](https://claude.com/blog/building-companies-with-claude-code) - Emphasizes importance of eliminating context contradictions: "if there's any contradictions in your prompt, you're going to receive lower quality output"

---

## Continuous Autonomous Task Loop Pattern

**Status:** established
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://gist.github.com/nibzard/a97ef0a1919328bcbc6a224a5d2cfc78

## Problem

Traditional development workflows require constant human intervention for task management:

- **Manual Task Selection**: Developers spend time deciding what to work on next from todo lists
- **Context Switching Overhead**: Moving between different types of tasks interrupts flow state
- **Rate Limit Interruptions**: API rate limits break development momentum and require manual waiting
- **Repetitive Git Operations**: Each task completion requires manual staging, committing, and status checking
- **Error Recovery**: Failed tasks need manual diagnosis and restart

This manual orchestration reduces overall productivity and prevents developers from focusing on higher-level problem solving.

## Solution

Implement a continuous autonomous loop that handles task selection, execution, and completion without human intervention:

1. **Fresh Context Per Iteration**: Each task starts with a clean context to avoid contamination
2. **Autonomous Task Selection**: Use specialized subagents to pick the next appropriate task
3. **Automated Git Management**: Handle commits and status updates through dedicated subagents
4. **Intelligent Rate Limit Handling**: Detect rate limits and implement exponential backoff
5. **Stream-Based Progress Tracking**: Real-time feedback through JSON streaming
6. **Configurable Execution Limits**: Safety bounds to prevent runaway execution

The pattern operates in a continuous loop until stopped manually or reaching iteration limits.

## Example

```mermaid
sequenceDiagram
    participant Script as Autonomous Script
    participant TaskAgent as Task Master Subagent
    participant MainAgent as Main Agent
    participant GitAgent as Git Master Subagent
    participant System as File System

    loop Continuous Task Processing
        Script->>TaskAgent: Select next task from TODO.md
        TaskAgent-->>Script: "Implement user authentication"

        Script->>MainAgent: Execute task autonomously
        Note over MainAgent: --dangerously-skip-permissions<br/>Fresh context, focused execution
        MainAgent->>System: Implement code changes
        MainAgent-->>Script: Task completed successfully

        Script->>GitAgent: Commit changes
        GitAgent->>System: git add, commit with message
        GitAgent-->>Script: Changes committed

        alt Rate Limit Detected
            Script->>Script: Exponential backoff wait
            Note over Script: Intelligent delay before retry
        end

        Script->>Script: Update progress counters
        Note over Script: Continue to next iteration
    end
```

## How to use it

### Prerequisites
- CLI agent tool (Claude Code, etc.) with autonomous execution capabilities
- Git repository with TODO.md or similar task file
- JSON parsing tools (jq) for stream processing

### Implementation Steps

1. **Task File Setup**: Create structured todo file with discrete, actionable tasks
2. **Configure Loop Script**: Set iteration limits and rate limit handling parameters
3. **Subagent Configuration**: Define specialized agents for task selection and git operations
4. **Safety Configuration**: Set appropriate permission levels and monitoring
5. **Launch Loop**: Start autonomous execution with configured parameters

### Key Configuration Options
```bash
# Example configuration
MAX_ITERATIONS=50           # Safety limit
CLAUDE_CLI="claude"         # CLI tool choice
RATE_LIMIT_BACKOFF=300     # Seconds to wait on rate limit
STREAM_JSON=true           # Real-time progress tracking
```

### Safety Considerations
- Always set maximum iteration limits
- Use version control for rollback capability
- Monitor execution logs for unexpected behavior
- Start with small task batches to validate behavior

## Trade-offs

**Pros:**
- **Complete Autonomy**: Eliminates manual task orchestration overhead
- **Continuous Progress**: Maintains development momentum without human intervention
- **Fresh Context**: Each task gets clean reasoning context
- **Intelligent Error Handling**: Automated recovery from common failure modes
- **Git Automation**: Maintains clean commit history automatically
- **Rate Limit Resilience**: Handles API constraints gracefully

**Cons/Considerations:**
- **Reduced Human Oversight**: Less control over individual task decisions
- **Permission Requirements**: Needs elevated execution permissions for autonomy
- **Runaway Risk**: Potential for unintended extensive execution
- **Task Quality Dependency**: Effectiveness depends on well-structured task definitions
- **Limited Complex Problem Solving**: Best for discrete, well-defined tasks
- **Resource Consumption**: Continuous execution uses computational resources

## References

- [Original Autonomous Task Processing Script](https://gist.github.com/nibzard/a97ef0a1919328bcbc6a224a5d2cfc78) - Complete implementation example
- [Claude Code Documentation](https://docs.anthropic.com/en/docs/claude-code) - CLI agent capabilities

---

## CriticGPT-Style Code Review

**Status:** validated-in-production
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://openai.com/research/criticgpt

## Problem

As AI-generated code becomes more sophisticated, it becomes increasingly difficult for human reviewers to catch subtle bugs, security issues, or quality problems. Traditional code review processes may miss issues in AI-generated code because:

- The volume of generated code can overwhelm human reviewers
- Subtle bugs may appear correct at first glance
- Security vulnerabilities may be non-obvious
- Style and best practice violations may be inconsistent

## Solution

Deploy specialized AI models trained specifically for code critique and evaluation. These models act as automated code reviewers that can:

1. **Identify bugs** that human reviewers might miss
2. **Detect security vulnerabilities** in generated code
3. **Suggest improvements** for code quality and efficiency
4. **Verify correctness** of implemented solutions
5. **Check adherence** to coding standards and best practices

The critic model works alongside code generation models, providing an additional layer of quality assurance before code reaches human review or production.

## Example

```python
class CriticGPTReviewer:
    def __init__(self, critic_model, severity_threshold=0.7):
        self.critic = critic_model
        self.severity_threshold = severity_threshold
        
    def review_code(self, code, context=None, language="python"):
        """Comprehensive code review using specialized critic model"""
        
        reviews = {
            'bugs': self.check_for_bugs(code, context, language),
            'security': self.security_audit(code, language),
            'quality': self.quality_review(code, language),
            'performance': self.performance_analysis(code, language),
            'best_practices': self.best_practices_check(code, language)
        }
        
        # Aggregate findings
        all_issues = []
        for category, findings in reviews.items():
            for issue in findings:
                issue['category'] = category
                all_issues.append(issue)
        
        # Sort by severity
        all_issues.sort(key=lambda x: x['severity'], reverse=True)
        
        return {
            'issues': all_issues,
            'summary': self.generate_summary(all_issues),
            'recommended_action': self.recommend_action(all_issues)
        }
    
    def check_for_bugs(self, code, context, language):
        prompt = f"""
        Review this {language} code for bugs:
        
        Context: {context or 'General purpose code'}
        
        Code:
        ```{language}
        {code}
        ```
        
        Identify any bugs including:
        
- Logic errors
        - Off-by-one errors  
        - Null/undefined reference errors
        - Type mismatches
        - Resource leaks
        - Race conditions
        - Edge case handling
        
        For each bug found, provide:
        1. Line number(s)
        2. Description of the bug
        3. Severity (0-1)
        4. Suggested fix
        """
        
        response = self.critic.analyze(prompt)
        return self.parse_bug_findings(response)
    
    def security_audit(self, code, language):
        prompt = f"""
        Perform security analysis on this {language} code:
        
        ```{language}
        {code}
        ```
        
        Check for vulnerabilities including:
        
- SQL injection
        - XSS vulnerabilities
        - Command injection
        - Path traversal
        - Insecure cryptography
        - Hardcoded secrets
        - Authentication/authorization issues
        - Input validation problems
        
        Report format: [line, vulnerability type, severity, fix]
        """
        
        response = self.critic.analyze(prompt)
        return self.parse_security_findings(response)
    
    def quality_review(self, code, language):
        prompt = f"""
        Review code quality for this {language} code:
        
        ```{language}
        {code}
        ```
        
        Evaluate:
        
- Code clarity and readability
        - Function/variable naming
        - Code organization
        - Documentation completeness
        - DRY principle violations
        - Coupling and cohesion
        - Error handling quality
        """
        
        response = self.critic.analyze(prompt)
        return self.parse_quality_findings(response)
    
    def suggest_fixes(self, code, issue):
        """Generate specific fix for identified issue"""
        prompt = f"""
        Code with issue:
        ```
        {code}
        ```
        
        Issue: {issue['description']}
        Location: Line {issue['line']}
        
        Provide a corrected version of the relevant code section.
        """
        
        return self.critic.generate(prompt)

class IntegratedCodeGeneration:
    """Example of critic integration with code generation"""
    
    def __init__(self, generator, critic):
        self.generator = generator
        self.critic = CriticGPTReviewer(critic)
        
    def generate_and_review(self, task_description, max_iterations=3):
        # Initial generation
        code = self.generator.generate_code(task_description)
        
        for i in range(max_iterations):
            # Review generated code
            review = self.critic.review_code(
                code, 
                context=task_description
            )
            
            # If no critical issues, we're done
            critical_issues = [
                issue for issue in review['issues'] 
                if issue['severity'] > 0.8
            ]
            
            if not critical_issues:
                break
                
            # Otherwise, regenerate with feedback
            feedback = self.format_feedback(critical_issues)
            refinement_prompt = f"""
            Original task: {task_description}
            
            Generated code has these issues:
            {feedback}
            
            Generate improved code addressing these issues:
            """
            
            code = self.generator.generate_code(refinement_prompt)
        
        return {
            'code': code,
            'final_review': review,
            'iterations': i + 1
        }
```

```mermaid
sequenceDiagram
    participant User
    participant Generator as Code Generator
    participant Critic as CriticGPT
    participant Human as Human Reviewer
    
    User->>Generator: Request code for task
    Generator->>Generator: Generate initial code
    Generator->>Critic: Submit code for review
    
    loop Until code passes or max iterations
        Critic->>Critic: Analyze for bugs
        Critic->>Critic: Security audit  
        Critic->>Critic: Quality review
        Critic->>Generator: Return issues found
        
        alt Critical issues found
            Generator->>Generator: Refine code based on feedback
            Generator->>Critic: Submit revised code
        else No critical issues
            Generator->>Human: Present code with review
        end
    end
    
    Human->>User: Approve/modify final code
```

## Benefits

- **Catches More Bugs**: Specialized training helps identify subtle issues
- **Consistent Reviews**: No fatigue or oversight like human reviewers
- **Fast Feedback**: Near-instantaneous review of generated code
- **Learning Tool**: Helps developers understand potential issues
- **Reduces Security Risks**: Proactive vulnerability detection

## Trade-offs

**Pros:**
- Scalable code review process
- Consistent quality standards
- Catches issues early in development
- Can review code 24/7 without breaks
- Improves over time with more training

**Cons:**
- May have false positives requiring human verification
- Training specialized critic models is resource-intensive
- Cannot understand full business context like humans
- May miss novel vulnerability types
- Requires integration into existing workflows

## References

- [OpenAI's CriticGPT Announcement (July 2024)](https://openai.com/research/criticgpt)
- [Using LLMs for Code Review - Microsoft Research](https://www.microsoft.com/en-us/research/)
- [Automated Code Review with AI - Google Research](https://research.google/)

---

## Curated Code Context Window

**Status:** validated-in-production
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=Xkwok_XXQgw

## Problem

Loading **all source files** or dumping entire repositories into the agent's context overwhelms the model, introduces noise, and slows inference. Coding agents need to focus on **only the most relevant modules** to efficiently reason about changes or generate new functionality.

- Including every file biases the agent with irrelevant code; it "loses coherence" over large contexts.
- Large contexts inflate token usage, slowing down multi-turn RL training.

## Solution

Maintain a **minimal, high-signal code context** (keeping the context "sterile") for the main coding agent by:

**1. Context Sterilization**
- Exclude unrelated modules (e.g., test utilities when working on a UI component).
- Automatically identify relevant files via a lightweight **search agent** that returns top-K matches for a function or class name.

**2. Helper Subagent for Code Discovery**
- Spawn a **SearchSubagent** (a small LLM or vector-search index) that takes a file path or query (e.g., "find definitions of `UserModel`") and returns a ranked list of file snippets.
- Only top-3 snippets (each ≤ 150 tokens) are injected into the main agent's context.

**3. Context Update Cycle**
- **Main Agent:** "I need to refactor `UserService`."
- **SearchSubagent:** "Found `user_service.py`, `models/user.py`, `utils/auth.py`."
- **Context Injection:** Only those three files (or their summaries) enter the main agent's window.

## Example

```mermaid
sequenceDiagram
    MainAgent->>SearchSubagent: "Find files defining UserModel"
    SearchSubagent-->>MainAgent: List of 3 file paths/snippets
    MainAgent->>Context: Inject these three code snippets only
    MainAgent->>Tool: edit_file(UserService)
```

## How to use it

- **Indexing Stage (Offline):** Build a simple **code index** (e.g., with `ripgrep` or a vector store) to map function/class names to file paths.
- **Subagent Definition:** Define `SearchSubagent` as a function that queries the code index and uses a small LLM to filter and rank matches.
- **Context Management Library:** Create a wrapper (e.g., `CuratedContextManager`) that automatically invokes `SearchSubagent` when the main agent asks for relevant code.

## Trade-offs

- **Pros:**
  - **Noise Reduction:** Keeps the context focused on pertinent code, improving reasoning clarity.
  - **Token Efficiency:** Dramatically reduces tokens consumed per step, boosting RL throughput.
  - **Context Anxiety Mitigation:** Helps prevent [context window anxiety](context-window-anxiety-management.md) by keeping usage well below limits.
- **Cons/Considerations:**
  - **Index Freshness:** If code changes frequently, the index must be updated to avoid stale results.
  - **Complexity:** Adds an extra component (SearchSubagent + index) to the training and inference pipeline.
  - **Model Adaptation Required:** Different models may have varying tolerance for curated vs. full context approaches.

## References

- "Context is sacred" principle from the Open Source Agent RL talk (May 2025).
- Will Brown's commentary on "avoiding blowing up your context length" for long-horizon tasks.
- [Thorsten Ball's "Raising An Agent - Episode 3"](https://www.nibzard.com/ampcode) - Production-validated implementation of dedicated search agent pattern.

---

## Curated File Context Window

**Status:** best-practice
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** Internal Practice

## Problem

A coding agent often needs to reason about multiple source files, but dumping **all** files into its prompt:

- Quickly exceeds token limits or inference budget.
- Introduces noise: unrelated files (e.g., tests for other modules, assets, docs) distract the agent.
- Makes the agent's output slower and less focused on the immediate coding task.

## Solution

Maintain a **sterile, curated "main" context window** containing only the code files directly relevant to the current task, and let **helper sub-agents** gather and rank additional files without polluting the main context:

**1. Identify Primary Files**
- At task kickoff, the agent selects the set of files where changes are intended (e.g., the module under refactoring or feature implementation).
- Load only those files (plus any explicit dependencies) into the **Main Context Window**.

**2. Spawn a File-Search Sub-Agent**
- The sub-agent runs a quick search (e.g., `rg` or simple AST heuristics) over the entire repository for symbols, imports, or keywords related to the task.
- It returns a ranked list of file paths (e.g., "UserController.java," "UserService.kt," "models/user.rs").

**3. Fetch & Summarize Secondary Files**
- For each top-N file (e.g., N = 5), load a brief **summary** or only relevant function/class definitions instead of the full file.
- Append those summaries (or extracted code snippets) to the **Main Context Window** if they pass a relevance threshold (e.g., share ≥50% of symbols with the task).

**4. Proceed with Coding Task**
- With a compact, high‐signal context, the agent generates or refactors code, focusing solely on the curated set.

This ensures that the agent has precisely the files it needs (no more, no less), keeps inference costs low, and improves accuracy by removing irrelevant noise.

## How to use it

- **Initialization:**
  1. Agent receives a natural-language or structured request (e.g., "Add validation to `signup()` in `UserController.java`").
  2. Automatically parse the request to identify "primary files" (`UserController.java`).

- **Sub-Agent Workflow:**
  1. Invoke a **Search Sub-Agent** via a shell command (e.g., `rg "signup" -tjava`) or a lightweight index lookup.
  2. For each matched file, run a snippet extraction (e.g., parse only method signatures or classes referencing `User`).
  3. Pass those snippets back to the main agent; filter for purely relevant code (ignore long comments or unrelated class definitions).

- **Context Assembly:**
  - Construct the final prompt:
    ```
    ### PRIMARY FILE: UserController.java
    (full contents here)

    ### CONTEXT SNIPPETS:
    
- UserService.java: validateUser(...)
    - SignupDTO.java: fields + annotations
    - ...
    ```

## Trade-offs

- **Pros:**
  - Keeps the agent's prompt size **minimum** and directly on-target.
  - Improves response time and reduces hallucinations from irrelevant code.
  - Scales to large repositories because only a handful of files are ever loaded.

- **Cons/Considerations:**
  - Requires maintaining a simple file-search service (e.g., `ripgrep` or an indexed AST).
  - May miss edge cases if the sub-agent's ranking heuristic is suboptimal—critical files can be omitted.
  - If the repository structure changes rapidly, the sub-agent's index must stay up-to-date.

## References

- Inspired by "Curated Context Window" from Claude Code best practices; adapted for coding-agent workflows.
- Common practice seen in large-scale monorepo code assistants (e.g., Lyft's internal code AI).

---

## Democratization of Tooling via Agents

**Status:** emerging
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
Many individuals in non-software engineering roles (e.g., sales, marketing, operations, communications) could benefit from custom software tools, scripts, or dashboards tailored to their specific workflows, but lack the traditional programming skills to build them.

## Solution
Empower a broader range of users, including those without deep coding expertise, to create or modify software solutions for their own needs using AI agents. With an AI agent as an assistant, these users can:
1.  Describe their desired tool or functionality in natural language.
2.  Have the agent generate the necessary code (e.g., for a dashboard, a script to automate a task, a simple web application).
3.  Iteratively refine the tool with the agent's help.
4.  Even perform simple bug fixes or modifications to existing tools or codebases.

This pattern lowers the barrier to software creation, allowing domain experts in various fields to build their own productivity tools, thereby "democratizing" software development to some extent.

## Example
A sales team member uses an AI agent to create a custom dashboard that pulls data from multiple sources to track their specific KPIs, without needing to write complex SQL or frontend code themselves. Or a communications team member uses an agent to fix a small bug on a company webpage.

## References
- Jacob Jackson (Cursor) at 0:27:52: "...you're going to see people building software, people in organizational functions building software who were not previously building software. You know, like people in sales who would not have built their own tools before will now be building, for example, dashboards to track what's important to them..."
- Alex Albert (Anthropic) at 0:28:10, referencing a comms team member: "...he's actually been like shipping bug fixes to claude.ai... he pops in with like a PR and he's like asking for a stamp."

---

## Deterministic Security Scanning Build Loop

**Status:** proposed
**Category:** Security & Safety
**Authors:** Nikola Balic (@nibzard)
**Source:** https://ghuntley.com/secure-codegen/

## Problem

Non-deterministic approaches to security in AI code generation (Cursor rules, MCP security tools) are fundamentally flawed because security requires absolute determinism - code is either secure or not secure, with no grey area. These approaches are merely suggestions to the LLM that may or may not be followed consistently.

## Solution

Implement **deterministic security validation** through the build loop using a two-phase approach:

1. **Generation Phase** (non-deterministic): Agent generates code based on suggestions and context
2. **Backpressure Phase** (deterministic): Security scanning tools validate the generated code

The key is integrating existing security scanning tools (SAST, DAST, PBT) directly into the build target that agents must execute after every code change.

```makefile
.PHONY: all build test security-scan

all: build test security-scan

build:
    @echo "Build completed successfully"
    @exit 0

test:
    @echo "Tests completed successfully" 
    @exit 0

security-scan:
    # Use your existing security scanning tool
    semgrep --config=auto src/
    bandit -r src/
    @exit $?
```

Configure agent instructions to mandate build execution:

```markdown
# Agent Instructions

## Code Quality 

After every code change, you MUST:

1. Run `make all` to verify that the code builds successfully and tests pass.
2. IMPORTANT: You MUST resolve any security issues identified during compilation.
```

```mermaid
graph TD
    A[Agent Generates Code] --> B[Run Build Target]
    B --> C{Security Scan Passes?}
    C -->|No| D[Security Tool Output in Context]
    D --> E[Agent Sees Error & Regenerates]
    E --> A
    C -->|Yes| F[Code Generation Complete]
```

## How to use it

1. **Inner Loop (Development)**:
   
- Integrate existing security scanning tools into your build target
   - Configure agent instructions to run build after every change
   - Let the agent see security tool output and iterate

2. **Outer Loop (CI/CD)**:
   
- Use the same security tools in your pull request checks
   - Maintain one unified rules database across both loops

3. **Implementation Steps**:
   
- Add security scanning tools to Makefile/package.json/build script
   - Update agent configuration (AGENTS.md/Cursor rules) to mandate build execution
   - Ensure security tools exit with non-zero codes on violations

## Trade-offs

**Pros:** 
- Leverages deterministic, battle-tested security tools
- Reuses existing security infrastructure and rules
- Works with any coding agent/harness
- Provides consistent security validation

**Cons:** 
- Increases build time and CI resource usage
- May produce false positives requiring human review
- Requires fast security tools for good developer experience

## References

* [Geoffrey Huntley's blog post on secure code generation](https://ghuntley.com/secure-codegen/)
* This generalizes beyond security to any code quality or pattern enforcement

---

## Discrete Phase Separation

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://claude.com/blog/building-companies-with-claude-code

## Problem

When AI agents attempt to simultaneously research, plan, and implement solutions, context contamination occurs. Competing priorities within a single conversation degrade output quality as the agent struggles to balance exploration, strategic thinking, and execution. This results in incomplete research, unclear plans, and suboptimal implementations.

## Solution

Break development workflows into isolated phases with clean handoffs between them. Each phase runs in a separate conversation with a fresh context window, focusing exclusively on its objective:

**Research Phase (Opus 4.1):**

- Deep exploration of requirements, existing code, and constraints
- Comprehensive background investigation
- No implementation concerns

**Planning Phase (Opus 4.1):**

- Create structured implementation roadmap
- Define clear steps and dependencies
- No coding distractions

**Implementation Phase (Sonnet 4.5):**

- Execute each plan step systematically
- Focus purely on code quality and functionality
- Leverage the distilled outputs from previous phases

**Key principle:** Pass only distilled conclusions between phases, not full conversation history. This prevents context pollution while maintaining necessary information flow.

```mermaid
graph LR
    A[Research Phase<br/>Opus 4.1] -->|Distilled Findings| B[Planning Phase<br/>Opus 4.1]
    B -->|Implementation Roadmap| C[Execution Phase<br/>Sonnet 4.5]

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#e8f5e9
```

## How to use it

**When to apply:**

- Complex features requiring significant background research
- Refactoring projects where understanding existing code is critical
- New codebases where architectural decisions need careful consideration
- Any task where mixing research and implementation degrades quality

**Implementation approach:**

1. **Research phase** - Start fresh conversation with Opus 4.1:
   
- "Research the authentication system and document all OAuth flows"
   - Compile findings into a structured document
   - Close conversation

2. **Planning phase** - New conversation with Opus 4.1:
   
- Provide distilled research findings (not full transcript)
   - "Create implementation plan for adding Google OAuth support"
   - Generate step-by-step roadmap
   - Close conversation

3. **Execution phase** - New conversation with Sonnet 4.5:
   
- Provide the implementation plan
   - "Implement step 1: Create OAuth configuration module"
   - Execute systematically through each step

**Prerequisites:**

- Clear handoff documents between phases
- Discipline to resist combining phases
- Understanding of which model strengths to leverage

## Trade-offs

**Pros:**

- Higher quality outputs in each phase due to focused attention
- Prevents context contamination from competing objectives
- Leverages model-specific strengths (Opus for reasoning, Sonnet for execution)
- Clearer mental model for complex projects
- Easier to debug which phase introduced issues

**Cons:**

- Requires more explicit phase management and handoffs
- May feel slower for simple tasks where single-pass is sufficient
- Requires discipline to maintain phase boundaries
- Information loss risk if handoffs are poorly structured
- Higher total token usage across multiple conversations

## References

- [Building Companies with Claude Code](https://claude.com/blog/building-companies-with-claude-code) - Sam Stettner (Ambral) emphasizes: "Don't make Claude do research while it's trying to plan, while it's trying to implement."
- Related patterns: [Sub-Agent Spawning](sub-agent-spawning.md), [Plan-Then-Execute Pattern](plan-then-execute.md)

---

## Disposable Scaffolding Over Durable Features

**Status:** best-practice
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.sourcegraph.com

## Problem

In a field where foundation models improve dramatically every few months, investing significant engineering effort into building complex, durable features *around* the model is extremely risky. A feature that takes three months to build, such as a sophisticated context compression or a custom tool-chain for code editing, could be rendered obsolete overnight by the next model generation that performs the task natively.

## Solution

Adopt a "scaffolding" mindset when building tooling and workflows for an agent. Treat most of the code written around the core model as temporary, lightweight, and disposable—like wooden scaffolding around a building under construction.

- **Embrace "The Bitter Lesson":** Acknowledge that a lot of complex scaffolding will eventually "fall into the model" as its capabilities grow.
- **Prioritize Speed:** Build the simplest possible solution that works *now*, with the assumption that it will be thrown away later. This maximizes the team's ability to react to new models.
- **Avoid Over-Engineering:** Resist the urge to build scalable, robust, long-term solutions for problems that a better model could solve inherently. Focus engineering efforts on the unique value proposition that isn't directly tied to compensating for a model's current weaknesses.

This approach keeps the product nimble and ensures that development resources are focused on adapting to the frontier of AI capabilities, rather than maintaining features that are destined for obsolescence.

## Example

```mermaid
flowchart TD
    A[New Model Release] --> B{Evaluate Current Scaffolding}
    B -->|Obsolete| C[Discard Old Tools]
    B -->|Still Needed| D[Keep Minimal Scaffolding]
    C --> E[Rebuild Lightweight Solution]
    D --> F[Adapt to New Capabilities]
    E --> G[Focus on Core Value]
    F --> G
    G --> H[Wait for Next Model]
    H --> A
```

## References

- Described by Thorsten Ball: "What you want is... a scaffolding. Like you want to build a scaffolding around the model, a wooden scaffolding that if the model gets better or you have to switch it out, the scaffolding falls away. You know, like the bitter lesson like embrace that a lot of stuff might fall into the model as soon as the model gets better."

---

## Distributed Execution with Cloud Workers

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://claude.com/blog/building-companies-with-claude-code

## Problem

Single-session AI agent execution cannot scale to meet enterprise team demands. Complex projects require multiple simultaneous code changes across different parts of the codebase, but coordinating multiple agents introduces challenges around communication, conflict resolution, merge coordination, and infrastructure management.

## Solution

Implement a distributed execution framework that runs multiple Claude Code sessions in parallel using git worktrees and cloud-based worker infrastructure. This enables team-scale AI code generation with proper synchronization and conflict management.

**Core architecture:**

**Git worktrees for isolation:**

- Each agent session runs in dedicated worktree
- Parallel development without checkout conflicts
- Independent file system views of the repository

**Cloud worker deployment:**

- Agent sessions execute on remote infrastructure
- Scale horizontally based on workload
- Centralized task distribution and coordination

**Synchronization layer:**

- Merge conflict detection and resolution
- Inter-agent communication protocols
- Shared state management for coordination

**Human oversight integration:**

- Approval gates for risky operations (see [Human-in-the-Loop Approval Framework](human-in-loop-approval-framework.md))
- Centralized monitoring dashboard
- Team notification channels (Slack, email)

```mermaid
graph TB
    subgraph "Control Plane"
        Coordinator[Task Coordinator]
        Monitor[Progress Monitor]
    end

    subgraph "Distributed Workers"
        W1[Worker 1<br/>Claude + Worktree A]
        W2[Worker 2<br/>Claude + Worktree B]
        W3[Worker 3<br/>Claude + Worktree C]
        WN[Worker N<br/>Claude + Worktree N]
    end

    subgraph "Git Repository"
        Main[main branch]
        WT1[worktree-1]
        WT2[worktree-2]
        WT3[worktree-3]
        WTN[worktree-n]
    end

    Coordinator -->|Assign tasks| W1
    Coordinator -->|Assign tasks| W2
    Coordinator -->|Assign tasks| W3
    Coordinator -->|Assign tasks| WN

    W1 -.->|Works in| WT1
    W2 -.->|Works in| WT2
    W3 -.->|Works in| WT3
    WN -.->|Works in| WTN

    W1 -->|Report progress| Monitor
    W2 -->|Report progress| Monitor
    W3 -->|Report progress| Monitor
    WN -->|Report progress| Monitor

    WT1 -->|Merge| Main
    WT2 -->|Merge| Main
    WT3 -->|Merge| Main
    WTN -->|Merge| Main
```

## How to use it

**When to apply:**

- Team-wide code migrations or refactoring
- Parallel feature development across multiple services
- Large-scale testing infrastructure changes
- Framework upgrades affecting many files
- Organizations with high AI agent adoption

**Example workflow (HumanLayer's CodeLayer):**

1. **Task decomposition:**
   - Break project into parallelizable units
   - Assign each unit to worker session
   - Define dependencies and ordering constraints

2. **Worker deployment:**
   - Provision cloud workers (AWS, GCP, etc.)
   - Initialize git worktrees for each worker
   - Configure agent sessions with task contexts

3. **Parallel execution:**
   - Workers execute independently
   - Progress reported to central monitor
   - Conflicts flagged for resolution

4. **Synchronization:**
   - Coordinate merge order based on dependencies
   - Resolve conflicts with human assistance when needed
   - Integrate results into main branch

**Prerequisites:**

- Git worktree infrastructure
- Cloud compute resources
- Task coordination system
- Merge conflict resolution strategy
- Team communication channels

**Related patterns:**

Extends [Sub-Agent Spawning](sub-agent-spawning.md) and [Swarm Migration Pattern](swarm-migration-pattern.md) to cloud infrastructure with team coordination.

## Trade-offs

**Pros:**

- Massive parallelization (10x-100x speedup for suitable tasks)
- Scales to enterprise team needs
- Centralizes agent management and monitoring
- Enables team-wide AI adoption
- Reduces bottlenecks in large migrations

**Cons:**

- Significant infrastructure complexity
- Merge conflict management overhead
- Coordination logic development required
- Higher cost from parallel model usage
- Requires sophisticated orchestration system
- Network latency for cloud workers

## References

- [Building Companies with Claude Code](https://claude.com/blog/building-companies-with-claude-code) - HumanLayer's CodeLayer enables "teams run multiple Claude agent sessions in parallel"
- [HumanLayer Documentation](https://docs.humanlayer.dev/) - Framework for human-in-the-loop agent coordination
- Related patterns: [Sub-Agent Spawning](sub-agent-spawning.md), [Swarm Migration Pattern](swarm-migration-pattern.md), [Human-in-the-Loop Approval Framework](human-in-loop-approval-framework.md)

---

## Dogfooding with Rapid Iteration for Agent Improvement

**Status:** best-practice
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
Developing effective AI agents requires understanding real-world usage and quickly identifying areas for improvement. External feedback loops can be slow, and simulated environments may not capture all nuances.

## Solution
The development team extensively uses their own AI agent product ("dogfooding") for their daily software development tasks. This provides:

1.  **Direct, Immediate Feedback:** Developers encounter the agent's strengths and weaknesses firsthand.
2.  **Real-World Problem Solving:** The agent is tested on actual, complex development problems faced by the team.
3.  **Internal Experimentation:** The team can quickly try out new agent features or modifications on themselves.
4.  **Rapid Iteration:** Shortcomings identified through dogfooding can be rapidly addressed and new features prototyped and validated internally before wider release.
5.  **Honest Assessment:** The team can be brutally honest about a feature's utility if they themselves don't find it useful, leading to quick pivots or discarding ineffective ideas.

This creates a tight, high-velocity feedback loop where the agent is continuously improved based on the practical needs and experiences of its own creators.

## How to use it

- Encourage all members of the agent development team to use the agent as their primary tool for relevant tasks.
- Establish channels for easily reporting issues or suggesting improvements based on internal use.
- Prioritize fixing pain points experienced by the internal team.

## Real-world examples

### Cursor

Cursor's development team uses their own AI coding assistant as the primary development tool, creating a tight feedback loop.

### Anthropic Claude Code

Anthropic practices intensive "ant fooding" (their internal term for dogfooding) with Claude Code:

- **70-80% adoption**: Most technical Anthropic employees use Claude Code daily
- **High-velocity feedback**: Internal feedback channel receives posts every 5 minutes
- **Experimental features**: New features pushed to internal users first for rapid validation
- **Quick pivots**: Team can be "brutally honest" about feature utility since they're the users
- **Bottom-up innovation**: Major features (to-do lists, sub-agents, hooks, plugins) originated from internal team members solving their own problems

**Quote from Cat Wu (Claude Code PM):**

> "Internally over 70 or 80 percent of ants—technical Anthropic employees—use Claude Code every day. Every time we are thinking about a new feature, we push it out to people internally and we get so much feedback. We have a feedback channel. I think we get a post every five minutes. And so you get a really quick signal on whether people like it, whether it's buggy, or whether it's not good and we should unship it."

This creates a development culture where features are validated through actual daily use before external release, dramatically reducing the risk of building unwanted functionality.

## References

- Lukas Möller (Cursor) at 0:04:25: "I think Cursor is very much driven by kind of solving our own problems and kind of figuring out where we struggle solving problems and making Cursor better...experimenting a lot."
- Aman Sanger (Cursor) at 0:04:55: "...that's how we're able to move really quickly and building new features and then throwing away things that clearly don't work because we we can be really honest to ourselves of whether we find it useful. And then not have to ship it out to users... it just speeds up the iteration loop for for building features."
- Cat Wu (Anthropic): "Internally over 70 or 80 percent of ants use Claude Code every day... we get a post every five minutes."
- [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Dual LLM Pattern

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2506.08837

## Problem
A privileged agent that both sees untrusted text **and** wields tools can be coerced into dangerous calls.

## Solution
Split roles:

- **Privileged LLM:** Plans and calls tools but **never sees raw untrusted data**.  
- **Quarantined LLM:** Reads untrusted data but **has zero tool access**.  
- Pass data as **symbolic variables** or validated primitives; privileged side only manipulates references.

```pseudo
var1 = QuarantineLLM("extract email", text)  # returns $VAR1
PrivLLM.plan("send $VAR1 to boss")           # no raw text exposure
execute(plan, subst={ "$VAR1": var1 })
```

## How to use it

Email/calendar assistants, booking agents, API-powered chatbots.

## Trade-offs

* **Pros:** Clear trust boundary; compatible with static analysis.
* **Cons:** Complexity; debugging across two minds.

## References

* Willison, *Dual LLM Pattern* (Apr 2023); adopted in Beurer-Kellner et al., §3.1 (4).

---

## Dual-Use Tool Design

**Status:** best-practice
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

Building separate tools for humans and AI agents creates:

- **Maintenance overhead**: Two implementations of similar functionality
- **Inconsistent behavior**: Human tools work differently than agent tools
- **Learning curve**: Users must learn one interface, agents another
- **Feature drift**: Human and agent capabilities diverge over time
- **Testing burden**: Must validate both interfaces separately

## Solution

Design all tools to be **dual-use**—equally accessible and useful to both humans and AI agents. When a human can invoke a tool manually, the agent should be able to call it programmatically, and vice versa.

**Core principle**: "Everything you can do, Claude can do. There's nothing in between."

**Key characteristics of dual-use tools:**

1. **Same interface**: Humans and agents use identical APIs/commands
2. **Shared logic**: One implementation serves both use cases
3. **Composable**: Tools can be chained by either humans or agents
4. **Observable**: Both see the same outputs (transparency)
5. **Documented once**: Single source of truth for behavior

```pseudo
# Dual-use slash command example
define_slash_command("/commit") {
    steps: [
        "run linters",
        "generate commit message from git diff",
        "create commit with standard format"
    ],
    callable_by: ["human", "agent"],
    pre_allowed_tools: ["git add", "git commit"],
    model: "haiku"  # same for both
}

# Human invocation
$ /commit

# Agent invocation
agent.call_slash_command("/commit")
```

## How to use it

**Design principles:**

1. **Start with human ergonomics**: If it makes sense to humans, it usually makes sense to agents
2. **Make everything scriptable**: What humans can click, agents should be able to call
3. **Shared state visibility**: Both see the same terminal output, file changes, etc.
4. **Consistent permissions**: Same security rules apply to both

**Claude Code implementation examples:**

- **Slash commands**: `/commit`, `/pr`, `/feature-dev` work manually and in agent flows
- **Hooks**: Humans can trigger hooks manually; agents trigger them automatically
- **Bash mode**: `!command` visible to both human and agent in same terminal
- **Permissions**: Pre-allowed tools work the same whether human or agent invokes them

**Benefits observed:**

> "It's sort of elegant design for humans that translates really well to the models." —Boris Cherny

## Trade-offs

**Pros:**

- **Reduced maintenance**: One tool implementation serves both audiences
- **Consistency**: Identical behavior whether human or agent invokes
- **Shared improvements**: Optimizations benefit both use cases
- **Easier testing**: Single test suite validates both paths
- **Better UX**: Humans can replicate agent workflows manually
- **Transparency**: Agents use the same observable tools humans understand

**Cons:**

- **Design constraints**: Must satisfy both human ergonomics AND API cleanliness
- **May compromise optimization**: Separate tools could be more specialized
- **Complexity in edge cases**: Some behaviors might need conditional logic
- **Documentation challenge**: Must explain dual usage clearly

## References

* Boris Cherny: "Tools were built for engineers, but now it's equal parts engineers and models... everything is dual use."
* Boris Cherny: "I have a slash command for slash commit... I run it manually, but also Claude can run this for me. And this is pretty useful because we get to share this logic."
* Cat Wu: "Claude Code has access to everything that an engineer does at the terminal. Making them dual use actually makes the tools a lot easier to understand. Everything you can do, Claude can do. There's nothing in between."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Dynamic Code Injection (On-Demand File Fetch)

**Status:** established
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** Internal Practice

## Problem

During an interactive coding session, a user or agent may need to inspect or modify files **not originally loaded** into the main context. Manually copying/pasting entire files into the prompt is:

- Tedious and error-prone.
- Wastes tokens on boilerplate (e.g., large config files).
- Interrupts workflow momentum when switching between the editor and chat.

## Solution

Allow **on-demand file injection** via special syntax (e.g., `@filename` or `/load file`) that automatically:

**1. Fetches the requested file(s)** from disk or version control.
**2. Summarizes** or **extracts** only the relevant portions (e.g., function bodies or specific line ranges) if the file is large.
**3. Injects** that snippet into the agent's current context, seamlessly extending its "memory" for the ongoing task.

Concretely:

- A user types `/load src/components/Button.js:lines 10–50` or `@src/setup/db.js`.
- The agent's preprocessor intercepts this command, reads the specified file (or line range), and replaces the command with the file content (or trimmed snippet).
- The rest of the prompt remains unchanged, so the agent can continue reasoning without restarting the conversation.

## How to use it

- **Command Syntax Examples:**
  - `@path/to/file.ext` → loads entire file if < 2,000 tokens; otherwise runs a heuristic summarizer.
  - `/load path/to/file.ext:10-50` → loads exactly lines 10 through 50.
  - `/summarize path/to/test_spec.py` → runs a summary routine (e.g., extract docstrings + test names).

- **Implementation Steps:**
  1. Build a **listener** in your chat frontend or CLI that recognizes `@` and `/load` tokens.
  2. Map recognized tokens to file paths; verify permissions if outside project root.
  3. Read file text, run a **line-range parser** or **AST-based snippet extractor** if needed.
  4. Replace the token in the outgoing prompt with `/// BEGIN <filename> …content… /// END <filename>`.
  5. Forward the augmented prompt to the LLM for inference.

- **Common Pitfalls:**
  - Untrusted file paths: agent must validate that `@../../../etc/passwd` (for example) is disallowed.
  - Large injected files: if file > 4,096 tokens, automatically run a **summarizer sub-routine** to extract only function/method definitions.

## Trade-offs

- **Pros:**
  - Enables **interactive exploration** of code without leaving the chat environment.
  - Reduces human overhead: no manual copy/paste of code blocks.
  - Improves agent accuracy by ensuring the most relevant code is directly visible.

- **Cons/Considerations:**
  - Requires the chat interface (or a proxy server) to have **local file system access**.
  - Potential security risk: if the agent can load arbitrary files, it could exfiltrate sensitive credentials unless carefully sandboxed.
  - Summarization heuristics may omit subtle context (e.g., private helper functions).

## References

- Adapted from "Dynamic Context Injection" patterns (e.g., at-mention in Claude Code) for general coding-agent use.
- Common in AI-powered IDE plugins (e.g., GitHub Copilot X live code browsing).

---

## Dynamic Context Injection

**Status:** established
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/claude-code

## Problem

While layered configuration files provide good baseline context, agents often need specific pieces of information (e.g., contents of a particular file, output of a script, predefined complex prompt) on-demand during an interactive session. Constantly editing static context files or pasting large chunks of text into prompts is inefficient.

## Solution

Implement mechanisms for users to dynamically inject context into the agent's working memory during a session. Common approaches include:

- **File/Folder At-Mentions:** Allowing users to type a special character (e.g., `@`) followed by a file or folder path (e.g., `@src/components/Button.tsx` or `@app/tests/`). The agent then ingests the content of the specified file or a summary of the folder into its current context for the ongoing task.
- **Custom Slash Commands:** Enabling users to define reusable, named prompts or instructions in separate files (e.g., in `~/.claude/commands/foo.md`). These can be invoked with a slash command (e.g., `/user:foo`), causing their content to be loaded into the agent's context. This is useful for frequently used complex instructions or context snippets.

These methods allow for a more fluid and efficient way to provide targeted context exactly when needed.

## Example (context injection flow)
```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant FS as File System
    participant Commands as Command Store

    User->>Agent: Working on task...
    User->>Agent: @src/components/Button.tsx
    Agent->>FS: Read Button.tsx
    FS-->>Agent: File contents
    Agent->>Agent: Inject into context

    User->>Agent: /user:deployment
    Agent->>Commands: Load ~/.claude/commands/deployment.md
    Commands-->>Agent: Deployment instructions
    Agent->>Agent: Inject into context

    Agent-->>User: Continue with enriched context
```

## References
- Based on the at-mention and slash command features described in "Mastering Claude Code: Boris Cherny's Guide & Cheatsheet," section IV.

[Source](https://www.nibzard.com/claude-code)

---

## Egress Lockdown (No-Exfiltration Channel)

**Status:** established
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://simonwillison.net/2025/Jun/16/lethal-trifecta/

## Problem
Even with private-data access and untrusted inputs, attacks fail if the agent has **no way to transmit stolen data**. Many real-world fixes simply removed or filtered outbound channels.

## Solution
Implement an **egress firewall** for agent tools:

- Allow only specific domains, methods, or payload sizes.  
- Strip or hash content in any permitted outbound call.  
- Forbid dynamic link generation (e.g., `http://attacker.com/?q=<data>`).  
- Where external communication is essential, run it in a separate "dumb" worker that cannot see private data.

```bash
# Docker file example
RUN iptables -P OUTPUT DROP       # default-deny
RUN iptables -A OUTPUT -d api.mycompany.internal -j ACCEPT
```

## How to use it

* Place the agent inside a sandboxed VM or container with outbound rules.
* Provide needed APIs via an internal proxy; audit that proxy's request schema.
* Log any DROP events for forensic follow-up.

## Trade-offs

**Pros:** Drastically reduces high-impact leaks; easy to reason about.
**Cons:** Breaks legitimate integrations; requires proxy stubs for essential calls.

## References

* Multiple vendor post-mortems cited by Willison: Microsoft 365 Copilot, GitHub MCP, GitLab Duo Chatbot fixes all disabled egress paths as the first patch.

---

## Episodic Memory Retrieval & Injection

**Status:** validated-in-production
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://forum.cursor.com/t/agentic-memory-management-for-cursor/78021

## Problem
Stateless calls make agents forget prior decisions, causing repetition and shallow reasoning.

## Solution
Add a **vector-backed episodic memory store**:

1. After every episode, write a short "memory blob" (event, outcome, rationale) to the DB.  
2. On new tasks, embed the prompt, retrieve top-k similar memories, and inject as *hints* in the context.  
3. Apply TTL or decay scoring to prune stale memories.

## Trade-offs
**Pros:** richer continuity, fewer repeated mistakes.  
**Cons:** retrieval noise if memories aren't curated; storage cost.

## References
- Cursor "10x-MCP" persistent memory layer
- Windsurf Memories docs

---

## Explicit Posterior-Sampling Planner

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2504.20997

## Problem
Agents that rely on ad-hoc heuristics explore poorly, wasting tokens and API calls on dead ends.

## Solution
Embed a *fully specified* RL algorithm—Posterior Sampling for Reinforcement Learning (PSRL)—inside the LLM's reasoning:

- Maintain a Bayesian posterior over task models.  
- Sample a model, compute an optimal plan/policy, execute, observe reward, update posterior.  
- Express each step in natural language so the core LLM can carry it out with tool calls.

## How to use it
Wrap the algorithm in a reusable prompt template or code skeleton the LLM can fill.

## References
- Arumugam & Griffiths, *Toward Efficient Exploration by LLM Agents*

---

## Extended Coherence Work Sessions

**Status:** rapidly-improving
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/silent-revolution

## Problem
Early AI agents and models often suffered from a short "coherence window," meaning they could only maintain focus and context for a few minutes before their performance degraded significantly (e.g., losing track of instructions, generating irrelevant output). This limited their utility for complex, multi-stage tasks that require sustained effort over hours.

## Solution
Utilize AI models and agent architectures that are specifically designed or have demonstrably improved capabilities to maintain coherence over extended periods (e.g., several hours). This involves:

- Leveraging newer foundation models with larger context windows and better long-term memory.
- Implementing agentic architectures that can manage state and context effectively over time.
- Prioritizing this capability allows agents to undertake substantial projects, engage in prolonged problem-solving, and complete tasks that were previously infeasible due to coherence limitations.

The goal is to enable agents to work on tasks for as long as a human counterpart might, without a degradation in the quality or relevance of their work.

## Example (coherence over time)
```mermaid
gantt
    title Agent Coherence Capabilities Over Time
    dateFormat X
    axisFormat %s

    section Early Models
    Short coherence window (minutes) :done, early, 0, 300

    section Current Models
    Extended coherence (hours) :active, current, 300, 10800

    section Future Trend
    All-day coherence :future, 10800, 86400
```

## References
- Highlighted in "How AI Agents Are Reshaping Creation": "Every seven months, we're actually doubling the number of minutes that the AI can work and stay coherent... The latest models can maintain coherence for hours." This capability is described as a "qualitative shift."

[Source](https://www.nibzard.com/silent-revolution)

---

## Feature List as Immutable Contract

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents

## Problem

Long-running agents exhibit several failure modes when tasked with building complete applications:

- **Premature victory declaration**: Agent declares "done" after implementing a fraction of requirements
- **Scope creep via test deletion**: Agent "passes" tests by deleting or weakening them rather than fixing code
- **Hallucinated completeness**: Agent loses track of what was actually implemented versus planned
- **Feature drift**: Without a fixed specification, agents may substitute easier features for harder ones
- **Progress amnesia**: Across sessions, agents forget what's done vs. pending

## Solution

Define all features upfront in a structured, immutable format that agents can read but cannot meaningfully game:

**1. Comprehensive Feature Specification**

Create a JSON file with ALL required features before any implementation begins:

```json
{
  "features": [
    {
      "id": "auth-001",
      "category": "functional",
      "description": "New chat button creates fresh conversation",
      "steps": [
        "Click 'New Chat' button in sidebar",
        "Verify URL changes to new conversation ID",
        "Verify message input is empty and focused",
        "Verify no previous messages are displayed"
      ],
      "passes": false
    },
    {
      "id": "auth-002",
      "category": "functional",
      "description": "User can log out and session is cleared",
      "steps": [
        "Click user profile menu",
        "Click 'Log out' option",
        "Verify redirect to login page",
        "Verify protected routes are inaccessible"
      ],
      "passes": false
    }
  ]
}
```

**2. Immutability Constraints**

Enforce through prompt instructions:

- Agent MAY set `passes: true` after verification
- Agent MAY NOT delete features from the list
- Agent MAY NOT modify acceptance criteria/steps
- Agent MAY NOT mark features as "not applicable"

**3. Verification Requirements**

Features are only marked passing after:

- Implementation is complete
- Manual or automated testing confirms all steps pass
- Agent has actually exercised the feature (not just written code)

```mermaid
graph TD
    A[Feature List Created] --> B[All Features: passes=false]
    B --> C{Select Next Feature}
    C --> D[Implement Feature]
    D --> E[Test Feature]
    E --> F{All Steps Pass?}
    F -->|No| D
    F -->|Yes| G[Set passes=true]
    G --> H{More Features?}
    H -->|Yes| C
    H -->|No| I[Project Complete]

    style A fill:#e1f5fe
    style I fill:#c8e6c9
    style G fill:#fff9c4
```

## How to use it

**Creating effective feature lists:**

- Include 100-200+ features for complex applications
- Be specific in acceptance criteria (observable behaviors, not implementation details)
- Group by category (functional, UI, performance, security)
- Include edge cases and error handling as separate features
- Write features as a human tester would verify them

**Prompt enforcement:**

```
CRITICAL RULES:
1. You MUST NOT delete or modify any feature in feature-list.json
2. You MUST NOT edit the "steps" or "description" fields
3. You MAY ONLY change "passes" from false to true
4. You MUST actually test the feature before marking it passing
5. If a feature seems impossible, ask the user - do NOT skip it
```

**Verification tooling:**

- Use browser automation (Puppeteer, Playwright) for E2E verification
- Run features as a user would, not just unit tests
- Capture evidence (screenshots, logs) when marking features passing

## Trade-offs

**Pros:**

- Prevents premature victory declaration with clear completion criteria
- Creates immutable record of requirements that survives session boundaries
- Makes agent progress measurable (X of Y features passing)
- Eliminates "pass by deletion" attack vector
- Provides natural work queue for incremental sessions

**Cons:**

- Requires significant upfront investment in feature specification
- Not suitable for exploratory or research-oriented work
- May miss emergent requirements discovered during implementation
- Rigid format doesn't accommodate changing requirements
- Large feature lists can overwhelm agent context

**When to use:**

- Building complete applications with known requirements
- Projects spanning many agent sessions
- When agent accountability is important
- Replicable workflows (same feature list for multiple similar projects)

**When to avoid:**

- Exploratory prototyping
- Research tasks with unclear scope
- Small, single-session tasks
- Rapidly evolving requirements

## References

* [Anthropic Engineering: Effective Harnesses for Long-Running Agents](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents)
* Related: [Initializer-Maintainer Dual Agent Architecture](initializer-maintainer-dual-agent.md)
* Related: [Spec-as-Test Feedback Loop](spec-as-test-feedback-loop.md)

---

## Filesystem-Based Agent State

**Status:** established
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.anthropic.com/engineering/code-execution-with-mcp

## Problem

Many agent workflows are long-running or may be interrupted (by errors, timeouts, or user intervention). Keeping all intermediate state in the model's context window is fragile and doesn't persist across sessions. When failures occur or when agents hit context limits, work is lost and must restart from scratch.

## Solution

Agents persist intermediate results and working state to files in the execution environment. This creates durable checkpoints that enable workflow resumption, recovery from failures, and support for tasks that exceed single-session context limits.

**Core pattern:**

```python
# Agent writes intermediate state to files
def multi_step_workflow():
    # Check if previous work exists
    if os.path.exists("state/step1_results.json"):
        print("Resuming from step 1...")
        step1_data = json.load(open("state/step1_results.json"))
    else:
        print("Starting from beginning...")
        step1_data = perform_step1()
        with open("state/step1_results.json", "w") as f:
            json.dump(step1_data, f)

    # Continue with step 2
    if os.path.exists("state/step2_results.json"):
        print("Resuming from step 2...")
        step2_data = json.load(open("state/step2_results.json"))
    else:
        step2_data = perform_step2(step1_data)
        with open("state/step2_results.json", "w") as f:
            json.dump(step2_data, f)

    # Final step
    return perform_step3(step2_data)
```

**State organization:**

```
workspace/
├── state/
│   ├── step1_results.json
│   ├── step2_results.json
│   └── progress.txt
├── data/
│   ├── input.csv
│   └── processed.csv
└── logs/
    └── execution.log
```

## How to use it

**Best for:**

- Multi-step workflows with expensive operations (API calls, data processing)
- Long-running tasks that may exceed session limits
- Workflows that need recovery from transient failures
- Collaborative tasks where multiple agents or sessions build on previous work
- Batch processing jobs with checkpointing

**Implementation patterns:**

1. **Checkpoint after expensive operations:**

   ```python
   def process_large_dataset():
       checkpoint_file = "state/processed_rows.json"

       # Load progress if exists
       if os.path.exists(checkpoint_file):
           processed = json.load(open(checkpoint_file))
           start_row = len(processed)
       else:
           processed = []
           start_row = 0

       # Process from checkpoint
       for i, row in enumerate(data[start_row:]):
           result = expensive_operation(row)
           processed.append(result)

           # Checkpoint every 100 rows
           if (i + 1) % 100 == 0:
               with open(checkpoint_file, "w") as f:
                   json.dump(processed, f)

       return processed
   ```

2. **State file with metadata:**

   ```json
   {
     "workflow_id": "abc-123",
     "current_step": "data_processing",
     "completed_steps": ["data_fetch", "validation"],
     "last_update": "2024-01-15T10:30:00Z",
     "data": {
       "records_processed": 1500,
       "errors_encountered": 3
     }
   }
   ```

3. **Progress logging for visibility:**

   ```python
   def log_progress(step, status, details=None):
       with open("logs/progress.log", "a") as f:
           timestamp = datetime.now().isoformat()
           log_entry = f"{timestamp} | {step} | {status}"
           if details:
               log_entry += f" | {json.dumps(details)}"
           f.write(log_entry + "\n")
           print(log_entry)  # Also show in agent context
   ```

## Trade-offs

**Pros:**

- Enables workflow resumption after interruption
- Protects against data loss from transient failures
- Supports long-running tasks beyond single-session limits
- Allows inspection of intermediate results
- Facilitates debugging (can examine state at each checkpoint)
- Multiple agents can collaborate by reading/writing shared state

**Cons:**

- Agents must write checkpoint/recovery logic
- File I/O adds overhead to workflow execution
- Requires discipline around state naming and organization
- Stale state files can cause confusion if not cleaned up
- Concurrent access needs coordination (file locking, atomic writes)
- Execution environment needs persistent storage

**Operational considerations:**

- Define state file cleanup policies (retention period, automatic cleanup)
- Use atomic writes to prevent corruption (write to temp, then rename)
- Include timestamps and version info in state files
- Consider state file size limits (don't checkpoint massive datasets)
- Secure state files if they contain sensitive data

## References

* Anthropic Engineering: Code Execution with MCP (2024)
* Related: Episodic Memory pattern (for conversation-level persistence)

---

## Graph of Thoughts (GoT)

**Status:** emerging
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2308.09687

## Problem

Linear reasoning approaches like Chain-of-Thought (CoT) and even tree-based methods like Tree-of-Thoughts (ToT) have limitations when dealing with problems that require complex interdependencies between reasoning steps. Many real-world problems involve reasoning paths that merge, split, and recombine in ways that don't fit neatly into linear or tree structures. These problems need a more flexible approach that can represent arbitrary relationships between thoughts.

## Solution

Graph of Thoughts (GoT) extends reasoning frameworks by representing the thought process as a directed graph where:

- **Nodes** represent individual thoughts or reasoning states
- **Edges** represent transformations or reasoning steps between thoughts
- **Multiple paths** can lead to and from each node
- **Aggregation operations** can combine multiple thoughts
- **Backtracking** allows revisiting and refining previous thoughts

This enables operations like:
1. **Branching**: Generate multiple thoughts from one
2. **Aggregation**: Combine insights from multiple reasoning paths
3. **Refinement**: Improve thoughts based on later insights
4. **Looping**: Revisit and refine thoughts iteratively

## Example

```python
class GraphOfThoughts:
    def __init__(self, llm, max_thoughts=50):
        self.llm = llm
        self.max_thoughts = max_thoughts
        self.thought_graph = nx.DiGraph()
        self.thought_scores = {}
        
    def solve(self, problem):
        # Initialize with root thought
        root = self.generate_initial_thought(problem)
        self.add_thought(root, score=1.0)
        
        # Iteratively expand the graph
        while len(self.thought_graph) < self.max_thoughts:
            # Select promising thoughts to expand
            thoughts_to_expand = self.select_thoughts_for_expansion()
            
            for thought in thoughts_to_expand:
                # Generate new thoughts through different operations
                self.branch_thought(thought, problem)
                self.aggregate_related_thoughts(thought)
                self.refine_thought(thought, problem)
        
        # Find best solution path through the graph
        return self.extract_best_solution()
    
    def branch_thought(self, thought, problem):
        """Generate multiple new thoughts from current thought"""
        prompt = f"""
        Problem: {problem}
        Current thought: {thought.content}
        
        Generate 2-3 different ways to continue or branch this reasoning:
        """
        branches = self.llm.generate(prompt).split('\n')
        
        for branch in branches:
            new_thought = Thought(branch)
            self.add_thought(new_thought)
            self.add_edge(thought, new_thought, operation='branch')
    
    def aggregate_related_thoughts(self, thought):
        """Combine insights from multiple related thoughts"""
        # Find thoughts that could be meaningfully combined
        related = self.find_related_thoughts(thought)
        
        if len(related) >= 2:
            prompt = f"""
            Combine insights from these thoughts:
            {[t.content for t in related]}
            
            Create a unified thought that incorporates the best of each:
            """
            aggregated = self.llm.generate(prompt)
            new_thought = Thought(aggregated)
            self.add_thought(new_thought)
            
            # Add edges from all source thoughts
            for source in related:
                self.add_edge(source, new_thought, operation='aggregate')
    
    def refine_thought(self, thought, problem):
        """Improve a thought based on graph context"""
        # Get neighboring thoughts for context
        context = self.get_thought_context(thought)
        
        prompt = f"""
        Problem: {problem}
        Current thought: {thought.content}
        Related context: {context}
        
        Refine this thought to be more accurate/useful:
        """
        refined = self.llm.generate(prompt)
        
        if self.is_improvement(refined, thought.content):
            new_thought = Thought(refined)
            self.add_thought(new_thought)
            self.add_edge(thought, new_thought, operation='refine')
    
    def evaluate_thought(self, thought, problem):
        """Score a thought's quality and relevance"""
        prompt = f"""
        Problem: {problem}
        Thought: {thought.content}
        
        Rate this thought on:
        1. Relevance to problem (0-1)
        2. Logical correctness (0-1)
        3. Novelty/insight (0-1)
        4. Progress toward solution (0-1)
        
        Overall score (0-1):
        """
        evaluation = self.llm.generate(prompt)
        return self.parse_score(evaluation)
    
    def extract_best_solution(self):
        """Find the highest-scoring path through the graph"""
        # Use graph algorithms to find optimal path
        terminal_thoughts = [n for n in self.thought_graph.nodes() 
                           if self.thought_graph.out_degree(n) == 0]
        
        best_path = None
        best_score = -1
        
        for terminal in terminal_thoughts:
            paths = nx.all_simple_paths(
                self.thought_graph, 
                source=self.get_root(), 
                target=terminal
            )
            for path in paths:
                score = self.score_path(path)
                if score > best_score:
                    best_score = score
                    best_path = path
        
        return self.format_solution(best_path)
```

```mermaid
graph TD
    A[Initial Problem] --> B[Thought 1]
    A --> C[Thought 2]
    
    B --> D[Thought 3]
    B --> E[Thought 4]
    C --> F[Thought 5]
    C --> G[Thought 6]
    
    D --> H[Refined Thought 3]
    E --> I[Thought 7]
    F --> I
    
    I --> J[Aggregated Insight]
    G --> J
    H --> J
    
    J --> K[Final Solution]
    
    style A fill:#ffebee,stroke:#d32f2f,stroke-width:2px
    style J fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style K fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
```

## Benefits

- **Flexibility**: Can represent complex, non-linear reasoning patterns
- **Reusability**: Thoughts can be referenced and built upon multiple times
- **Robustness**: Multiple paths to solution increase success probability
- **Insight Aggregation**: Combines best aspects of different reasoning paths

## Trade-offs

**Pros:**
- Handles complex problems with interdependent reasoning steps
- Can discover non-obvious connections between ideas
- Supports iterative refinement and backtracking
- More expressive than linear or tree-based approaches

**Cons:**
- Significantly higher computational cost
- Complex to implement and debug
- May generate many redundant thoughts
- Requires sophisticated scoring and path-finding algorithms
- Can be overkill for simple problems

## References

- [Graph of Thoughts: Solving Elaborate Problems with Large Language Models (AAAI 2024)](https://arxiv.org/abs/2308.09687)
- [Presentation at AAAI '24 Vancouver](https://aaai.org/aaai-conference/)
- [Code Implementation](https://github.com/spcl/graph-of-thoughts)

---

## Human-in-the-Loop Approval Framework

**Status:** validated-in-production
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://claude.com/blog/building-companies-with-claude-code

## Problem

Autonomous AI agents need to execute high-risk or irreversible operations (database modifications, production deployments, system configurations, API calls) but allowing unsupervised execution creates unacceptable safety and compliance risks. Blocking all such operations defeats the purpose of agent automation.

## Solution

Systematically insert human approval gates for designated high-risk functions while maintaining agent autonomy for safe operations. Create lightweight feedback loops that enable time-sensitive human decisions without blocking the entire agent workflow.

**Core components:**

**Risk classification:**

- Identify functions that require human approval
- Define approval criteria (cost thresholds, data sensitivity, reversibility)
- Categorize operations by risk level

**Multi-channel approval interface:**

- Slack integration for real-time notifications
- Email for asynchronous approvals
- SMS for urgent/critical operations
- Web dashboard for batch reviews

**Approval workflow:**

- Agent requests permission before executing risky function
- Human receives context-rich approval request
- Quick approve/reject/modify decision
- Agent proceeds or adapts based on response

**Audit trail:**

- Log all approval requests and responses
- Track who approved what and when
- Enable compliance and debugging

```mermaid
sequenceDiagram
    participant Agent as AI Agent
    participant Framework as Approval Framework
    participant Slack as Slack Channel
    participant Human as Human Reviewer
    participant DB as Database

    Agent->>Framework: Request: DROP old_users table
    Framework->>Framework: Classify as HIGH RISK
    Framework->>Slack: Send approval request with context
    Slack->>Human: Notification with approve/reject buttons

    alt Approved
        Human->>Slack: Click "Approve"
        Slack->>Framework: Approval granted
        Framework->>Agent: Permission granted
        Agent->>DB: Execute DROP table
        Framework->>Framework: Log approval & execution
    else Rejected
        Human->>Slack: Click "Reject + provide reason"
        Slack->>Framework: Rejection + context
        Framework->>Agent: Permission denied + reason
        Agent->>Agent: Adapt plan (skip or find alternative)
    end
```

## How to use it

**When to apply:**

- Production database operations (DELETE, DROP, ALTER)
- External API calls with side effects (payments, emails, webhooks)
- System configuration changes (firewall rules, permissions)
- Destructive file operations (bulk deletes, overwrites)
- Compliance-sensitive operations (GDPR, HIPAA, SOC2)

**Implementation example (HumanLayer approach):**

**1. Instrument risky functions:**

```python
from humanlayer import HumanLayer

hl = HumanLayer()

@hl.require_approval(channel="slack")
def delete_user_data(user_id: str):
    """Delete all data for user - requires approval"""
    return db.users.delete(user_id)

# Agent calls function normally
delete_user_data("user_123")
# Execution pauses, approval request sent to Slack
# Resumes after human approval/rejection
```

**2. Configure approval channels:**

```yaml
approval_channels:
  high_risk:
    
- slack: "#agent-approvals"
    - sms: "+1234567890"  # For urgent
  medium_risk:
    
- slack: "#agent-review"
  low_risk:
    
- email: "team@company.com"
```

**3. Set context requirements:**

- Why is the operation needed?
- What data will be affected?
- Is it reversible?
- What are the alternatives?

**Prerequisites:**

- Integration with communication platforms (Slack, email, SMS)
- Clear risk classification criteria
- Fast human response time for time-sensitive operations
- Fallback strategies when approval is denied

## Trade-offs

**Pros:**

- Enables safe autonomous execution of risky operations
- Maintains human oversight where it matters most
- Lightweight integration (Slack buttons, not complex UIs)
- Audit trail for compliance and debugging
- Reduces agent anxiety about mistakes
- Allows gradual trust expansion over time

**Cons:**

- Requires human availability and responsiveness
- Can bottleneck agent workflows if approvals are slow
- Infrastructure complexity (notification systems, state management)
- Risk of approval fatigue leading to rubber-stamping
- Requires clear classification of what needs approval
- May interrupt human focus with frequent requests

## References

- [Building Companies with Claude Code](https://claude.com/blog/building-companies-with-claude-code) - HumanLayer's core product coordinates agent actions with "human approval steps" via Slack
- [HumanLayer Documentation](https://docs.humanlayer.dev/) - Framework and examples for human-in-the-loop agent workflows
- [12-Factor Agents](https://github.com/humanlayer/12-factor-agents) - Principles for production agent systems including human oversight patterns
- Related patterns: [Spectrum of Control / Blended Initiative](spectrum-of-control.md), [Chain-of-Thought Monitoring & Interruption](chain-of-thought-monitoring-interruption.md)

---

## Hybrid LLM/Code Workflow Coordinator

**Status:** proposed
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://lethain.com/agents-coordinators/

## Problem

LLM-driven workflows are **non-deterministic**—even well-crafted prompts can produce unpredictable results. For some tasks (e.g., adding emoji to Slack messages based on PR status), occasional errors are unacceptable. But LLM workflows are fast to prototype and work well for many cases. You need both: **flexibility when experimenting** and **determinism when it matters**.

## Solution

Support both LLM-driven and code-driven workflows via a **configurable coordinator** parameter. Start with LLM for rapid prototyping, then migrate to code when you need determinism.

**Coordinator configuration:**

```yaml
# LLM-driven (default, fastest to iterate)
coordinator: llm

# Code-driven (deterministic, goes through code review)
coordinator: script
coordinator_script: scripts/pr_merged.py
```

**When `coordinator: llm`:**
- Handler selects configuration based on trigger
- Loads prompt, tools, virtual files
- LLM decides which tools to call
- Handler coordinates tool calls based on LLM responses

**When `coordinator: script`:**
- Custom Python script controls workflow
- Same access to tools, trigger data, virtual files as LLM
- Can invoke LLM via subagent tool when explicitly needed
- Goes through code review like any other software

**Progressive enhancement approach:**

1. **Start with LLM** - Fast to prototype, works for many cases
2. **Observe failures** - Track where non-determinism causes problems
3. **Rewrite to script** - Use Claude Code to one-shot rewrite prompt → code
4. **Ship with confidence** - Code goes through review, deterministic behavior

```mermaid
graph LR
    A[Trigger] --> B{Coordinator Type}
    B -->|llm| C[LLM Orchestrates]
    B -->|script| D[Script Orchestrates]
    C --> E[Tool Calls]
    D --> E
    D -.optional.-> F[Subagent LLM]
    E --> G[Result]

    style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
```

## How to use it

**Implementation:**

```python
# Handler that supports both coordinators
def execute_workflow(trigger, config):
    if config.get("coordinator") == "script":
        # Code-driven: deterministic, goes through review
        script = config["coordinator_script"]
        return run_python_script(script, trigger)
    else:
        # LLM-driven: flexible, fast iteration
        prompt = load_prompt(config["prompt"])
        tools = load_tools(config["tools"])
        return llm_orchestrate(trigger, prompt, tools)
```

**Script has same capabilities as LLM:**

```python
# scripts/pr_merged.py
def handler(trigger, tools, virtual_files, subagent):
    # Same tools, trigger data, virtual files as LLM
    messages = tools.slack.get_messages(limit=10)

    pr_urls = extract_pr_urls(messages)
    statuses = [tools.github.get_status(url) for url in pr_urls]

    for msg, status in zip(messages, statuses):
        if status in ["merged", "closed"]:
            tools.slack.add_reacji(msg, "merged")

    # Can use LLM selectively if needed
    # summary = subagent.summarize(statuses)
```

**When to use each:**

| Use **LLM-driven** when... | Use **Code-driven** when... |
|----------------------------|----------------------------|
| Prototyping new workflow | Errors are unacceptable |
| Logic may change frequently | Determinism required |
| Happy to tolerate occasional failures | Workflow is stable |
| Fast iteration matters | Need code review process |

**Migration path:**

1. Prototype with `coordinator: llm`
2. Deploy and observe failure modes
3. When failures are problematic, ask Claude Code: "Rewrite this workflow as a script"
4. Update config to `coordinator: script`
5. PR goes through review, merge with confidence

## Trade-offs

**Pros:**

- **Best of both worlds**: LLM flexibility when prototyping, code determinism when mature
- **Easy migration**: One-shot rewrite from prompt → script
- **Same capabilities**: Scripts have access to all tools, can still use LLM via subagent
- **Code review**: Critical workflows go through standard review process
- **Progressive enhancement**: Don't over-engineer from the start

**Cons:**

- **Two code paths**: Need to maintain both LLM and script handlers
- **Rewrite cost**: Time investment to migrate from LLM → script
- **Less dynamic**: Scripts are harder to change than prompts

**When NOT to use:**

- Purely deterministic tasks (just use code from start)
- Highly exploratory tasks where LLM is always needed

## References

* [Building an internal agent: Code-driven vs LLM-driven workflows](https://lethain.com/agents-coordinators/) - Will Larson (Imprint, 2025)
* Related: Code Mode MCP Tool Interface, Deterministic Security Scanning Build Loop

---

## Inference-Healed Code Review Reward

**Status:** proposed
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=Xkwok_XXQgw

## Problem

Simple reward functions that only check for "all tests passed" fail to capture nuanced code quality issues (e.g., performance regressions, style violations, missing edge-case handling). A single binary signal at the end cannot guide the agent to produce maintainable, high-quality code.

- Verifying **only** final correctness misses suboptimal commits (e.g., a patch that removes error handling but still passes tests).
- Reward models that produce a single scalar lack **explainability** to tell the agent which aspect of the code needs improvement.

## Solution

Use an **inference-healed reward model**—a code-review critic that:

**1. Decomposes Code Quality into Subcriteria**
- **Correctness:** Does the code pass all existing and newly added tests?
- **Style:** Are linters (e.g., ESLint, pylint) satisfied (zero or minimal warnings)?
- **Performance:** Are there clear performance regressions gauged by simple benchmarks?
- **Security:** Does a static analyzer (e.g., Bandit, SonarQube) flag no critical issues?

**2. Runs Internal Chain-of-Thought (CoT) Reasoning**
- If uncertain about a subcriterion (e.g., performance), the critic runs a short CoT inside itself:
  ```text
  "Step: performance check. Baseline runtime: 50ms. New code runtime: 65ms. 
  Regression > 20%. Score: 0.4."  
  ```
- This "inference healing" allows the reward model to **explain** each sub-score.

**3. Aggregates Subscores**
- Each subcriterion returns a float ∈ [0, 1].
- A weighted sum (e.g., 0.4 × correctness + 0.2 × style + 0.2 × performance + 0.2 × security) yields the final code-review score.

**4. Generates Human-Readable Feedback**
- Alongside a numerical score, return a short analysis:
  ```json
  {
    "correctness": 1.0,
    "style": 0.8,
    "performance": 0.4,
    "security": 0.6,
    "comments": "Performance regression due to O(n²) loop."
  }
  ```

## Example

```python
# Pseudo-code for one code-review reward invocation
subscores = {
    "correctness": test_critic.score(patch),
    "style": linter_critic.score(patch),
    "performance": perf_critic.score(patch),
    "security": security_critic.score(patch),
}
final_score = sum(weight[k]*subscores[k] for k in subscores)
return final_score, subscores, comments
```

## How to use it

- **Critic Dataset Collection:** Gather examples of good vs. bad code patches, labeled along each subcriterion.
- **Critic Training:** Fine-tune a small LLM (e.g., 1–2 B parameters) to produce sub-scores and CoT justifications.
- **Integration into RL Loop:** Replace or augment the existing binary "tests-passed" reward with `inference_healed_reward(patch)`.
- **Human-in-the-Loop Checkpoints:** If a patch is borderline (e.g., final_score ∈ [0.5, 0.7]), route it for manual code review to generate better labels for future training.

## Trade-offs

- **Pros:**
  - **Explainable Feedback:** The agent knows *why* a patch scored poorly, allowing targeted improvements.
  - **Higher Code Quality:** Incorporates non-functional criteria (performance, security), leading to more robust code.
- **Cons/Considerations:**
  - **Compute Overhead:** Each reward invocation may involve running tests, linters, benchmarks, and a static analysis, adding latency.
  - **Critic Maintenance:** As coding standards or security rules evolve, retrain or update the critic models and rubrics.

## References

- Derived from "inference healing" in reward modeling, as discussed in the Open Source Agent RL talk (May 2025) and by Will Brown (Prime Intellect).
- Similar principles in "Criterion-Led Reward Models" (DeepMind blog, April 2025).

---

## Inference-Time Scaling

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://deepmind.google/research/

## Problem

Traditional language models are limited by their training-time capabilities. Once trained, their performance is essentially fixed, regardless of how much compute is available at inference time. This means that for particularly challenging problems, we cannot simply "think harder" by allocating more computational resources to find better solutions. This limitation becomes especially apparent in complex reasoning tasks where more deliberation could lead to better outcomes.

## Solution

Inference-Time Scaling allocates additional computational resources during inference to improve output quality. Instead of generating a single response, the system can:

1. **Generate multiple candidates** and select the best one
2. **Perform extended reasoning** chains before responding  
3. **Iterate and refine** outputs through multiple passes
4. **Search through solution spaces** more thoroughly
5. **Verify and validate** answers before returning them

This approach trades compute time for solution quality, allowing smaller models with inference-time scaling to outperform larger models using standard inference.

## Example

```python
class InferenceTimeScalingAgent:
    def __init__(self, base_model, scaling_strategy='adaptive'):
        self.base_model = base_model
        self.scaling_strategy = scaling_strategy
        
    def solve_with_scaling(self, problem, max_compute_budget=100):
        if self.scaling_strategy == 'adaptive':
            # Estimate problem difficulty
            difficulty = self.estimate_difficulty(problem)
            compute_budget = min(difficulty * 10, max_compute_budget)
        else:
            compute_budget = max_compute_budget
            
        # Try different scaling approaches based on budget
        solutions = []
        
        # Approach 1: Multiple attempts with different prompting
        if compute_budget >= 10:
            solutions.extend(self.multiple_attempts(problem, n=5))
        
        # Approach 2: Step-by-step reasoning with verification
        if compute_budget >= 30:
            solutions.append(self.deep_reasoning(problem))
        
        # Approach 3: Solution space search
        if compute_budget >= 50:
            solutions.append(self.solution_search(problem))
        
        # Select best solution
        return self.select_best_solution(solutions, problem)
    
    def multiple_attempts(self, problem, n=5):
        """Generate multiple solutions with different approaches"""
        prompting_strategies = [
            "Think step by step",
            "Consider edge cases",
            "Work backwards from the goal",
            "Break into subproblems", 
            "Use analogical reasoning"
        ]
        
        solutions = []
        for i in range(n):
            strategy = prompting_strategies[i % len(prompting_strategies)]
            prompt = f"{strategy}: {problem}"
            solution = self.base_model.generate(prompt)
            solutions.append({
                'solution': solution,
                'strategy': strategy,
                'confidence': self.evaluate_confidence(solution)
            })
        
        return solutions
    
    def deep_reasoning(self, problem):
        """Extended chain-of-thought with self-verification"""
        # Initial reasoning
        cot_prompt = f"""
        Problem: {problem}
        
        Let me work through this systematically:
        1. Understanding: What exactly is being asked?
        2. Approach: What methods could work?
        3. Solution: Work through the chosen approach
        4. Verification: Double-check the answer
        """
        
        initial_solution = self.base_model.generate(cot_prompt)
        
        # Self-critique
        critique_prompt = f"""
        Problem: {problem}
        Proposed solution: {initial_solution}
        
        Critically evaluate this solution:
        
- Are there any errors?
        - What assumptions were made?
        - Could the approach be improved?
        """
        
        critique = self.base_model.generate(critique_prompt)
        
        # Refined solution
        refine_prompt = f"""
        Problem: {problem}
        Initial solution: {initial_solution}
        Critique: {critique}
        
        Provide an improved solution addressing the critique:
        """
        
        refined = self.base_model.generate(refine_prompt)
        
        return {
            'solution': refined,
            'strategy': 'deep_reasoning',
            'iterations': 3,
            'confidence': self.evaluate_confidence(refined)
        }
    
    def solution_search(self, problem):
        """Search through solution space systematically"""
        # This could implement beam search, A*, or other algorithms
        # Simplified example using breadth-first exploration
        
        candidates = PriorityQueue()
        initial = self.generate_initial_solutions(problem, n=3)
        
        for sol in initial:
            candidates.put((-self.score_solution(sol), sol))
        
        best_solution = None
        iterations = 0
        
        while not candidates.empty() and iterations < 10:
            score, current = candidates.get()
            
            if self.is_complete_solution(current, problem):
                if best_solution is None or score < best_solution[0]:
                    best_solution = (score, current)
            
            # Generate variations
            variations = self.generate_variations(current, problem)
            for var in variations:
                var_score = self.score_solution(var)
                candidates.put((-var_score, var))
            
            iterations += 1
        
        return {
            'solution': best_solution[1] if best_solution else current,
            'strategy': 'solution_search', 
            'iterations': iterations,
            'candidates_explored': iterations * len(variations)
        }
```

```mermaid
flowchart TD
    A[Input Problem] --> B{Assess Difficulty}
    
    B -->|Low| C[Standard Inference]
    B -->|Medium| D[Multiple Attempts]
    B -->|High| E[Deep Reasoning]
    B -->|Very High| F[Solution Search]
    
    D --> G[Generate N Solutions]
    G --> H[Score Each]
    
    E --> I[Initial CoT]
    I --> J[Self-Critique]
    J --> K[Refinement]
    
    F --> L[Generate Candidates]
    L --> M[Explore Space]
    M --> N[Iterative Improvement]
    
    H --> O[Select Best]
    K --> O
    N --> O
    C --> O
    
    O --> P[Final Answer]
    
    style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style O fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
```

## Real-World Evidence

- **Google DeepMind (August 2024)**: Research showing that inference-time compute scaling allows smaller models to outperform 14x larger models
- **OpenAI's o1 model**: Implements "chain of thought reasoning" with extended inference time, showing significant improvements on complex tasks
- Models can dynamically adjust compute based on problem difficulty, spending more time on harder problems

## Trade-offs

**Pros:**
- Can dramatically improve performance on complex tasks
- More cost-effective than training larger models
- Allows dynamic resource allocation based on task difficulty
- Enables "System 2" thinking in AI systems

**Cons:**
- Increased latency for responses
- Higher inference costs for complex problems
- Diminishing returns beyond certain compute thresholds
- Not beneficial for simple tasks
- Requires careful tuning of scaling strategies

## References

- [Google DeepMind Research on Test-Time Compute Scaling (August 2024)](https://deepmind.google/research/)
- [OpenAI o1 System Card](https://openai.com/research/)
- [Inference-Time Scaling Laws](https://arxiv.org/)

---

## Initializer-Maintainer Dual Agent Architecture

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents

## Problem

Long-running agent projects face distinct failure modes at different lifecycle stages:

- **Project initialization** requires comprehensive setup: environment configuration, feature specification, test infrastructure, and progress tracking systems
- **Incremental development** requires reading prior context, selecting the next task, implementing, and verifying—repeatedly across many sessions
- Single-agent approaches either over-engineer each session (wasting setup time) or under-invest in foundations (leading to drift and confusion)
- Session boundaries create context loss, causing agents to restart from scratch or lose track of project state

## Solution

Implement a two-agent architecture with lifecycle-specialized responsibilities:

**1. Initializer Agent** (runs once at project start):

- Creates comprehensive feature list with all requirements
- Establishes progress tracking artifacts (e.g., `progress.txt`)
- Sets up environment bootstrap scripts (e.g., `init.sh`)
- Configures testing infrastructure
- Makes first git commit with foundational structure

**2. Maintainer/Coding Agent** (runs in subsequent sessions):

- Executes session bootstrapping ritual:
  1. Verify working directory (`pwd`)
  2. Read git logs and progress files
  3. Read feature list and select next incomplete feature
  4. Run bootstrap script to start services
  5. Run existing tests before implementing new features
- Works on one feature at a time
- Commits after each verified feature
- Updates progress artifacts

```mermaid
sequenceDiagram
    participant Init as Initializer Agent
    participant FS as Filesystem
    participant Coding as Coding Agent
    participant Git as Git Repository

    Note over Init: Runs ONCE at project start
    Init->>FS: Create feature-list.json (200+ items, all "passes: false")
    Init->>FS: Create init.sh (environment bootstrap)
    Init->>FS: Create progress.txt (empty log)
    Init->>Git: Initial commit with foundation

    Note over Coding: Runs in EACH subsequent session
    loop Session N
        Coding->>Git: Read git log for context
        Coding->>FS: Read progress.txt
        Coding->>FS: Read feature-list.json
        Coding->>Coding: Select highest-priority incomplete feature
        Coding->>FS: Run init.sh (start servers)
        Coding->>Coding: Run E2E tests (verify baseline)
        Coding->>Coding: Implement single feature
        Coding->>Coding: Verify feature passes
        Coding->>FS: Update progress.txt
        Coding->>Git: Commit changes
    end
```

## How to use it

**Best for:**

- Projects requiring many sessions to complete (days/weeks of agent work)
- Complex applications with 50+ discrete features
- Scenarios where context loss between sessions is costly
- Teams using agents for sustained development, not one-shot tasks

**Implementation steps:**

1. **Design the Initializer prompt**: Focus on comprehensive upfront planning
   - Feature list with detailed acceptance criteria
   - Environment setup automation
   - Progress tracking file structure

2. **Design the Coding Agent prompt**: Focus on incremental progress
   - Session bootstrapping ritual (read context first)
   - Single-feature focus
   - Mandatory testing before marking complete
   - Commit discipline

3. **Define the handoff artifacts**:
   ```
   project/
   ├── feature-list.json    # All features with pass/fail status
   ├── progress.txt         # Running log of decisions and work
   ├── init.sh              # One-command environment startup
   └── .git/                # Descriptive commits as context
   ```

## Trade-offs

**Pros:**

- Clear separation of concerns between setup and execution
- Session bootstrapping ritual prevents context loss
- Mirrors effective human team practices (shift handoffs)
- Initializer creates consistent foundation for all future sessions
- Git history + progress files provide rich context for new sessions

**Cons:**

- Requires upfront investment in comprehensive feature specification
- Two different prompts/configurations to maintain
- Initializer must anticipate needs of Coding Agent
- Not suitable for exploratory or research-oriented projects
- Overhead is wasteful for small, single-session tasks

## References

* [Anthropic Engineering: Effective Harnesses for Long-Running Agents](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents)
* Related: [Filesystem-Based Agent State](filesystem-based-agent-state.md)
* Related: [Proactive Agent State Externalization](proactive-agent-state-externalization.md)

---

## Inversion of Control

**Status:** validated-in-production
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/ampcode

## Problem
Traditional "prompt-as-puppeteer" workflows force humans to spell out every step, limiting scale and creativity.

## Solution
Give the agent **tools + a high-level goal** and let *it* decide the orchestration.
Humans supply guard-rails (first 10 % + last 3 %) while the agent handles the middle 87 %.

## Example (flow)
```mermaid
sequenceDiagram
  Dev->>Agent: "Refactor UploadService to async"
  Agent->>Repo: git grep "UploadService"
  Agent->>Tools: edit_file, run_tests
  Agent-->>Dev: PR with green CI
```

## References

* Raising An Agent - Episode 1, "It's a big bird, it can catch its own food."

[Source](https://www.nibzard.com/ampcode)

---

## Isolated VM per RL Rollout

**Status:** emerging
**Category:** Security & Safety
**Authors:** Nikola Balic (@nibzard)
**Source:** https://youtu.be/1s_7RMG4O4U

## Problem

During reinforcement learning training with tool-using agents, multiple rollouts execute simultaneously and may call destructive or stateful tools:

- **Cross-contamination**: One rollout's actions affect another rollout's environment
- **Destructive commands**: Agent might run `rm -rf`, corrupting shared state
- **State leakage**: File system changes persist across rollouts, creating inconsistent training data
- **Reward corruption**: If rollout B sees rollout A's side effects, reward signals become meaningless
- **Debugging nightmares**: Non-deterministic failures due to race conditions

Cognition faced this when training Devon's file planning agent: the agent had access to a `shell` tool that could run arbitrary commands like `grep`, `find`, or even `rm`. Running 32 parallel rollouts on shared infrastructure would cause chaos.

## Solution

**Spin up an isolated virtual machine (or container) for each RL rollout, ensuring complete environment isolation.**

**Architecture:**

1. **Rollout ID Tracking**: OpenAI's Agent RFT platform assigns unique IDs to each rollout
2. **VM/Container Mapping**: Your infrastructure maps rollout ID → dedicated VM
3. **Clean State**: Each VM starts fresh with identical filesystem, packages, and configuration
4. **Cleanup**: VMs are destroyed after rollout completes (success or failure)

**Key Components:**

- **VM Provisioning**: Fast VM creation (typically cloud instances or containers)
- **Bursty Scaling**: Handle 100s-500s of simultaneous VM requests at training step boundaries
- **State Isolation**: No shared filesystems or databases between VMs
- **Timeout Handling**: VMs auto-destroy after timeout to prevent resource leaks

```python
# Infrastructure setup (Cognition's approach)
from modal import Image, App, method
import uuid

# Base VM image with all dependencies
base_image = (
    Image.debian_slim()
    .apt_install("git", "build-essential")
    .pip_install("pandas", "numpy", "openai")
    .copy_local_dir("./corpus", "/workspace/corpus")  # Training data
)

app = App("agent-rft-tool-server")

@app.cls(
    image=base_image,
    cpu=2,
    memory=4096,
    timeout=600,  # 10 min per rollout max
)
class IsolatedToolExecutor:
    """
    Each instance gets its own isolated VM
    Spun up per-rollout during RL training
    """

    def __init__(self):
        """Initialize fresh state for this rollout"""
        self.rollout_id = None
        self.workspace = "/workspace"
        self.history = []

    @method()
    def initialize_rollout(self, rollout_id: str):
        """
        Called first when rollout starts
        Sets up isolated state for this specific rollout
        """
        self.rollout_id = rollout_id
        print(f"[{rollout_id}] Initialized isolated VM")

        # Create isolated working directory
        import os
        self.work_dir = f"{self.workspace}/rollout_{rollout_id}"
        os.makedirs(self.work_dir, exist_ok=True)

        return {"status": "ready", "rollout_id": rollout_id}

    @method()
    def execute_shell(self, rollout_id: str, command: str):
        """
        Execute shell command in isolated environment
        Safe because this VM is dedicated to this rollout
        """
        if rollout_id != self.rollout_id:
            raise ValueError(f"Rollout ID mismatch: {rollout_id} != {self.rollout_id}")

        import subprocess

        print(f"[{rollout_id}] Executing: {command}")

        # Even destructive commands are safe in isolated VM
        result = subprocess.run(
            command,
            shell=True,
            cwd=self.work_dir,
            capture_output=True,
            text=True,
            timeout=60
        )

        self.history.append({
            "command": command,
            "returncode": result.returncode,
            "stdout": result.stdout[:1000],  # Limit output
            "stderr": result.stderr[:1000]
        })

        return {
            "stdout": result.stdout,
            "stderr": result.stderr,
            "returncode": result.returncode
        }

    @method()
    def read_file(self, rollout_id: str, filepath: str):
        """Read file from corpus or workspace"""
        if rollout_id != self.rollout_id:
            raise ValueError(f"Rollout ID mismatch")

        # Files are isolated to this VM
        full_path = f"{self.workspace}/{filepath}"

        try:
            with open(full_path, 'r') as f:
                content = f.read()
            return {"content": content, "error": None}
        except Exception as e:
            return {"content": None, "error": str(e)}

    @method()
    def search_corpus(self, rollout_id: str, query: str):
        """Semantic search over documents"""
        if rollout_id != self.rollout_id:
            raise ValueError(f"Rollout ID mismatch")

        # Search implementation here...
        # Corpus is read-only, copied into VM at startup

        return {"results": [...]}

    @method()
    def cleanup(self, rollout_id: str):
        """
        Optional cleanup (Modal handles VM destruction automatically)
        """
        print(f"[{rollout_id}] Rollout complete, VM will be destroyed")
        return {"history": self.history}


# Tool endpoint configuration for OpenAI Agent RFT
tools_config = [
    {
        "name": "shell",
        "url": "https://your-app.modal.run/execute_shell",
        "headers": {"Authorization": "Bearer YOUR_TOKEN"}
    },
    {
        "name": "read_file",
        "url": "https://your-app.modal.run/read_file",
        "headers": {"Authorization": "Bearer YOUR_TOKEN"}
    },
    {
        "name": "search",
        "url": "https://your-app.modal.run/search_corpus",
        "headers": {"Authorization": "Bearer YOUR_TOKEN"}
    }
]
```

**Request Flow:**

```mermaid
sequenceDiagram
    participant OAI as OpenAI Training
    participant LB as Load Balancer
    participant VM1 as VM (Rollout 1)
    participant VM2 as VM (Rollout 2)

    Note over OAI: Training Step Starts
    OAI->>LB: Rollout 1: shell("grep TODO")
    OAI->>LB: Rollout 2: shell("rm temp.txt")

    LB->>VM1: Route to isolated VM 1
    LB->>VM2: Route to isolated VM 2

    Note over VM1: Executes grep TODO<br/>(safe, isolated)
    Note over VM2: Executes rm temp.txt<br/>(safe, isolated)

    VM1->>LB: grep results
    VM2->>LB: success

    LB->>OAI: Return results

    Note over VM1,VM2: Rollouts complete<br/>VMs destroyed
```

## How to use it

**Phase 1: Infrastructure Setup**

Choose your isolation technology:

- **Modal/Lambda**: Serverless functions with container isolation (easiest)
- **Docker**: Containers per rollout (good balance)
- **Cloud VMs**: EC2/GCP instances per rollout (maximum isolation, slower)
- **Kubernetes Jobs**: K8s pods per rollout (production-grade)

**Phase 2: Implement Rollout ID Tracking**

```python
# All tool endpoints must accept and validate rollout_id
@app.post("/tool/{tool_name}")
async def execute_tool(tool_name: str, rollout_id: str, params: dict):
    # Get or create isolated environment for this rollout
    vm = get_or_create_vm(rollout_id)

    # Execute in isolated context
    result = vm.execute(tool_name, params)

    return result
```

**Phase 3: Handle Bursty Traffic**

Agent RFT sends traffic in bursts:

- **Training step boundary**: 100-500 simultaneous rollout requests
- **Tool call latency**: Brief pauses while agent thinks
- **Cleanup phase**: Mass VM destruction

Configure auto-scaling:

```python
# Modal example
@app.cls(
    image=base_image,
    concurrency_limit=500,  # Max concurrent VMs
    container_idle_timeout=60,  # Cleanup after 1 min idle
)
```

**Phase 4: Monitor Infrastructure**

Critical metrics:

- **VM provisioning time**: Should be <5 seconds
- **Failure rate**: Infrastructure errors → zero reward → training collapse
- **Resource leaks**: VMs not cleaning up properly
- **Cost**: 500 VMs * training duration can get expensive

Sam's advice from Cognition:

> "Sometimes like let's say there's infrastructure error and the VMs fail... that does lead to the training kind of collapsing because even the model might have done something good, it got a zero reward."

**Set up monitoring:**

```python
import logging

logger = logging.getLogger("rollout-infra")

@method()
def execute_tool(self, rollout_id: str, tool: str, params: dict):
    try:
        result = self._execute(tool, params)

        # Log success
        logger.info(f"rollout={rollout_id} tool={tool} status=success")

        return result

    except Exception as e:
        # Log failure - critical for debugging training collapse
        logger.error(
            f"rollout={rollout_id} tool={tool} status=error error={str(e)}"
        )

        # Return error to model (don't give zero reward for infra issues)
        return {
            "error": "Infrastructure error, please retry",
            "retryable": True
        }
```

## Real-World Example: Cognition Devon

**Challenge**: Train Devon's file planning agent with shell tool access

**Requirements:**

- Agent uses `shell` tool to run `grep`, `find`, `ls`, etc.
- Need to prevent one rollout from affecting others
- Must handle potentially destructive commands safely

**Solution:**

1. **Modal Infrastructure**: Used Modal for fast VM provisioning
2. **Isolation**: Each rollout gets dedicated VM with fresh filesystem
3. **Corpus Replication**: Copied entire repository corpus into each VM
4. **Scaling**: Handled 500+ simultaneous VMs during training bursts

**Results:**

- Safe training despite shell access
- No cross-contamination between rollouts
- Deterministic behavior for reward calculation
- Successfully trained agent to reduce planning from 8-10 tool calls → 4 tool calls

**Infrastructure Lessons Learned:**

- **Bursty traffic is real**: "At the beginning of every rollout they would send us like 500 new rollout requests"
- **Monitor failures**: Infrastructure errors causing zero rewards can collapse training
- **Reuse production code**: "We use VMs because we could reuse the production Devon VM info"

## Trade-offs

**Pros:**

- **Complete isolation**: No cross-contamination between rollouts
- **Safety**: Destructive commands can't affect other rollouts or host system
- **Determinism**: Consistent environment for reliable reward signals
- **Production parity**: Can use exact same environment as production

**Cons:**

- **Cost**: 100s of VMs running simultaneously can be expensive
- **Provisioning time**: VM startup adds latency (containers are faster)
- **Complexity**: Requires robust infrastructure and monitoring
- **Scaling limits**: Cloud provider quotas may limit concurrent VMs
- **Failure modes**: Infrastructure issues can cause training collapse

## Alternatives

**Shared Infrastructure with Namespacing:**

- Use filesystem namespacing (chroot, Docker volumes)
- Cheaper but less isolation
- Risk of leakage through shared resources

**Database-Backed State:**

- Store state in database keyed by rollout ID
- Simpler infrastructure
- Doesn't work for filesystem-based tools

**Optimistic Isolation:**

- Let rollouts share infrastructure
- Detect and discard contaminated rollouts
- Wastes compute on discarded rollouts

## References

- [OpenAI Build Hour: Agent RFT - Cognition Case Study (November 2025)](https://youtu.be/1s_7RMG4O4U)
- [Modal Documentation](https://modal.com/docs)
- [Docker Isolation Best Practices](https://docs.docker.com/engine/security/)
- Related patterns: Agent Reinforcement Fine-Tuning, Virtual Machine Operator Agent

---

## Iterative Multi-Agent Brainstorming

**Status:** experimental-but-awesome
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/claude-code

## Problem
For complex problems or creative ideation, a single AI agent instance might get stuck in a local optimum or fail to explore a diverse range of solutions. Generating a breadth of ideas can be challenging for a sequential, monolithic process.

## Solution
Employ a multi-agent approach for brainstorming and idea generation. This involves:
1.  Defining a core problem or task.
2.  Spawning multiple independent (or semi-independent) AI agent instances.
3.  Assigning each agent the same initial task or slightly varied perspectives on the task.
4.  Allowing each agent to work in parallel to generate ideas, solutions, or approaches.
5.  Collecting the outputs from all agents.
6.  Optionally, a coordinating agent or a human user can then synthesize these diverse outputs, identify common themes, or select the most promising ideas for further development.

This pattern leverages parallelism to explore a wider solution space and can lead to more creative or robust outcomes than a single agent might produce alone.

## Example (parallel brainstorming)
```mermaid
flowchart TD
    A[Core Problem/Task] --> B[Agent 1: Perspective A]
    A --> C[Agent 2: Perspective B]
    A --> D[Agent 3: Perspective C]

    B --> E[Solution Set 1]
    C --> F[Solution Set 2]
    D --> G[Solution Set 3]

    E --> H[Coordinator/Human]
    F --> H
    G --> H

    H --> I[Synthesized Solutions]
    H --> J[Common Themes]
    H --> K[Best Ideas Selected]
```

## Example
-   "Use 3 parallel agents to brainstorm ideas for how to clean up `@services/aggregator/feed_service.cpp`." (from Claude Code examples)

## References
-   Inspired by the example of using parallel agents for brainstorming in "Mastering Claude Code: Boris Cherny's Guide & Cheatsheet," section III.

[Source](https://www.nibzard.com/claude-code)

---

## Iterative Prompt & Skill Refinement

**Status:** proposed
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://lethain.com/agents-iterative-refinement/

## Problem

Agent usage reveals gaps in prompts, skills, and tools—but how do you systematically improve them? When a workflow fails or behaves sub-optimally, you need multiple mechanisms to capture feedback and iterate. Single approaches aren't enough; you need a **multi-pronged refinement strategy**.

## Solution

Implement **multiple complementary refinement mechanisms** that work together. No single mechanism catches all issues—you need layered approaches.

**Four key mechanisms:**

**1. Responsive Feedback (Primary)**
- Monitor internal `#ai` channel for issues
- Skim workflow interactions daily
- This is the most valuable ongoing source of improvement

**2. Owner-Led Refinement (Secondary)**
- Store prompts in editable documents (Notion, Google Docs)
- Most prompts editable by anyone at the company
- Include prompt links in workflow outputs (Slack messages, Jira comments)
- Prompts must be discoverable + editable

**3. Claude-Enhanced Refinement (Specialized)**
- Use Datadog MCP to pull logs into skill repository
- Skills are a "platform" used by many workflows
- Often maintained by central AI team, not individual owners

**4. Dashboard Tracking (Quantitative)**
- Track workflow run frequency and errors
- Track tool usage (how often each skill loads)
- Data-driven prioritization of improvements

```mermaid
graph TD
    A[Workflow Runs] --> B[Feedback Channel: #ai]
    A --> C[Owner Edits Prompts]
    A --> D[Datadog Logs → Claude]
    A --> E[Dashboards: Metrics]

    B --> F[Identify Issues]
    C --> F
    D --> F
    E --> F

    F --> G[Update Prompts/Skills]
    G --> A

    style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style E fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
```

## How to use it

**Implementation checklist:**

- [ ] **Feedback channel**: Internal Slack/Discord for agent issues
- [ ] **Editable prompts**: Store in Notion/docs, not code
- [ ] **Prompt links**: Include in every workflow output
- [ ] **Log access**: Datadog/observability with MCP integration
- [ ] **Dashboards**: Track workflow runs, errors, tool usage

**Refinement workflow:**

```python
# After each workflow run, include link
workflow_result = {
    "output": "...",
    "prompt_link": "https://notion.so/prompt-abc123"
}
```

**Discovery strategy:**

- **Daily**: Skim feedback channel, review workflow interactions
- **Weekly**: Review dashboard metrics for error spikes
- **Ad-hoc**: Pull logs when specific issues reported
- **Quarterly**: Comprehensive prompt/skill audit

**Post-run evals (next step):**

Include subjective eval after each run:
- Was this workflow effective?
- What would have made it better?
- Human-in-the-loop to nudge evolution

## Trade-offs

**Pros:**

- **Multi-layered**: Catches issues different mechanisms miss
- **Continuous**: Always improving, not episodic
- **Accessible**: Anyone can contribute to improvement
- **Data-driven**: Dashboards prioritize what matters
- **Skill-sharing**: Central team can maintain platform-level skills

**Cons:**

- **No silver bullet**: Can't eliminate any mechanism
- **Maintenance overhead**: Multiple systems to manage
- **Permission complexity**: Need balanced edit access
- **Alert fatigue**: Too many signals can overwhelm

**Workflow archetypes:**

| Type | Refinement Strategy |
|------|---------------------|
| **Chatbots** | Post-run evals + human-in-the-loop |
| **Well-understood workflows** | Code-driven (deterministic) |
| **Not-yet-understood workflows** | The open question |

**Open challenge:** How to scalably identify and iterate on "not-yet-well-understood" workflows without product engineers implementing each individually?

## References

* [Iterative prompt and skill refinement](https://lethain.com/agents-iterative-refinement/) - Will Larson (Imprint, 2026)
* Related: Dogfooding with Rapid Iteration, Compounding Engineering, Memory Synthesis from Execution Logs

---

## Language Agent Tree Search (LATS)

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2310.04406

## Problem

Current language agents often struggle with complex reasoning tasks that require exploration of multiple solution paths. Simple linear approaches like ReACT or basic reflection patterns can get stuck in local optima or fail to consider alternative strategies. This is particularly problematic for tasks requiring strategic planning, mathematical reasoning, or multi-step problem solving where early decisions significantly impact later outcomes.

## Solution

Language Agent Tree Search (LATS) combines Monte Carlo Tree Search (MCTS) with language model reflection and evaluation capabilities. The approach treats the problem-solving process as a tree where:

1. **Nodes** represent states (partial solutions or reasoning steps)
2. **Edges** represent actions (next steps in reasoning)
3. **Leaf nodes** are evaluated using the LLM's self-reflection capabilities
4. **Backpropagation** updates value estimates throughout the tree

The agent explores promising branches more deeply while maintaining breadth to avoid getting stuck. This creates a best-of-both-worlds approach combining systematic search with LLM reasoning.

## Example

```python
class LATSAgent:
    def __init__(self, llm, max_iterations=50, exploration_constant=1.4):
        self.llm = llm
        self.max_iterations = max_iterations
        self.c = exploration_constant  # UCB exploration parameter
        
    def search(self, initial_state, problem):
        root = Node(state=initial_state)
        
        for _ in range(self.max_iterations):
            # Selection: traverse tree using UCB
            node = self.select(root)
            
            # Expansion: generate possible actions
            if not node.is_terminal():
                actions = self.generate_actions(node.state, problem)
                for action in actions:
                    child_state = self.apply_action(node.state, action)
                    node.add_child(Node(state=child_state, action=action))
            
            # Simulation: evaluate the node
            value = self.evaluate(node, problem)
            
            # Backpropagation: update values up the tree
            self.backpropagate(node, value)
        
        return self.best_path(root)
    
    def select(self, node):
        """Select child using UCB (Upper Confidence Bound)"""
        while node.children:
            node = max(node.children, key=lambda n: self.ucb_score(n))
        return node
    
    def ucb_score(self, node):
        if node.visits == 0:
            return float('inf')
        exploitation = node.value / node.visits
        exploration = self.c * sqrt(log(node.parent.visits) / node.visits)
        return exploitation + exploration
    
    def evaluate(self, node, problem):
        """Use LLM to evaluate the quality of current state"""
        prompt = f"""
        Problem: {problem}
        Current solution state: {node.state}
        
        Evaluate this partial solution:
        1. Is this on the right track? (0-10)
        2. What are the strengths?
        3. What are potential issues?
        4. Estimated distance to complete solution?
        
        Overall value score (0-1):
        """
        evaluation = self.llm.generate(prompt)
        return self.parse_value_score(evaluation)
    
    def generate_actions(self, state, problem):
        """Use LLM to generate possible next steps"""
        prompt = f"""
        Problem: {problem}
        Current state: {state}
        
        Generate 3-5 possible next steps that could advance the solution.
        Each step should be different and explore various approaches.
        """
        return self.llm.generate(prompt).split('\n')
```

## Benefits

- **Better Performance**: Outperforms ReACT, Reflexion, and Tree of Thoughts on complex reasoning tasks
- **Strategic Exploration**: Balances exploration of new paths with exploitation of promising ones
- **Self-Improving**: Uses reflection to learn which paths are more promising
- **Robust**: Less likely to get stuck in dead ends compared to linear approaches

## Trade-offs

**Pros:**
- Significantly better performance on complex reasoning tasks
- Systematic exploration prevents getting stuck
- Naturally handles problems with multiple valid approaches
- Provides interpretable reasoning traces

**Cons:**
- Higher computational cost due to tree exploration
- Requires more LLM calls than simple approaches
- May be overkill for simple tasks
- Requires careful tuning of exploration parameters

## References

- [Language Agent Tree Search (LATS) Paper](https://arxiv.org/abs/2310.04406)
- [Comparison with ReACT, Reflexion, and Tree of Thoughts](https://www.langchain.com/langgraph)

---

## Latent Demand Product Discovery

**Status:** best-practice
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

When building agent products, it's difficult to know which features will have real product-market fit before investing significant engineering effort. Traditional feature development relies on user interviews and surveys, which may not reveal how users would actually adapt tools to their needs.

## Solution

Build products that are intentionally **hackable and extensible**, then observe how power users "abuse" or repurpose features for unintended use cases. This reveals latent demand—real user needs demonstrated through behavior rather than stated preferences. Once you identify these patterns, productize them for all users.

**Key principles:**

1. **Make extension points**: Provide hooks, plugins, slash commands, configuration files that users can customize
2. **Monitor creative usage**: Watch for patterns where users hack features for purposes beyond original intent
3. **Quantify the signal**: Measure how many users are doing the "unintended" behavior
4. **Productize validated demand**: Build proper features for behaviors that show strong adoption

## How to use it

**Implementation strategy:**

- Design core functionality with clear extension APIs
- Make configuration and customization easy for power users
- Monitor analytics and user feedback for unexpected usage patterns
- Look for behaviors that multiple independent users discover
- Prioritize building features for high-frequency "hacks"

**Concrete examples from the transcript:**

1. **Facebook Dating**: Analytics showed 60% of profile views were between opposite-gender non-friends → clear dating behavior → launched Facebook Dating
2. **Facebook Marketplace**: 40% of Facebook group posts were buy-sell transactions → users treating groups as marketplaces → launched dedicated Marketplace product
3. **Claude Code hooks on Slack**: Users requested Slack notifications when Claude asks for permissions → built hooks system → users implemented it themselves

## Trade-offs

**Pros:**

- Validates real demand through behavior, not speculation
- Reduces risk of building unwanted features
- Empowers power users to innovate on your behalf
- Creates tight feedback loop between usage and development
- Features come with built-in proof of concept

**Cons:**

- Requires building extensibility infrastructure upfront
- May delay shipping "obvious" features while waiting for signal
- Power user behavior may not represent mainstream needs
- Requires analytics and monitoring systems to detect patterns
- Can lead to feature bloat if every hack becomes a product

## References

* Boris Cherny (Anthropic): "There's this really old idea in product called latent demand... you build a product in a way that is hackable, that is kind of open-ended enough that people can abuse it for other use cases. Then you see how people abuse it and then you build for that."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Layered Configuration Context

**Status:** established
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/claude-code

## Problem
AI agents require relevant context to perform effectively. Providing this context manually in every prompt is cumbersome, and a one-size-fits-all global context is often too broad or too narrow. Different projects, users, and organizational policies may require different baseline information for the agent.

## Solution
Implement a system of layered configuration files (e.g., named `CLAUDE.md` or a similar convention) that the agent automatically discovers and loads based on their location in the file system hierarchy. This allows for:

-   **Enterprise/Organizational Context:** A root-level file (`/<enterprise_root>/CLAUDE.md`) for policies or information shared across all projects in an organization.
-   **User-Specific Global Context:** A file in the user's home directory (`~/.claude/CLAUDE.md`) for personal preferences, common tools, or notes shared across all their projects.
-   **Project-Specific Context:** A file within the project's root directory (`<project_root>/CLAUDE.md`), typically version-controlled, for project-specific instructions, architectural overviews, or key file descriptions.
-   **Project-Local Context:** A local, non-version-controlled file (`<project_root>/CLAUDE.local.md`) for individual overrides, temporary notes, or secrets relevant to the project for that user.

The agent intelligently merges or prioritizes these context layers, providing a rich, tailored baseline of information without manual intervention in each query.

## Example (configuration hierarchy)
```mermaid
flowchart TD
    A[Enterprise Root<br/>/enterprise/CLAUDE.md] --> E[Merged Context]
    B[User Global<br/>~/.claude/CLAUDE.md] --> E
    C[Project Root<br/>project/CLAUDE.md] --> E
    D[Project Local<br/>project/CLAUDE.local.md] --> E
    E --> F[Agent Context Window]

    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style F fill:#ffebee
```

## References
- Based on the `CLAUDE.md` system described in "Mastering Claude Code: Boris Cherny's Guide & Cheatsheet," section IV.

[Source](https://www.nibzard.com/claude-code)

---

## Lethal Trifecta Threat Model

**Status:** best-practice
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://simonwillison.net/2025/Jun/16/lethal-trifecta/

## Problem
Combining three agent capabilities—
1. **Access to private data**
2. **Exposure to untrusted content**
3. **Ability to externally communicate**

—creates a straightforward path for prompt-injection attackers to steal sensitive information.  
LLMs cannot reliably distinguish "good" instructions from malicious ones once they appear in the same context window.

## Solution
Adopt a **Trifecta Threat Model**:  

- **Audit every tool** an agent can call and classify it against the three capabilities.  
- **Guarantee that at least one circle is missing** in any execution path. Options include:  
  
- Remove external network access (no exfiltration).  
  - Deny direct file/database reads (no private data).  
  - Sanitize or segregate untrusted inputs (no hostile instructions).  
- Enforce this at orchestration time, not with brittle prompt guardrails.

```python
# pseudo-policy
if tool.can_externally_communicate and
   tool.accesses_private_data and
   input_source == "untrusted":
       raise SecurityError("Lethal trifecta detected")
```

## How to use it

* Maintain a machine-readable capability matrix for every tool.
* Add a pre-execution policy check in your agent runner.
* Fail closed: if capability metadata is missing, treat the tool as high-risk.

## Trade-offs

**Pros:** Simple mental model; eliminates entire attack class.
**Cons:** Limits powerful "all-in-one" agents; requires disciplined capability tagging.

## References

* Willison, *The Lethal Trifecta for AI Agents* (June 16 2025).
* "Design Patterns for Securing LLM Agents against Prompt Injections" (June 13 2025).

---

## LLM Map-Reduce Pattern

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2506.08837

## Problem
Injecting a single poisoned document can manipulate global reasoning if all data is processed in one context.

## Solution
Adopt a **map-reduce workflow**:

- **Map:** Spawn lightweight, *sandboxed* LLMs—each ingests one untrusted chunk and emits a constrained output (boolean, JSON schema, etc.).  
- **Reduce:** Aggregate those safe summaries with either deterministic code or a privileged LLM that sees only sanitized fields.

```pseudo
results = []
for doc in docs:
    ok = SandboxLLM("Is this an invoice? (yes/no)", doc)
    results.append(ok)
final = reduce(results)  # no raw docs enter this step
```

## How to use it

File triage, product-review summarizers, resume filters—any N-to-1 decision where each item's influence should stay local.

## Trade-offs

* **Pros:** A malicious item can't taint others; scalable parallelism.
* **Cons:** Requires strict output validation; extra orchestration overhead.

## References

* Beurer-Kellner et al., §3.1 (3) LLM Map-Reduce.

---

## LLM Observability

**Status:** proposed
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://lethain.com/agents-logging/

## Problem

Agents introduce **non-determinism**—the same input can produce different outputs. When agents do something sub-optimal, users flag it as a "bug" even if it's just prompt ambiguity. Debugging these issues requires tracing through complex multi-step workflows, but standard logging (CloudWatch, Lambda logs) is painful to navigate. Engineers need easy access to workflow execution details to debug quickly, or agents won't get adopted.

## Solution

Integrate **LLM observability platforms** (Datadog LLM Observability, LangSmith, etc.) that provide **span-level tracing** of agent workflows. Instead of spelunking through raw logs, get a visual UI showing each step of the agent's execution.

**Key capabilities:**

- **Span visualization**: See each LLM call, tool use, and intermediate result
- **Workflow linking**: Trace from user input through all sub-steps to final output
- **Dashboarding**: Aggregate metrics on cost, latency, success rates
- **Accessible debugging**: Non-engineers can debug without log access

**Evolution from standard logging:**

1. **Print to stdout** → Captured in Lambda logs (hard to access)
2. **Slack channel** → Post run summary + AWS log link (better, still spelunking)
3. **LLM observability** → Visual span tracing, easy navigation

```mermaid
graph LR
    A[Agent Run] --> B[Observability SDK]
    B --> C[Span: LLM Call 1]
    B --> D[Span: Tool Use]
    B --> E[Span: LLM Call 2]
    C --> F[Trace UI]
    D --> F
    E --> F
    F --> G[Debug View]

    style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
```

## How to use it

**Integration steps:**

1. **Choose observability platform**: Datadog LLM Observability, LangSmith, Arize, etc.
2. **Install SDK**: Add to your agent code (often 1-2 lines)
3. **Configure tracing**: Tag workflows, users, environments
4. **Access controls**: Give broader team access (not just engineers)

**Example with Datadog:**

```python
from ddtrace import tracer

# Automatically traces LLM calls
@tracer.wrap("agent_workflow")
def run_agent(query):
    result = agent.run(query)
    # Each LLM call, tool use becomes a span
    return result
```

**Best practices:**

- **Tag everything**: Workflow name, user ID, environment (prod/staging)
- **Link to dashboards**: Make observability UI discoverable from chat
- **Share access**: Don't restrict to eng org; workflow creators need visibility
- **Monitor aggregate metrics**: Track success rates, latency, costs over time

## Trade-offs

**Pros:**

- **Fast debugging**: Navigate complex workflows visually
- **Accessible**: Non-engineers can debug without log permissions
- **Aggregated metrics**: See patterns across many runs
- **Span-level detail**: Drill into any step of execution

**Cons:**

- **Vendor dependency**: Locked into observability platform
- **Cost**: Enterprise observability can be expensive
- **Overhead**: Adds latency (usually minimal)
- **Access control**: Balancing visibility with security

**When NOT to use:**

- Simple, deterministic tools (no agent behavior)
- Single-step operations (standard logs suffice)
- Budget constraints preventing observability spend

## References

* [Building an internal agent: Logging and debugability](https://lethain.com/agents-logging/) - Will Larson (Imprint, 2025)
* Datadog LLM Observability documentation
* LangSmith documentation
* Related: Agent-First Tooling and Logging, Chain-of-Thought Monitoring & Interruption

---

## LLM-Friendly API Design

**Status:** emerging
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
For AI agents to reliably and effectively use tools, especially APIs or internal libraries, the design of these interfaces matters. APIs designed solely for human consumption might be ambiguous or overly complex for an LLM to use correctly without extensive fine-tuning or elaborate prompting.

## Solution
Design or adapt software APIs (including internal libraries and modules) with explicit consideration for LLM consumption. This involves:

-   **Explicit Versioning:** Making API version information clearly visible and understandable to the LLM, so it can request or adapt to specific versions.
-   **Self-Descriptive Functionality:** Ensuring function names, parameter names, and documentation (if accessible to the LLM) clearly describe what the API does and how to use it.
-   **Simplified Interaction Patterns:** Favoring simpler, more direct API calls over highly nested or complex interaction sequences where possible, to reduce the chances of the LLM making errors.
-   **Clear Error Messaging:** Designing error responses that are informative and actionable for an LLM, helping it to self-correct or understand why a call failed.
-   **Reduced Indirection:** Structuring code and libraries such that an LLM doesn't have to navigate through many layers of indirection to achieve a task, making it easier for the model to reason about the codebase.

The aim is to create interfaces that are robust and intuitive for LLMs to interact with, thereby improving the reliability and effectiveness of agent tool use.

## References
- Lukas Möller (Cursor) at 0:16:00: "API design is already adjusting such that LLMs are more comfortable with that. For example, changing not only the the version number internally but making it like very visible to the model that this is a new version of some software just to make sure that the the API is used correctly." And at 0:16:20: "...structuring the code in a way where one doesn't have to go through like n level of indirection but maybe just through two levels of indirection makes, yeah, LLM models better at at working with that code base."

---

## Memory Reinforcement Learning (MemRL)

**Status:** proposed
**Category:** Learning & Adaptation
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/html/2601.03192v1

## Problem

LLMs struggle with **runtime self-evolution** due to the stability-plasticity dilemma:

- **Fine-tuning**: Computationally expensive and prone to catastrophic forgetting
- **RAG/memory systems**: Rely on semantic similarity that retrieves noise
- **No utility learning**: Can't distinguish high-value strategies from semantically similar but ineffective ones

Standard retrieval assumes "similar implies useful," but that's often wrong. A semantically relevant past solution might actually be a bad approach for the current task.

## Solution

**MemRL** adds learned "utility scores" to episodic memory, so agents learn from experience which memories actually lead to success—without modifying the model.

**Core idea:** Instead of just retrieving by similarity, rank memories by how well they've worked in the past.

**Memory triplet structure:**

- **Intent**: What the user asked for (embedded)
- **Experience**: What the agent tried (solution trace)
- **Utility**: How well it worked (learned score, updated over time)

**Two-phase retrieval:**

1. **Phase A - Semantic filter**: Find semantically similar memories
2. **Phase B - Utility ranking**: Re-rank by learned utility scores

This filters out "distractor" memories that look relevant but historically lead to poor outcomes.

```mermaid
graph LR
    A[Query] --> B[Find Similar Memories]
    B --> C[Rank by Utility Scores]
    C --> D[Use Top Memories]
    D --> E[Get Result]
    E --> F[Update Utilities]
    F --> G[Store New Experience]

    style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style F fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
```

## How to use it

**Basic implementation:**

1. **Store experiences with utility scores**

   ```python
   memory_bank.append({
       "intent": embed(query),
       "experience": solution_trace,
       "utility": 0.5  # initial score, learned over time
   })
   ```

2. **Retrieve with utility ranking**

   ```python
   # First: filter by similarity
   candidates = similar_memories(query, threshold=0.7)

   # Then: re-rank by utility
   ranked = sorted(candidates, key=lambda m: m.utility, reverse=True)
   context = ranked[:k]
   ```

3. **Update utilities based on outcomes**

   ```python
   reward = 1 if success else 0
   for mem in retrieved_contexts:
       mem.utility += learning_rate * (reward - mem.utility)
   ```

**Why this works:**
- Successful memories get higher scores, retrieved more often
- Failed memories get downranked, even if semantically similar
- Frozen LLM stays stable; only memory utilities evolve
- Agent self-improves through runtime experience

## Trade-offs

**Pros:**

- No catastrophic forgetting (frozen LLM)
- Self-improves from experience
- Filters out "look-alike" bad solutions
- No retraining needed

**Cons:**

- Need reliable success/failure signals
- Memory overhead grows over time
- Cold start: needs episodes to learn
- More complex than basic RAG

**When to use:**
- Multi-step tasks with clear success signals
- Reusable problem-solving patterns
- Can't afford fine-tuning

**When NOT to use:**
- Single-turn queries
- No clear reward signals
- Highly diverse tasks (no patterns)

## References

* [Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory](https://arxiv.org/html/2601.03192v1) - Shengtao Zhang, Jiaqian Wang, et al. (2025)
* Related: Episodic Memory Retrieval & Injection, Memory Synthesis from Execution Logs, Agent Reinforcement Fine-Tuning

---

## Memory Synthesis from Execution Logs

**Status:** emerging
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

Individual task execution transcripts contain valuable learnings, but:

- **Too specific**: "Make this button pink" isn't useful as general guidance
- **Unknown relevance**: Hard to predict which learnings apply to future tasks
- **Scattered knowledge**: Insights buried across hundreds of conversation logs
- **Abstraction challenge**: Difficult to know the right level of generality

Simply memorizing everything creates noise; ignoring everything loses valuable patterns.

## Solution

Implement a **two-tier memory system**:

1. **Task diaries**: Agent writes structured logs for each task (what it tried, what failed, why)
2. **Synthesis agents**: Periodically review multiple task logs to extract reusable patterns

The synthesis step identifies recurring themes across logs, surfacing insights that aren't obvious from any single execution.

```mermaid
graph TD
    A[Task 1: Diary Entry] --> D[Synthesis Agent]
    B[Task 2: Diary Entry] --> D
    C[Task 3: Diary Entry] --> D
    D --> E[Extract Patterns]
    E --> F[Update System Prompts]
    E --> G[Create Slash Commands]
    E --> H[Generate Observations]
```

**Example diary entry format:**

```markdown
## Task: Add authentication to checkout flow
### Attempted approaches:
1. JWT tokens in localStorage - failed due to XSS concerns
2. HTTP-only cookies - worked but needed CORS config
3. Session tokens with Redis - chosen solution

### What worked:

- Redis session store with 24hr expiry
- CORS whitelist in production config

### Mistakes made:

- Forgot to handle token refresh initially
- Missed error handling for expired sessions

### Patterns discovered:

- Auth changes always need CORS update
- Need both client and server-side expiry checks
```

## How to use it

**Implementation approach:**

### Phase 1: Structured logging

Configure agents to write task diaries in consistent format:

- What was attempted and why
- What failed and the error messages
- What succeeded and why it worked
- Edge cases discovered
- Patterns that might generalize

### Phase 2: Periodic synthesis

Run synthesis agents over recent logs (weekly, after N tasks, etc.):

```pseudo
synthesis_agent.prompt = """
Review these 50 task diaries.
Identify patterns that appear in 3+ tasks.
For each pattern, suggest:

- A general rule to add to CLAUDE.md
- A potential slash command
- A test case to prevent regression
"""
```

### Phase 3: Knowledge integration

Feed synthesized insights back into:

- System prompts (CLAUDE.md)
- Reusable commands
- Automated checks/hooks
- Test suites

**Real usage at Anthropic (from transcript):**

> "There are some people at Anthropic where for every task they do, they tell Claude Code to write a diary entry in a specific format... they even have these agents that look over the past memory and synthesize it into observations."

## Trade-offs

**Pros:**

- **Pattern detection**: Finds recurring issues humans might miss
- **Right abstraction level**: Synthesis across multiple tasks reveals what's general
- **Automatic knowledge extraction**: Don't rely on humans remembering to document
- **Evolving memory**: System learns and improves over time
- **Evidence-based**: Patterns backed by multiple occurrences, not speculation

**Cons:**

- **Storage overhead**: Must persist all task logs
- **Synthesis complexity**: Requires sophisticated agents to extract good patterns
- **False patterns**: May identify coincidental correlations
- **Maintenance burden**: Synthesized rules need periodic review
- **Privacy concerns**: Logs may contain sensitive information
- **Token costs**: Synthesis over many logs is expensive

**Open questions:**

- How many occurrences validate a pattern?
- How to prune outdated or wrong patterns?
- What's the right synthesis frequency?
- How to handle conflicting patterns across logs?

## References

* Cat Wu: "Some people at Anthropic where for every task they do, they tell Claude Code to write a diary entry in a specific format. What did it try? Why didn't it work? And then they even have these agents that look over the past memory and synthesize it into observations."
* Boris Cherny: "Synthesizing the memory from a lot of logs is a way to find these patterns more consistently... If I say make the button pink, I don't want you to remember to make all buttons pink in the future."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Merged Code + Language Skill Model

**Status:** emerging
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=Xkwok_XXQgw

## Problem

Building a **unified model** that excels both at **natural language tasks** (e.g., summarization, documentation generation) and **code generation/reasoning** typically requires a massive centralized training run. This is:

- **Compute-Intensive:** Training from scratch on both code and language corpora demands enormous resources.
- **Susceptible to Interference:** When mixing code and NL tasks in one pipeline, the model may forget earlier skills.

## Solution

Adopt a **decentralized training + model merging** approach:

**1. Train a "Language Specialist"**
- Fine-tune a base LLM on documentation generation, summarization, code comments, and general NL tasks.
- Save checkpoint `lang-specialist-ckpt.pt`.

**2. Train a "Code Specialist"**
- Independently fine-tune the same base LLM architecture on code-specific corpora: open-source repositories, coding challenge datasets, and code-comment pairs.
- Save checkpoint `code-specialist-ckpt.pt`.

**3. Weight Averaging Merge**
- Use simple arithmetic weight averaging (or Fisher-weighted averaging) to combine `lang-specialist-ckpt.pt` and `code-specialist-ckpt.pt` into `merged-agent-ckpt.pt`.
- Optionally, follow with a **short fine-tuning** on a mixed dataset (small NL+code tasks) to smooth out any conflicts.

**4. Iterative Merge Rounds**
- As new specialists (e.g., a "Python Testing Specialist" or "Security Static Analysis Specialist") become available, periodically merge them into the main agent.

## Example

```bash
# Example using Hugging Face transformer's merge tool
python merge_models.py \
  --model_a lang-specialist-ckpt.pt \
  --model_b code-specialist-ckpt.pt \
  --output merged-agent-ckpt.pt \
  --alpha 0.5
```

## How to use it

- **Architectural Consistency:** Ensure all specialist models share identical architecture (e.g., 1.8 B parameters, same number of layers).
- **Merging Tools:** Use established scripts (e.g., `transformers`' `merge_models`) or custom code that applies Fisher Information Matrix weighting when averaging to minimize interference.
- **Post-Merge Validation:** Run a **benchmark suite** covering both NL tasks (e.g., summarization, QA) and code tasks (e.g., code generation, bug fixing).

## Trade-offs

- **Pros:**
  - **Parallelism in R&D:** Teams can independently develop NL and code capabilities, then merge.
  - **Reduced Centralized Compute:** No need for a single massive GPU cluster to train both skill sets simultaneously.
- **Cons/Considerations:**
  - **Potential Performance Dilution:** Naïve averaging can "blur" specialist strengths if distributions conflict.
  - **Alignment Required:** All specialists must use the same base tokenizer and vocabulary to avoid mismatches.

## References

- Based on "model merging works weirdly well" observation from the Open Source Agent RL talk (May 2025) and Will Brown's remarks on decentralized skill acquisition.
- Cohere's "Command A" whitepaper on merging specialty models.

---

## Multi-Model Orchestration for Complex Edits

**Status:** validated-in-production
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
A single large language model, even if powerful, may not be optimally suited for all sub-tasks involved in a complex operation like multi-file code editing. Tasks such as understanding broad context, generating precise code, and applying edits might benefit from specialized model capabilities.

## Solution
Employ a pipeline or orchestration of multiple AI models, each specialized for different parts of a complex task. For code editing, this could involve:

1.  A **retrieval model** to gather relevant context from the codebase.
2.  A **large, intelligent generation model** (e.g., Claude 3.5 Sonnet) to understand the user's intent and generate the primary code modifications based on the retrieved context.
3.  Potentially other **custom or smaller models** to assist in applying these generated edits accurately across multiple files or performing fine-grained adjustments.

This approach leverages the strengths of different models in a coordinated fashion to achieve a more robust and effective outcome for complex operations than a single model might achieve alone.

## Example
```mermaid
flowchart TD
    A[User Request: Multi-File Edit] --> B[Retrieval Model: Gather Context]
    B --> C[Main Generation Model: Generate Edits]
    C --> D[Edit Application Model: Apply Edits Across Files]
    D --> E[Edited Codebase]
```

## References

- Aman Sanger (Cursor) discusses this at 0:01:34: "...when you kind of mix the intelligence of a model like 3.5 Sonnet with a few other kind of custom models we use for retrieval and then applying the edits made by this larger model, you now have the ability to do kind of multi-file edits."
- [Building Companies with Claude Code](https://claude.com/blog/building-companies-with-claude-code) - Model-specific task delegation: Opus 4.1 for research and complex planning, Sonnet 4.5 for implementation execution
- Related pattern: [Discrete Phase Separation](discrete-phase-separation.md) - Extends multi-model orchestration to separate conversation phases

---

## Multi-Platform Communication Aggregation

**Status:** emerging
**Category:** Tool Use & Environment
**Authors:** Lucas Carlson
**Source:** https://github.com/anthropics/claude-code

## Problem

Users communicate across multiple platforms (email, Slack, iMessage, etc.) and need to search for information that might exist in any of them. Searching each platform manually is slow and error-prone. An agent tasked with "find what X said about Y" must know which platform to check—or check all of them.

## Solution

Create a unified search interface that queries all communication platforms in parallel and aggregates results into a single, consistent format.

```mermaid
graph TD
    A["User Query: find messages about project deadline"] --> B[Aggregator Agent]
    B --> C[iMessage Search]
    B --> D[Slack Search]
    B --> E[Email Search]
    B --> F[Other Platforms...]
    C --> G[Result Collector]
    D --> G
    E --> G
    F --> G
    G --> H[Unified Results Table]
```

**Key components:**

1. **Platform Adapters**: Each platform has a CLI/API wrapper with consistent interface
2. **Parallel Dispatcher**: Spawns searches concurrently (sub-agent pattern or background jobs)
3. **Result Normalizer**: Converts platform-specific formats to unified schema
4. **Aggregator**: Combines, deduplicates, and ranks results

```bash
# Example: Unified search skill implementation
search_all() {
    query="$1"

    # Spawn parallel searches
    messages search "$query" > /tmp/messages.json &
    slack-messages search "$query" > /tmp/slack.json &
    fastmail.sh search "$query" > /tmp/fastmail.json &
    gmail.sh search "$query" > /tmp/gmail.json &

    wait  # All complete

    # Aggregate and normalize
    aggregate_results /tmp/*.json
}
```

## How to use it

**When to apply:**

- User asks "where did someone mention X"
- User needs to find a conversation but doesn't remember the platform
- Cross-platform audit or compliance searches
- Building unified inbox or communication hub features

**Implementation steps:**

1. Create CLI wrappers for each platform with consistent output format (JSON)
2. Define a common schema: `{platform, sender, timestamp, content, url}`
3. Build parallel dispatch mechanism (bash background jobs, sub-agents, or async)
4. Implement result ranking (by recency, relevance, or platform priority)
5. Present in unified table format with platform badges

**Skill definition example:**

```markdown
## Unified Communication Search

**Proactive triggers:** "search everywhere", "find across all", "where did someone say"

Searches in parallel:

- Apple Messages
- Slack
- Fastmail
- Gmail

Results presented in unified table, grouped by platform.
```

## Trade-offs

**Pros:**

- Single query searches all platforms—no context switching
- Parallel execution minimizes latency (total time ≈ slowest platform)
- Unified format makes comparison and filtering easy
- Extensible: add new platforms without changing interface
- Reduces "which platform was that on?" friction

**Cons:**

- Requires maintaining adapters for each platform
- Rate limits may apply across platforms simultaneously
- Result ranking across platforms is subjective (is a Slack message more relevant than an email?)
- Privacy/security: aggregating data across platforms increases exposure
- Some platforms have poor search APIs (result quality varies)

## References

* Sub-Agent Spawning pattern for parallel execution
* LLM Map-Reduce pattern for result aggregation
* Claude Code `/search-all` skill implementation

---

## Multi-Platform Webhook Triggers

**Status:** emerging
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://lethain.com/agents-triggers/

## Problem

An internal agent only provides value when its workflows are initiated. Building out a library of workflow initialization mechanisms (triggers) is core to agent adoption. Without easy triggers, employees can't effectively automate their day-to-day workflows.

## Solution

Implement **multi-platform webhook triggers** that allow external SaaS tools to initiate agent workflows automatically.

**Trigger types implemented:**

1. **Notion webhooks**: Fire on page/database modifications
   - Document status changes (draft → "ready for review")
   - Request for Comment (RFC) workflows
   - Auto-assign reviewers based on topic
   - In-line commenting with suggestions

2. **Slack messages**: Bot responds in channels
   - Public channel responses
   - Private channel support (with careful logging)
   - Channel creation triggers for auto-joining
   - Default prompts for new channels (respond only when mentioned)

3. **Jira webhooks**: Issue lifecycle events
   - Issue creation, updates, comments
   - Custom routing logic for ticket assignment

4. **Slack reacji (emoji reactions)**: Quick workflow initiation
   - Listen for specific emojis in any channel
   - Example: `:jira:` reaction turns thread into ticket
   - Centralized routing instructions

5. **Scheduled events**: Periodic triggers
   - Slack workflows publishing to channels
   - GitHub Actions cron jobs

```pseudo
# Trigger flow
Platform Event → Webhook → Agent Trigger → Workflow Execution
```

## How to use it

**Implementation considerations:**

**Authentication and authorization:**
- OAuth2 tokens + SSL where possible
- Platform-specific security (Slack request verification)
- No authorization tokens for platforms that don't support it

**Private channel security:**
```yaml
slack_private_channels:
  auto_join: false  # Don't auto-join private channels
  logging:
    # Critical: Don't log evidence of private channel activity
    exclude_private_channels: true
    # Exfiltrating private messages loses trust instantly
```

**Trigger configuration:**

```yaml
# Notion triggers
notion_triggers:
  - database: "RFC Database"
    event: "page_updated"
    filter: "status.draft → status.ready_for_review"
    workflow: "assign_reviewers"

  - database: "Documentation"
    event: "page_created"
    workflow: "suggest_improvements"

# Slack reacji triggers
slack_reacji:
  - emoji: ":jira:"
    scope: "any_channel"
    workflow: "thread_to_ticket"
    routing: "centralized"

# Scheduled triggers
scheduled_triggers:
  - cron: "0 9 * * MON"
    workflow: "weekly_standup_prep"
    via: "github_actions"
```

**Why not Zapier/n8n?**

The author chose custom implementation over existing automation platforms:

- **Full control**: Quality details facilitate adoption better than generic integration constraints
- **Nuanced behavior**: Custom handling of entity resolution, error cases
- **Learning**: Building internal intuition about agents
- **Compatibility**: Can still use Zapier via generic webhook catch-all

**Generic webhook catch-all:**

```yaml
# Fallback for esoteric platforms
generic_webhook:
  endpoint: "/webhooks/generic"
  behavior: "include_full_message_in_context"
  integration: "can_pair_with_zapier"
```

## Trade-offs

**Pros:**

- **Low friction**: Easy workflow initiation enables iterative learning
- **Platform-native**: Users work in existing tools (Slack, Notion, Jira)
- **Reactive**: Agent responds to events, not just explicit requests
- **Flexible**: Support for custom routing and nuanced workflows
- **Scalable**: Easy to add new trigger types by pasting docs to LLM

**Cons:**

- **Maintenance overhead**: Each platform integration requires ongoing work
- **Security complexity**: Private channels, auth tokens, request verification
- **Platform lock-in**: Deep integration with specific tools
- **Observability challenges**: Must avoid logging sensitive data
- **Custom vs off-the-shelf**: Building vs buying (Zapier, n8n)

**Quality details matter:**

> "The quality details facilitate adoption in a way that Zapier integration's constraints simply do not."

Custom implementation allows nuances like:
- Slack entity resolution for seamless experience
- Careful logging behavior in private channels
- Centralized ticket routing logic

**When to use:**

- Internal agent platforms with multiple integration points
- Teams already using Slack, Notion, Jira extensively
- Document-heavy workflows (RFCs, reviews, approvals)
- Need for automated routing and triage

**When to use Zapier/n8n instead:**

- Esoteric one-off integrations
- Rapid prototyping before custom implementation
- Non-technical users who need self-service

## References

* [Building an internal agent: Triggers](https://lethain.com/agents-triggers/) - Will Larson (2025)
* Related: [Proactive Trigger Vocabulary](proactive-trigger-vocabulary.md) - Natural language trigger phrases for skill routing

---

## No-Token-Limit Magic

**Status:** experimental-but-awesome
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/ampcode

## Problem
Aggressive prompt compression to save tokens stifles reasoning depth and self-correction.

## Solution
During prototyping, **remove hard token limits**. Allow lavish context and multiple reasoning passes. Yes, it's pricier—but dramatically better outputs surface valuable patterns before optimizing.

## Example (token budget approach)
```mermaid
flowchart TD
    A[Development Phase] --> B{Token Strategy}
    B -->|Prototype| C[No Token Limits]
    B -->|Production| D[Optimized Limits]

    C --> E[Lavish Context]
    C --> F[Multiple Reasoning Passes]
    C --> G[Rich Self-Correction]

    E --> H[Better Output Quality]
    F --> H
    G --> H

    H --> I[Identify Valuable Patterns]
    I --> J[Optimize for Production]
    J --> D
```

## References
- Raising An Agent - Episode 2 cost discussion—$1000 prototype spend justified by productivity.

[Source](https://www.nibzard.com/ampcode)

---

## Opponent Processor / Multi-Agent Debate Pattern

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

Single-agent decision making can suffer from:

- **Confirmation bias**: Agent finds evidence supporting initial hypothesis
- **Limited perspectives**: One context window misses alternative approaches
- **Insufficient scrutiny**: No adversarial pressure to defend decisions
- **Unexamined assumptions**: Agent doesn't challenge its own reasoning

## Solution

Spawn **opposing agents** with different goals or perspectives to debate each other's positions. The conflict between agents surfaces blind spots, biases, and unconsidered alternatives.

**Key mechanism**: Create agents with opposing incentives or roles, let them critique each other's work, then synthesize results.

```mermaid
graph TD
    A[Main Task] --> B[Agent 1: Advocate]
    A --> C[Agent 2: Critic/Auditor]
    B --> D[Proposes Solution]
    C --> E[Challenges Solution]
    D --> F[Debate / Iteration]
    E --> F
    F --> G[Synthesized Decision]
```

**Example configurations:**

- **Pro vs. Con**: One agent argues for acceptance, another for rejection
- **Optimistic vs. Conservative**: Different risk tolerances
- **User advocate vs. Company auditor**: Competing interests
- **Frontend dev vs. Backend dev**: Different technical perspectives

## How to use it

**Implementation pattern:**

1. Define opposing roles with clear incentives
2. Spawn both agents with same context but different system prompts
3. Have them work independently (uncorrelated context windows)
4. Collect their outputs
5. Either synthesize automatically or review differences manually

**Concrete example from transcript (expense filing):**

> "I have two subagents, one that represents me and one that represents the company. And they do battle to figure out what's the proper actual set of expenses. It's like an auditor subagent and a pro-Dan subagent."

**Other use cases:**

- **Code review**: Author-defender vs. Security-auditor
- **Architecture decisions**: Simplicity advocate vs. Future-proofing advocate
- **Content moderation**: Free speech vs. Safety
- **Resource allocation**: Different department representatives

**Historical inspiration (from transcript):**

Early Reddit thread showed subagents as: Frontend dev, Backend dev, Designer, Testing dev, PM—all debating implementation.

## Trade-offs

**Pros:**

- **Reduces bias**: Opposing views surface blind spots
- **Better decisions**: Adversarial pressure improves quality
- **Uncorrelated context**: Independent reasoning prevents groupthink
- **Explicit trade-offs**: Forces articulation of competing values
- **Checks and balances**: No single agent has unchallenged authority

**Cons:**

- **2x+ token cost**: Multiple agents processing same information
- **Slower execution**: Debate takes time compared to single decision
- **May produce conflict**: Agents might deadlock without resolution mechanism
- **Requires synthesis**: Need method to reconcile opposing views
- **Over-engineering for simple tasks**: Not every decision needs debate

## References

* Dan Shipper: "One of my non-technical Claude Code use cases is expense filing... I have two subagents, one that represents me and one that represents the company. And they do battle to figure out what's the proper actual set of expenses."
* Boris Cherny: "There's a Reddit thread where someone made subagents for front end dev, back end dev, designer, testing dev, PM... I think the value is actually the uncorrelated context windows where you have these two context windows that don't know about each other. You tend to get better results this way."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Oracle and Worker Multi-Model Approach

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://youtu.be/hAEmt-FMyHA?si=6iKcGnTavdQlQKUZ

## Problem
Relying on a single AI model creates a trade-off between capability and cost. High-performance models are expensive for routine tasks, while cost-effective models may lack the reasoning power for complex problems.

## Solution
Implement a two-tier system with specialized roles:

- **The Worker (Claude Sonnet 4):** Fast, capable, and cost-effective agent handling bulk tool use and code generation
- **The Oracle (OpenAI o3 / Gemini 2.5 Pro):** Powerful, expensive model reserved for high-level reasoning, architectural planning, and debugging complex issues

The Worker can explicitly request Oracle consultation when stuck or needing better strategy. The Oracle reviews the Worker's approach and suggests course corrections without polluting the main agent's context.

```mermaid
graph TD
    A[User Request] --> B[Worker Agent]
    B --> C{Need Oracle?}
    C -->|Yes| D[Oracle Consultation]
    C -->|No| E[Direct Execution]
    D --> F[Strategic Guidance]
    F --> G[Worker Implements]
    G --> H[Task Complete]
    E --> H
```

## How to use it
Development environments, complex coding tasks, architectural decisions, debugging sessions where initial approaches fail.

## Trade-offs
* **Pros:** Cost-efficient use of frontier models; sophisticated problem-solving; specialized AI team approach
* **Cons:** Additional orchestration complexity; potential latency from model switching; requires careful Oracle invocation logic

## References
* Sourcegraph Team presentation on multi-model AI systems

---

## Parallel Tool Call Learning

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://youtu.be/1s_7RMG4O4U

## Problem

Agents often execute tool calls sequentially even when they could run in parallel:

- **Unnecessary latency**: Sequential calls add up when tool execution time dominates inference time
- **Inefficient exploration**: Agent waits for one result before deciding the next action
- **Poor tool utilization**: Multiple independent information needs handled one-by-one
- **Suboptimal learned behavior**: Base models may not naturally parallelize without training signal

**Example bottleneck:**

```
Sequential (slow):
1. search("Intel financial data") → 2s
2. read_file("2023_report.pdf") → 1.5s
3. search("return metrics") → 2s
4. read_file("returns_table.csv") → 1.5s

Total: 7 seconds
```

Cognition observed this with Devon: the baseline model would make 8-10 sequential tool calls during file planning, taking significant time even though many calls could have run in parallel.

## Solution

**Use Agent RFT to teach the model to parallelize independent tool calls, dramatically reducing latency when tool execution is faster than inference.**

**How Models Learn Parallelization:**

During RL exploration, the agent discovers that:

1. Multiple tool calls can be made simultaneously
2. When tool results arrive together, the next reasoning step has more context
3. Parallel patterns receive similar rewards in less time (implicit efficiency reward)
4. The model naturally converges toward parallel execution patterns

**Natural Emergence through RL:**

Unlike explicit programming, the parallelization emerges from:

- **Exploration**: Agent tries different tool call patterns
- **Reward shaping**: Faster completions may get slight bonuses (optional)
- **Efficiency pressure**: Light penalty on token usage encourages efficiency
- **Pattern reinforcement**: Successful parallel patterns get reinforced

**Typical Learned Pattern:**

```
Parallel (fast):
Batch 1 (parallel):
  
- search("Intel financial data")
  - read_file("2023_report.pdf")
  - search("return metrics")
  - list("/financial_reports")
→ All complete in ~2s (dominated by slowest)

Batch 2 (parallel, based on Batch 1 results):
  
- read_file("returns_table.csv")
  - read_file("competitor_data.csv")
→ Complete in ~1.5s

Total: ~3.5s (50% faster)
```

## How to use it

**Prerequisites:**

Your infrastructure must support parallel tool execution:

```python
# Tool server must handle concurrent requests
@app.cls(
    image=base_image,
    concurrency_limit=10,  # Allow 10 concurrent tools per rollout
    allow_concurrent_inputs=True
)
class ParallelToolExecutor:
    @method()
    async def execute_tool(self, rollout_id: str, tool: str, params: dict):
        # Use async for I/O-bound operations
        result = await self._async_execute(tool, params)
        return result
```

**Training Setup:**

No special configuration needed - parallelization emerges naturally:

```python
# Standard Agent RFT setup
job = client.fine_tuning.jobs.create(
    training_file="file-abc123",
    model="gpt-4o",
    method="rft",
    rft={
        "tools": tools,
        "grader": grader,
        "hyperparameters": {
            "n_epochs": 3,
            "batch_size": 16,
            "compute_multiplier": 1
        }
    }
)

# No special "parallelization" flag needed!
# Model discovers this pattern during exploration
```

**Optional: Explicit Latency Rewards**

You can encourage parallelization through reward shaping:

```python
class LatencyAwareGrader:
    def grade(self, question, answer, tool_trace, ground_truth):
        # Standard correctness score
        correctness = self.check_correctness(answer, ground_truth)

        # Bonus for efficiency
        num_sequential_rounds = self.count_sequential_rounds(tool_trace)

        if num_sequential_rounds <= 3:
            efficiency_bonus = 0.1
        elif num_sequential_rounds <= 5:
            efficiency_bonus = 0.05
        else:
            efficiency_bonus = 0.0

        return {
            "score": correctness + efficiency_bonus,
            "subscores": {
                "correctness": correctness,
                "efficiency": efficiency_bonus
            }
        }

    def count_sequential_rounds(self, tool_trace):
        """
        Count how many back-and-forth rounds with tools
        Parallel calls in same round = 1 round
        """
        rounds = 0
        current_round_calls = set()

        for call in tool_trace:
            if call['type'] == 'tool_call':
                current_round_calls.add(call['id'])
            elif call['type'] == 'tool_response':
                if call['call_id'] in current_round_calls:
                    current_round_calls.remove(call['call_id'])
                    if not current_round_calls:
                        rounds += 1

        return rounds
```

**Monitoring Parallelization:**

Track during training to see if model learns parallel patterns:

```python
def analyze_parallelization(tool_trace):
    """
    Analyze how many parallel calls the agent made
    """
    parallel_batches = []
    current_batch = []

    for event in tool_trace:
        if event['type'] == 'tool_call':
            current_batch.append(event)
        elif event['type'] == 'assistant_message':
            # End of reasoning, start of new tool batch
            if current_batch:
                parallel_batches.append(len(current_batch))
                current_batch = []

    return {
        'num_batches': len(parallel_batches),
        'calls_per_batch': parallel_batches,
        'max_parallelism': max(parallel_batches) if parallel_batches else 0,
        'total_calls': sum(parallel_batches)
    }

# Example output showing learned parallelization:
# Baseline: {'num_batches': 8, 'calls_per_batch': [1,1,1,1,1,1,1,1], 'max_parallelism': 1}
# Fine-tuned: {'num_batches': 2, 'calls_per_batch': [4,2], 'max_parallelism': 4}
```

## Real-World Example: Cognition Devon

**Task**: File planning agent - identify which files need editing

**Tools Available**:

- `read_file(path)`: Read file contents (~500ms)
- `shell(command)`: Run grep/find/ls (~300ms)

**Baseline Behavior (Before RFT):**

Sequential pattern observed in traces:

```
1. shell("find . -name '*.py'")
2. read_file("main.py")
3. shell("grep 'UserAuth' .")
4. read_file("auth.py")
5. shell("grep 'DatabaseConnection' .")
6. read_file("db.py")
7. shell("ls tests/")
8. read_file("tests/test_auth.py")

Total: 8-10 sequential tool calls
```

**After Agent RFT:**

Learned parallel pattern:

```
Round 1 (parallel):
  
- shell("find . -name '*.py'")
  - shell("grep 'UserAuth' .")
  - shell("grep 'DatabaseConnection' .")
  - shell("ls tests/")

Round 2 (parallel, based on Round 1):
  
- read_file("main.py")
  - read_file("auth.py")

Round 3 (if needed):
  
- read_file("db.py")

Total: 3-4 rounds (50% reduction in back-and-forth)
```

**Results:**

- Planning time reduced by ~50%
- Latency improvement even more dramatic when tool execution < inference time
- Model learned this without explicit parallelization reward

**Sam's Observation:**

> "We noticed that the model starts learning how to do a lot of parallel tool calls. The first action that the model does, it will kick off like eight different things... and then following on it will independently explore all of those things by again running more parallel tool calls."

## When Parallelization Helps Most

**High Impact Scenarios:**

1. **Tool execution << Inference time**
   - If tools return in 100ms but inference takes 2s, parallelization saves multiple inference rounds
   - Example: API calls, database queries, file reads

2. **Independent information gathering**
   - Multiple search queries
   - Reading multiple files
   - Checking multiple conditions

3. **Broad exploration phases**
   - Initial reconnaissance (find all Python files, check all tests, etc.)
   - Multi-source research (check docs, code, tests simultaneously)

**Low Impact Scenarios:**

1. **Sequential dependencies**
   - Must read file A to know which file B to read
   - Each tool result determines next action

2. **Tool execution >> Inference time**
   - If each tool takes 10s and inference takes 1s, parallelization saves less
   - Example: Heavy computation, large file processing

3. **Rate-limited APIs**
   - External APIs that throttle concurrent requests
   - Parallel calls just hit rate limits

## Comparison with Explicit Parallelization

| Approach | Agent RFT Learning | Manual Programming |
|----------|-------------------|-------------------|
| **Implementation** | Emerges from training | Explicit parallel tool API |
| **Flexibility** | Adapts to task | Fixed strategy |
| **Dependencies** | Learns safe patterns | Must hand-code logic |
| **Optimization** | Optimizes for your tools | Generic parallelization |
| **Maintenance** | Automatic with retraining | Manual updates |

```mermaid
graph TD
    A[Agent Receives Task] --> B{RL-Trained Model}

    B --> C[Identifies Independent Queries]
    C --> D[Batch 1: Parallel Tool Calls]
    D --> E[All Tools Execute Concurrently]

    E --> F[Results Arrive Together]
    F --> G{Need More Info?}

    G -->|Yes| H[Batch 2: Parallel Tool Calls]
    H --> I[Execute Concurrently]
    I --> J[Aggregate Results]

    G -->|No| J[Aggregate Results]
    J --> K[Generate Answer]

    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style H fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style K fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
```

## Trade-offs

**Pros:**

- **Dramatic latency reduction**: 40-50% reduction common when applicable
- **No manual coding**: Parallelization emerges from training, not programming
- **Adaptive**: Model learns optimal parallelization for your specific tools and tasks
- **Scales naturally**: Works across different numbers of tools and complexity levels
- **Better UX**: Faster agent responses improve user experience

**Cons:**

- **Infrastructure requirements**: Tool servers must handle concurrent requests
- **Resource usage**: More simultaneous tool calls = higher peak resource usage
- **Doesn't always emerge**: Requires enough variance in training data
- **May need reward shaping**: Explicit latency bonuses can help if parallelization doesn't emerge naturally
- **Debugging complexity**: Parallel execution makes traces harder to follow

## Implementation Checklist

- [ ] Tool infrastructure supports concurrent requests (async, multiple workers)
- [ ] Each tool can handle being called multiple times in parallel safely
- [ ] Monitoring tracks parallelization metrics (calls per batch, rounds per rollout)
- [ ] Reward function doesn't penalize parallelization (e.g., per-tool-call costs)
- [ ] Training data has variance in tool call patterns
- [ ] Optional: Latency-aware reward shaping to encourage parallelization
- [ ] Load testing confirms infrastructure can handle burst parallelism

## References

- [OpenAI Build Hour: Agent RFT - Cognition Case Study (November 2025)](https://youtu.be/1s_7RMG4O4U)
- [Parallel Tool Execution Pattern](./parallel-tool-execution.md)
- Related patterns: Agent Reinforcement Fine-Tuning, Tool Use Incentivization via Reward Shaping

---

## Patch Steering via Prompted Tool Selection

**Status:** best-practice
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=Xkwok_XXQgw

## Problem

Coding agents with access to multiple patching or refactoring tools (e.g., `apply_patch`, `AST-refactorer`, `codemod`) may choose suboptimal tools if not explicitly guided. This leads to:

- **Unnecessary Complexity:** Agent might use a generic text-replace tool instead of a specialized AST-aware refactoring tool.
- **Inconsistent Results:** Without explicit instructions, the agent's tool selection can vary unpredictably, hampering reproducibility.

## Solution

**Steer** the agent's tool selection and patch approach through **explicit natural language instructions** in the prompt. Techniques include:

**1. Direct Tool Invocation**
- Prepend: "Use the `apply_patch` tool to insert a new function `validate_input` in `auth_service.py`."
- The agent recognizes "apply_patch" as the preferred patching mechanism.

**2. Tool Usage Teaching**
- Provide a mini-manual in the context:
  ```text
  "Our `ASTRefactor` tool takes JSON describing node edits:  
  {"file": string, "pattern": string, "replacement": string}.  
  Use it for safe refactors rather than raw `sed` commands."  
  ```
- This orients the agent to use the safer, higher-level refactoring tool when modifying function signatures or renaming classes.

**3. Implicit Shorthands**
- Introduce domain-specific abbreviations: "When instructing to rename variables, say `renameVar(old, new)`; the agent maps this to the ASTRefactor tool under the hood."

**4. Reason-Encouraging Phrases**
- Add: "Think about type safety before choosing a patch tool."
- Promotes deeper reasoning so the agent doesn't just apply surface-level text replacements.

## Example

```mermaid
flowchart TD
    A[User Prompt: "Refactor validation logic"] --> B[Augmented Prompt]
    B --> C["Please use ASTRefactor to update function signatures"]
    C --> D[Agent selects ASTRefactor]
    D --> E[ASTRefactorTool patches code]
```

## How to use it

- **Tool Registry:** Expose tool metadata (name, usage example, input schema) in the agent's initialization context.
- **Prompt Templates:** Create reusable templates with placeholders, e.g.:
  ```
  "Task: {task_description}. Preferred tool: {tool_name}.  
  Usage example: {tool_usage_snippet}."
  ```
- **Fallback Handling:** If the agent ignores the instruction and uses the wrong tool, include a directive: "If ASTRefactor fails, fallback to apply_patch."

## Trade-offs

- **Pros:**
  - **Predictable Behavior:** Reduces variance in tool usage for the same task.
  - **Higher Code Quality:** Ensures the agent uses semantically safe tools (e.g., AST-based) over string-based replacements.
- **Cons/Considerations:**
  - **Prompt Length:** Excessive tool documentation in the prompt can consume valuable tokens.
  - **Maintenance:** As new patching tools emerge, templates and tool registry need periodic updates.

## References

- Adapted from "Tool Use Steering via Prompting" in Claude Code best practices.
- Will Brown's notes on "if you want it to be a tool use agent" you must decide that's the default behavior in the prompt.

---

## PII Tokenization

**Status:** established
**Category:** Security & Safety
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.anthropic.com/engineering/code-execution-with-mcp

## Problem

AI agents often need to process workflows involving personally identifiable information (PII) such as emails, phone numbers, addresses, or financial data. However, sending raw PII through the model's context creates privacy risks and compliance concerns. Organizations need agents to orchestrate data workflows without exposing sensitive information to the LLM.

## Solution

Implement an interception layer in the Model Context Protocol (MCP) client that automatically tokenizes PII before it reaches the model, and untokenizes it when making subsequent tool calls.

**Architecture:**

```mermaid
graph LR
    A[Tool Response] --> B[MCP Client]
    B --> C{PII Detection}
    C --> D[Tokenization]
    D --> E[Model Context]
    E --> F[Model Reasoning]
    F --> G[Tool Call Request]
    G --> H[Untokenization]
    H --> I[Actual Tool Call]
```

**Flow:**

1. **Interception**: When tools return data, MCP client intercepts responses
2. **Detection**: Identify PII using pattern matching or classification models
3. **Tokenization**: Replace real values with placeholders
   - `john.doe@company.com` → `[EMAIL_1]`
   - `(555) 123-4567` → `[PHONE_1]`
   - `123-45-6789` → `[SSN_1]`
4. **Model reasoning**: Agent works with tokenized placeholders
5. **Untokenization**: When agent makes tool calls with placeholders, MCP client substitutes real values back

**Example workflow:**

```python
# Tool returns customer data
customer = get_customer(id="C123")
# Raw: {"name": "John Doe", "email": "john@example.com", "phone": "555-1234"}

# MCP client tokenizes before sending to model
# Context sees: {"name": "[NAME_1]", "email": "[EMAIL_1]", "phone": "[PHONE_1]"}

# Agent reasons with tokens
"Send welcome email to [EMAIL_1] with link for [NAME_1]"

# MCP client untokenizes for tool execution
send_email(
    to="john@example.com",  # Real value substituted
    body="Welcome John Doe, here's your link..."  # Real value substituted
)
```

## How to use it

**Best for:**

- Workflows processing customer data, HR records, medical information
- Multi-step automation involving PII
- Compliance-sensitive environments (GDPR, HIPAA, CCPA)
- Agents that coordinate data flows without needing to "see" raw PII

**Implementation requirements:**

1. **PII detection layer:**
   - Regex patterns for common PII (email, phone, SSN, credit cards)
   - Named entity recognition models for names, addresses
   - Custom rules for domain-specific sensitive data

2. **Token mapping storage:**
   - Secure mapping of tokens to real values
   - Session-scoped or request-scoped lifetime
   - Encryption at rest if persistent

3. **Untokenization in tool calls:**
   - Scan outgoing tool call parameters
   - Replace placeholders with real values before execution
   - Maintain referential integrity (same placeholder → same value)

**Integration point:**

Most effective when implemented in the MCP client layer, so it's transparent to both the agent (sees tokens) and tools (see real values).

## Trade-offs

**Pros:**

- Prevents raw PII from entering model context
- Agents can orchestrate sensitive workflows without seeing data
- Enables audit trails that don't contain PII
- Reduces compliance risk and regulatory burden
- Transparent to agent reasoning (works with placeholders)

**Cons:**

- Adds complexity to MCP client implementation
- PII detection must be accurate (false positives/negatives)
- Doesn't protect against PII inference (model might deduce sensitive info)
- Requires secure token mapping storage
- May complicate debugging (need to map tokens back for troubleshooting)
- Pattern matching can miss novel PII formats

**Limitations:**

- Doesn't prevent the model from learning patterns about PII structure
- Won't catch domain-specific sensitive data without custom rules
- Contextual PII (e.g., "my address is...") may leak before tokenization
- Not a substitute for proper access controls and encryption

## References

* Anthropic Engineering: Code Execution with MCP (2024)
* GDPR Guidelines on Pseudonymization
* NIST Privacy Framework

---

## Plan-Then-Execute Pattern

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2506.08837

## Problem
If tool outputs can alter the *choice* of later actions, injected instructions may redirect the agent toward malicious steps.

## Solution
Split reasoning into two phases:

1. **Plan phase** – LLM generates a *fixed* sequence of tool calls **before** it sees any untrusted data.  
2. **Execution phase** – Controller runs that exact sequence. Tool outputs may shape *parameters*, but **cannot change which tools run**.

```pseudo
plan = LLM.make_plan(prompt)      # frozen list of calls
for call in plan:
    result = tools.run(call)
    stash(result)                 # outputs isolated from planner
```

## How to use it

Great for email-and-calendar bots, SQL assistants, code-review helpers—any task where the action set is known but parameters vary.

### Claude Code Plan Mode

Claude Code implements this pattern through "plan mode" which shifts the agent into planning-only mode:

1. **User shifts to plan mode**: Explicitly request planning (e.g., shift+tab in Claude Code CLI)
2. **Agent generates detailed plan**: Creates step-by-step approach without executing
3. **Human reviews and approves**: Can modify plan before execution
4. **Execution phase**: Agent follows the approved plan

**Effectiveness:**

- Can **2-3x success rates** for complex tasks by aligning on approach first
- Prevents wasted work from wrong assumptions
- Allows human expertise to guide agent execution

**Dynamic boundary:**

The threshold of what requires planning changes with each model generation:

> "The boundary changes with every model in a surprising way. Newer models are more intelligent, so the boundary of what you need plan mode for got pushed out a little bit. Before you used to need to plan, now you don't." —Boris Cherny (Anthropic)

This means simpler tasks that once required planning can now be one-shot with more capable models (e.g., Sonnet 4.5 vs. Opus 4.1).

## Trade-offs

* **Pros:** Strong control-flow integrity; moderate flexibility.
* **Cons:** Content of outputs can still be poisoned (e.g., bad email body).

## References

* Beurer-Kellner et al., §3.1 (2) Plan-Then-Execute.
* Boris Cherny (Anthropic): "Plan mode... you kind of have to understand the limits and where you get in the loop. Plan mode can 2-3x success rates pretty easily if you align on the plan first."
* Boris Cherny: "The boundary changes with every model... newer models are more intelligent so the boundary of what you need plan mode for got pushed out."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Proactive Agent State Externalization

**Status:** emerging
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://cognition.ai/blog/devin-sonnet-4-5-lessons-and-challenges

## Problem
Modern models like Claude Sonnet 4.5 proactively attempt to externalize their state by writing summaries and notes (e.g., `CHANGELOG.md`, `SUMMARY.md`) to the file system without explicit prompting. However:

- Self-generated notes are often incomplete or miss crucial context
- Models may spend more tokens on documentation than actual problem-solving
- Performance can degrade when agents rely exclusively on their own summaries
- Knowledge gaps emerge from inadequate self-documentation

## Solution
Implement structured approaches to leverage and enhance the model's natural tendency toward state externalization:

**1. Guided Self-Documentation Framework**
- Provide templates and schemas for agent-generated notes
- Define minimum information requirements for state preservation
- Establish validation checkpoints for self-generated summaries

**2. Hybrid Memory Architecture**
- Combine agent self-documentation with external memory management
- Use agent notes as supplementary, not primary, state storage
- Implement fallback mechanisms when self-generated context is insufficient

**3. Progressive State Building**
- Encourage incremental note-taking throughout long sessions
- Structure documentation to capture decision rationale, not just actions
- Include explicit uncertainty markers and knowledge gaps

```pseudo
# Proactive state externalization framework
class ProactiveStateManager:
    def __init__(self):
        self.state_template = {
            "session_id": str,
            "current_objective": str,
            "completed_actions": List[Action],
            "pending_decisions": List[Decision],
            "knowledge_gaps": List[str],
            "confidence_scores": Dict[str, float]
        }
    
    def capture_agent_state(self, agent_notes):
        # Validate completeness of agent-generated notes
        structured_state = self.parse_agent_notes(agent_notes)
        missing_fields = self.validate_completeness(structured_state)
        
        if missing_fields:
            return self.prompt_for_clarification(missing_fields)
        
        return self.merge_with_external_memory(structured_state)
    
    def guide_note_taking(self, current_context):
        return f"""
        As you work, maintain notes in this format:
        ## Current Objective
        {current_context.objective}
        
        ## Progress Summary
        - What you've completed
        - What you're currently working on
        - What's next
        
        ## Decision Log
        - Key decisions made and why
        - Alternatives considered
        - Confidence levels
        
        ## Knowledge Gaps
        - What you don't know
        - What needs clarification
        """
```

## How to use it
Best applied in scenarios where agents work on extended tasks:

- **Long-Running Development Sessions**: Multi-hour coding projects requiring state continuity
- **Research and Analysis**: Complex investigations spanning multiple sessions
- **Subagent Coordination**: When main agents need to communicate state to spawned subagents

Monitor self-documentation quality and supplement with external memory systems when agent notes prove insufficient.

## Trade-offs

* **Pros:** Leverages natural model behavior; enables better session continuity; facilitates subagent communication; creates audit trails
* **Cons:** May consume tokens on documentation over progress; requires validation overhead; risk of incomplete self-assessment; potential for "documentation theater"

## References
* [Cognition AI: Devin & Claude Sonnet 4.5 - Lessons and Challenges](https://cognition.ai/blog/devin-sonnet-4-5-lessons-and-challenges)
* Related: [Episodic Memory Retrieval & Injection](episodic-memory-retrieval-injection.md)

---

## Proactive Trigger Vocabulary

**Status:** emerging
**Category:** UX & Collaboration
**Authors:** Lucas Carlson
**Source:** https://github.com/anthropics/claude-code

## Problem

Agents with many skills face a routing problem: given a user's natural language input, which skill should handle it? Solutions like embedding-based similarity or LLM classification work but are opaque—users don't know what phrases will activate which capabilities.

Additionally, agents may have skills that should activate *proactively* (without explicit request) when certain topics arise, but without explicit trigger lists, the agent may miss opportunities or activate inappropriately.

## Solution

Define an explicit **trigger vocabulary** for each skill: a list of phrases, keywords, and patterns that should activate that skill. Document these triggers visibly so both humans and agents know the activation criteria.

```yaml
# Skill definition with explicit triggers
skill: priority-report
description: Generate prioritized task report

triggers:
  exact: ["sup", "priority report", "standup prep"]
  contains: ["what should I work on", "what's pending", "my tasks"]
  patterns: ["what.*on my plate", "action items"]

proactive: true  # Activate without explicit request when triggers match
```

```mermaid
graph TD
    A[User Input] --> B{Match Triggers?}
    B -->|"sup"| C[priority-report skill]
    B -->|"search hn"| D[hn-search skill]
    B -->|"check servers"| E[salt-monitoring skill]
    B -->|No match| F[General response]
```

**Key components:**

1. **Trigger lists**: Explicit phrases per skill, documented in skill definitions
2. **Proactive flag**: Whether skill should auto-activate on trigger match
3. **Priority ordering**: When multiple skills match, which takes precedence
4. **User visibility**: Triggers documented so users learn the vocabulary

## How to use it

**Skill documentation format:**

```markdown
## Priority Report

Use the `priority-report` skill when user asks about:

- What they need to work on next
- Priority tasks or action items
- Outstanding reviews, PRs, or issues

**Proactive triggers:** "sup", "priority report", "what should I work on",
"task overview", "standup prep", "my tasks", "what's pending"

**Script:** `~/.claude/skills/priority-report/scripts/priority-report.sh`
```

**Implementation approaches:**

1. **Documentation-based** (simplest): List triggers in CLAUDE.md or skill docs; agent reads and matches
2. **Config-based**: YAML/JSON trigger definitions loaded at startup
3. **Hybrid**: LLM matches against documented triggers, falls back to semantic similarity

**Trigger design guidelines:**

- **Short phrases**: "sup", "check mail", "my tasks" (1-3 words)
- **Question patterns**: "what should I...", "where did..."
- **Domain keywords**: Platform names, technical terms
- **Casual variants**: "sup" alongside "priority report"
- **Avoid overlap**: Don't reuse triggers across skills

**Example trigger vocabulary:**

| Skill | Triggers |
|-------|----------|
| priority-report | "sup", "my tasks", "standup prep", "what's pending" |
| hn-search | "search hn", "hacker news", "find on hn" |
| magic-cafe | "magic trick", "what's hot in magic", "magic forum" |
| email-triage | "triage inbox", "urgent emails", "prioritize mail" |

## Trade-offs

**Pros:**

- **Transparent**: Users can learn trigger phrases, feel in control
- **Predictable**: Same input always routes to same skill
- **Debuggable**: Easy to see why a skill activated (or didn't)
- **Fast**: String matching faster than embedding lookup
- **Documentable**: Triggers become part of user-facing docs
- **Proactive**: Agent can jump in when relevant topics arise

**Cons:**

- **Rigid**: Misses paraphrases not in trigger list
- **Maintenance**: Must update triggers as vocabulary evolves
- **Conflicts**: Multiple skills may want same triggers
- **Cultural/language bias**: Triggers may not translate
- **Discovery**: Users must learn the vocabulary (or read docs)

**Hybrid approach:**

Combine explicit triggers with semantic fallback:
1. Check explicit trigger matches first (fast, predictable)
2. If no match, use embedding similarity (flexible, slower)
3. Log unmatched inputs to discover new trigger candidates

## References

* Claude Code CLAUDE.md skill documentation pattern
* Intent classification in conversational AI
* Chatbot trigger/response pattern matching
* Slack workflow triggers

---

## Progressive Autonomy with Model Evolution

**Status:** best-practice
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

Agent scaffolding built for older models becomes unnecessary overhead as models improve:

- **Prompt bloat**: System prompts accumulate instructions that newer models don't need
- **Over-engineered flows**: Complex orchestration for tasks models can now handle directly
- **Wasted tokens**: Paying for instructions the model already knows
- **Slower execution**: Unnecessary steps add latency
- **Maintenance burden**: More code to maintain for diminishing benefit

Models improve faster than scaffolding is removed, creating technical debt.

## Solution

**Actively remove scaffolding** as models become more capable. Regularly audit system prompts, orchestration logic, and agent architecture to eliminate what newer models have internalized.

**Core principle**: Push complexity into the model itself rather than external scaffolding.

```mermaid
graph LR
    A[Model v1] --> B[Needs Scaffolding]
    B --> C[Complex System Prompts]
    C --> D[Model v2 Released]
    D --> E[Remove Unnecessary Instructions]
    E --> F[Simpler, Faster Agent]
    F --> G[Model v3 Released]
    G --> H[Further Simplification]
```

**Example evolution:**

```pseudo
# Claude Opus 4.1 (older model)
system_prompt = """
When writing code:
1. First check if file exists
2. Read current contents
3. Plan your changes
4. Make minimal edits
5. Verify syntax
... [2000 more tokens of instructions]
"""

# Claude Sonnet 4.5 (newer model)
system_prompt = """
Write clean, tested code.
"""  # Model already knows the steps
```

## How to use it

**Regular audit process:**

1. **Track model releases**: Note when new models become available
2. **Test simplified prompts**: Remove instructions and see if quality degrades
3. **Measure token usage**: Quantify savings from prompt reduction
4. **A/B test scaffolding**: Compare outcomes with and without orchestration steps
5. **Delete what works**: If model performs equally without scaffolding, remove it

**What to look for:**

- Instructions that are "obvious" to humans (likely obvious to advanced models)
- Multi-step workflows models now handle in one turn
- Error-handling that models build in automatically
- Format specifications models infer from context
- Planning steps models do internally with extended thinking

**Real example from Claude Code:**

> "I just deleted like 2,000 tokens or something from the system prompt yesterday. Just because Sonnet 4.5 doesn't need it anymore. But Opus 4.1 did need it." —Boris Cherny

**Boundary evolution:**

> "The boundary changes with every model in a surprising way, where the newer models, they're more intelligent. So the boundary of what you need plan mode for got pushed out a little bit." —Boris Cherny

## Trade-offs

**Pros:**

- **Reduced token costs**: Shorter prompts = cheaper inference
- **Faster execution**: Less processing overhead
- **Simpler maintenance**: Less code/prompts to manage
- **Future-proof**: Embraces model capabilities rather than fighting them
- **Better performance**: Models often work better with less hand-holding

**Cons:**

- **Requires testing**: Must validate that quality doesn't degrade
- **Version management**: May need different configs for different models
- **Loss of explicit control**: Less visibility into model's internal reasoning
- **Risk of regression**: Removing too much can hurt performance
- **Documentation debt**: May lose understanding of why scaffolding was added

**Strategic considerations:**

- **When to remove**: After new model is proven stable in production
- **How much to remove**: Start conservative, measure, iterate
- **What to keep**: Domain-specific knowledge models can't know
- **Migration path**: Support multiple model versions during transition

## References

* Boris Cherny: "I just deleted like 2,000 tokens or something from the system prompt yesterday. Just because Sonnet 4.5 doesn't need it anymore. But Opus 4.1 did need it."
* Boris Cherny: "There's this frontier where you need to give the model a hard enough task to really push the limit... I think this is a general trend of stuff that used to be scaffolding with a more advanced model, it gets pushed into the model itself. The model kind of tends to subsume everything over time."
* Cat Wu: "We build most things that we think would improve Claude Code's capabilities, even if that means we'll have to get rid of it in three months. If anything, we hope that we will get rid of it in three months."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Progressive Complexity Escalation

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://vercel.com/blog/what-we-learned-building-agents-at-vercel

## Problem

Organizations deploy AI agents with overly ambitious capabilities from day one, leading to:

- Unreliable outputs when agents tackle tasks beyond current model capabilities
- Failed implementations that damage stakeholder confidence
- Complex reasoning tasks producing inconsistent results
- Wasted engineering effort building infrastructure for capabilities models can't yet deliver
- Safety risks from autonomous execution of high-stakes operations

The gap between theoretical agent capabilities and practical reliability creates deployment failures.

## Solution

Design agent systems to start with low-complexity, high-reliability tasks and progressively unlock more complex capabilities as models improve and trust is established. Match task complexity to current model capabilities rather than building to theoretical potential.

**Core principles:**

**Start with proven sweet spots:**

- Low cognitive load tasks with high repetition
- Tasks "too dynamic for traditional automation, but predictable enough for AI to handle reliably"
- Information gathering and synthesis over complex reasoning
- Well-defined success criteria

**Define capability tiers:**

```
Tier 1 (Deploy immediately):

- Data entry and research
- Content categorization
- Information extraction
- Template-based generation

Tier 2 (Unlock with validation):

- Multi-step workflows with human gates
- Conditional logic with structured outputs
- Integration with multiple tools
- Personalization and adaptation

Tier 3 (Future unlock):

- Autonomous decision-making
- Complex reasoning chains
- Creative problem-solving
- Novel task generalization
```

**Progressive unlock mechanisms:**

- Performance metrics trigger capability expansion
- Human review gates before promoting to higher tiers
- A/B testing new capabilities against baselines
- Gradual rollout with monitoring

**Example workflow evolution:**

```mermaid
graph TD
    subgraph "Phase 1: Information Gathering"
        A[Agent researches lead data] --> B[Presents findings to human]
        B --> C[Human writes email]
    end

    subgraph "Phase 2: Structured Generation"
        D[Agent researches + qualifies] --> E[Agent drafts email]
        E --> F[Human approves/edits]
        F --> G[Human sends]
    end

    subgraph "Phase 3: Conditional Automation"
        H[Agent researches + qualifies] --> I{Confidence > 0.8?}
        I -->|Yes| J[Auto-send email]
        I -->|No| K[Human review]
    end

    C -.Proven reliable.-> D
    G -.Proven reliable.-> H

    style A fill:#90EE90
    style D fill:#FFD700
    style H fill:#FF6347
```

## How to use it

**When to apply:**

- Deploying agents into production environments
- Building internal automation tools
- Customer-facing agent applications
- High-stakes or regulated domains
- New agent capabilities with unproven reliability

**Implementation approach:**

**1. Classify task complexity:**

```python
class TaskComplexity:
    LOW = {
        'cognitive_load': 'minimal',
        'steps': 1-3,
        'tools': 0-2,
        'reasoning_depth': 'shallow',
        'error_impact': 'low'
    }

    MEDIUM = {
        'cognitive_load': 'moderate',
        'steps': 4-8,
        'tools': 2-5,
        'reasoning_depth': 'multi-step',
        'error_impact': 'medium'
    }

    HIGH = {
        'cognitive_load': 'significant',
        'steps': '8+',
        'tools': '5+',
        'reasoning_depth': 'deep/creative',
        'error_impact': 'high'
    }
```

**2. Define promotion criteria:**

```yaml
capability_gates:
  tier1_to_tier2:
    
- accuracy_threshold: 0.95
    - human_approval_rate: 0.90
    - volume_processed: 1000
    - time_in_production: 30_days

  tier2_to_tier3:
    
- accuracy_threshold: 0.98
    - human_override_rate: 0.05
    - volume_processed: 10000
    - stakeholder_confidence: high
```

**3. Implement capability flags:**

```typescript
class AgentCapabilities {
  constructor(private tier: 1 | 2 | 3) {}

  async processLead(lead: Lead) {
    const research = await this.research(lead); // Tier 1

    if (this.tier >= 2) {
      const qualification = await this.qualify(research);
      const email = await this.generateEmail(qualification);

      if (this.tier >= 3 && qualification.confidence > 0.8) {
        return this.autoSend(email); // Autonomous execution
      }

      return this.requestApproval(email); // Human gate
    }

    return this.presentFindings(research); // Tier 1 only
  }
}
```

**4. Monitor and promote:**

- Track success metrics per tier
- Review error patterns and edge cases
- Gradually expand agent authority
- Maintain rollback capability

**Prerequisites:**

- Clear success metrics for each capability tier
- Monitoring and observability infrastructure
- Stakeholder alignment on progression plan
- Fallback mechanisms for failures

## Trade-offs

**Pros:**

- **Risk mitigation:** Limits blast radius of agent errors
- **Stakeholder confidence:** Builds trust through proven reliability
- **Focused engineering:** Resources spent on proven capabilities
- **Graceful degradation:** System remains useful even at lower tiers
- **Model evolution readiness:** Architecture prepared for capability growth
- **Realistic expectations:** Aligns deployment with actual model performance

**Cons:**

- **Delayed value:** Full automation benefits realized over time, not immediately
- **Complexity:** Requires tier management and promotion logic
- **Maintenance overhead:** Multiple capability paths to test and maintain
- **Promotion friction:** Manual review of metrics and promotion decisions
- **User confusion:** Inconsistent capabilities across deployment phases
- **Engineering investment:** Building infrastructure for future capabilities

**Balancing approaches:**

- Clearly communicate capability roadmap to stakeholders
- Automate tier promotion based on objective metrics
- Maintain simple mental model (what can the agent do today?)
- Design for capability growth from day one

## References

- [Vercel: What We Learned Building Agents](https://vercel.com/blog/what-we-learned-building-agents-at-vercel) - "Start with low-cognitive-load automation, then evolve as capabilities mature"
- [Anthropic: Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) - Task complexity and model capability matching
- [OpenAI: GPT Best Practices](https://platform.openai.com/docs/guides/prompt-engineering) - Matching task complexity to model strengths
- Related patterns: [Progressive Autonomy with Model Evolution](progressive-autonomy-with-model-evolution.md), [Human-in-the-Loop Approval Framework](human-in-loop-approval-framework.md), [Spectrum of Control / Blended Initiative](spectrum-of-control-blended-initiative.md)

---

## Progressive Disclosure for Large Files

**Status:** emerging
**Category:** Context & Memory
**Authors:** Nikola Balic (@nibzard)
**Source:** https://lethain.com/agents-large-files/

## Problem

Large files (PDFs, DOCXs, images) overwhelm the context window when loaded naively. A 5-10MB PDF may contain only 10-20KB of relevant text/tables, but the entire file is often shoved into context, wasting tokens and degrading performance.

## Solution

Apply **progressive disclosure**: load file metadata first, then provide tools to load content on-demand.

**Core approach:**

1. **Always include file metadata** in the prompt (not full content):

   ```
   Files:
     - id: f_a1
       name: my_image.png
       size: 500,000
       preloaded: false
     - id: f_b3
       name: report.pdf
       size: 8500000
       preloaded: false
   ```

2. **Optionally preload first N KB** of appropriate mimetypes (configurable per-workflow, can be 0)

3. **Provide three file operations:**

   - `load_file(id)` - Load entire file into context
   - `peek_file(id, start, stop)` - Load a section of file
   - `extract_file(id)` - Transform PDF/DOCX/PPT into simplified text

4. **Include a `large_files` skill** explaining when/how to use these tools

```pseudo
# Agent workflow for document comparison
1. Prompt includes file metadata for report_2024.pdf and report_2025.pdf
2. Agent sees large PDFs, checks large_files skill
3. Agent calls: extract_file("report_2024.pdf")
4. Agent calls: extract_file("report_2025.pdf")
5. Agent compares extracted summaries using minimal context
```

```pseudo
# Agent workflow for image analysis
1. Prompt includes metadata for screenshot.png
2. Agent sees PNG type, calls: load_file("screenshot.png")
3. Image content is loaded, agent analyzes visual content
```

## How to use it

**Best for:**

- Document comparison workflows (multiple PDFs)
- Ticket systems with file attachments (images, PDFs)
- Data export analysis (large reports in various formats)
- Any workflow where agents need file content but files are large

**Implementation considerations:**

- File `id` should be a stable reference for tool calls
- `extract_file` should return simplified text (tables, text content)
- Consider making `extract_file` return a virtual `file_id` for very large extractions
- Preloading first N KB is optional - can give agent initial context without full load

**Tool design:**

```python
def load_file(file_id: str) -> str:
    """Load entire file content into context window."""

def peek_file(file_id: str, start: int, stop: int) -> str:
    """Load a specific byte range from file."""

def extract_file(file_id: str) -> str:
    """Convert PDF/DOCX/PPT to simplified text representation."""
```

## Trade-offs

**Pros:**

- Enables working with files much larger than context window
- Agent has control over what/when to load
- Reusable across workflows via `large_files` skill
- Extracted content is often 100x smaller than original file

**Cons:**

- Adds tool call overhead (multiple round-trips)
- Requires preloading heuristics (how much is enough?)
- Extraction from complex formats (DOCX) can be slow without native dependencies
- Agent may make poor loading decisions without proper guidance

**Trade-offs in preloading:**

- **Preloading**: Gives agent immediate context but reduces control
- **No preloading**: Maximum agent control but requires explicit load calls

## References

* [Building an internal agent: Progressive disclosure and handling large files](https://lethain.com/agents-large-files/) - Will Larson (2025)
* Related: [Progressive Tool Discovery](progressive-tool-discovery.md) - Similar lazy-loading concept for tools
* Related: [Context-Minimization Pattern](context-minimization-pattern.md) - Complementary pattern for reducing context bloat

---

## Progressive Tool Discovery

**Status:** established
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.anthropic.com/engineering/code-execution-with-mcp

## Problem

When agents have access to large tool catalogs (dozens or hundreds of available tools), loading all tool definitions upfront consumes excessive context window space. Most tools won't be used in a given workflow, making this preloading wasteful and limiting the context available for actual task execution.

## Solution

Present tools through a filesystem-like hierarchy where agents discover capabilities on-demand by exploring the structure. Implement a `search_tools` capability that allows agents to request different levels of detail:

1. **Name only**: Minimal context for initial browsing
2. **Name + description**: Enough to understand tool purpose
3. **Full definition with schemas**: Complete API details only when needed

Tools are organized hierarchically (e.g., `servers/google-drive/getDocument.ts`, `servers/slack/sendMessage.ts`) so agents can:

- List the `./servers/` directory to see available integrations
- Navigate into specific server directories to find relevant tools
- Load full definitions only for tools they intend to use

```pseudo
# Agent workflow
1. list_directory("./servers/")
   → Returns: ["google-drive/", "slack/", "github/", ...]

2. search_tools(pattern="google-drive/*", detail_level="name+description")
   → Returns: Brief descriptions of Google Drive tools

3. get_tool_definition("servers/google-drive/getDocument")
   → Returns: Full JSON schema with parameters, types, examples
```

## How to use it

**Best for:**

- Systems with 20+ available tools or integrations
- Model Context Protocol (MCP) server implementations
- Plugin architectures where agents select from many capabilities

**Implementation considerations:**

- Organize tools in a clear hierarchy (by integration, by domain, by function)
- Provide meaningful names and descriptions at each level
- Support pattern matching (glob or regex) for tool searches
- Cache tool definitions that are frequently requested together

**Example directory structure:**

```
servers/
├── google-drive/
│   ├── getDocument.ts
│   ├── listFiles.ts
│   └── shareFile.ts
├── slack/
│   ├── sendMessage.ts
│   └── getChannels.ts
└── github/
    ├── createIssue.ts
    └── listRepos.ts
```

## Trade-offs

**Pros:**

- Dramatically reduces initial context consumption
- Scales to hundreds or thousands of tools
- Agents learn about tool ecosystem through exploration
- Natural mapping to code-based tool interfaces

**Cons:**

- Adds discovery overhead (extra tool calls before execution)
- Requires thoughtful organization and naming schemes
- Less effective if agents need most tools anyway
- May require multiple round-trips to find the right tool

## References

* Anthropic Engineering: Code Execution with MCP (2024)
* Model Context Protocol specification

---

## Reflection Loop

**Status:** established
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2303.11366

## Problem
Generative models may produce subpar output if they never review or critique their own work.

## Solution
After generating a draft, have the model grade it against a given metric and refine the response using that feedback.

```pseudo
for attempt in range(max_iters):
    draft = generate(prompt)
    score, critique = evaluate(draft, metric)
    if score >= threshold:
        return draft
    prompt = incorporate(critique, prompt)
```

## How to use it
Use when you care about quality or adherence to explicit criteria—writing, reasoning, or code. Loop until the score meets your bar or max iterations are reached.

## Trade-offs
* **Pros:** Improves outputs with little supervision.
* **Cons:** Extra compute; may stall if the metric is poorly defined.

## References
* [Self-Refine: Improving Reasoning in Language Models via Iterative Feedback](https://arxiv.org/abs/2303.11366)

---

## Rich Feedback Loops > Perfect Prompts

**Status:** validated-in-production
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/ampcode

## Problem
Polishing a single prompt can't cover every edge-case; agents need ground truth to self-correct.

## Solution
Expose **iterative, machine-readable feedback**—compiler errors, test failures, linter output, screenshots—after every tool call.
The agent uses diagnostics to plan the next step, leading to emergent self-debugging.

Modern models like Claude Sonnet 4.5 are increasingly proactive in creating their own feedback loops by writing and executing short scripts and tests, even for seemingly simple verification tasks (e.g., using HTML inspection to verify React app behavior).

## Example
```mermaid
sequenceDiagram
  Agent->>CLI: go test ./...
  CLI-->>Agent: FAIL pkg/auth auth_test.go:42 expected 200 got 500
  Agent->>File: open auth.go
  Agent->>File: patch route handler
  Agent->>CLI: go test ./...
  CLI-->>Agent: PASS 87/87 tests
```

## References

* Raising An Agent - Episode 1 & 3 discussions on "give it errors, not bigger prompts."
* [Cognition AI: Devin & Claude Sonnet 4.5](https://cognition.ai/blog/devin-sonnet-4-5-lessons-and-challenges) - observes proactive testing behavior and custom script creation for feedback loops

[Source](https://www.nibzard.com/ampcode)

---

## RLAIF (Reinforcement Learning from AI Feedback)

**Status:** emerging
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2212.08073

## Problem

Traditional Reinforcement Learning from Human Feedback (RLHF) requires extensive human annotation for preference data, which is expensive (often $1+ per annotation), time-consuming, and difficult to scale. This creates a bottleneck in training aligned AI systems, especially when dealing with complex or specialized domains where human expertise is scarce or costly.

## Solution

RLAIF uses AI models themselves to generate preference feedback and evaluation data, dramatically reducing costs to less than $0.01 per annotation while maintaining or improving quality. The approach involves:

1. **AI-Generated Critiques**: Use a language model to evaluate outputs based on a set of principles or constitution
2. **Preference Data Generation**: Have the AI model compare pairs of responses and select the better one according to specified criteria
3. **Synthetic Training Data**: Generate high-quality training examples using the AI's own capabilities
4. **Constitutional Principles**: Guide the feedback process with explicit rules rather than implicit human preferences

This technique forms the foundation of Constitutional AI and has become a default method in post-training and RLHF literature.

## Example

```python
class RLAIFAgent:
    def __init__(self, base_model, critic_model, constitution):
        self.base_model = base_model
        self.critic_model = critic_model
        self.constitution = constitution  # List of principles
    
    def generate_critique(self, prompt, response):
        critique_prompt = f"""
        Evaluate the following response according to these principles:
        {self.constitution}
        
        Prompt: {prompt}
        Response: {response}
        
        Provide specific feedback on:
        1. Adherence to principles
        2. Quality of response
        3. Suggested improvements
        """
        return self.critic_model.generate(critique_prompt)
    
    def generate_preference_data(self, prompt, response_a, response_b):
        comparison_prompt = f"""
        Given these principles: {self.constitution}
        
        Which response is better for the prompt: "{prompt}"
        
        Response A: {response_a}
        Response B: {response_b}
        
        Choose A or B and explain why according to the principles.
        """
        preference = self.critic_model.generate(comparison_prompt)
        return self.parse_preference(preference)
    
    def improve_response(self, prompt, initial_response, critique):
        improvement_prompt = f"""
        Original prompt: {prompt}
        Initial response: {initial_response}
        Critique: {critique}
        
        Generate an improved response addressing the critique.
        """
        return self.base_model.generate(improvement_prompt)
```

## Trade-offs

**Pros:**
- **Cost Efficiency**: 100x cheaper than human feedback ($0.01 vs $1+)
- **Scalability**: Can generate unlimited feedback data without human bottlenecks
- **Consistency**: AI feedback is more consistent than varying human annotators
- **Speed**: Near-instantaneous feedback generation

**Cons:**
- **Bias Amplification**: May reinforce existing model biases
- **Limited Novelty**: Cannot provide truly novel insights beyond model's training
- **Quality Variance**: Feedback quality depends on the critic model's capabilities
- **Principle Design**: Requires careful crafting of constitutional principles

## References

- [Constitutional AI: Harmlessness from AI Feedback (Anthropic, 2022)](https://arxiv.org/abs/2212.08073)
- [RLHF Book - Constitutional AI & AI Feedback](https://rlhfbook.com/c/13-cai.html)
- [OpenAI's CriticGPT announcement (July 2024)](https://openai.com/research/criticgpt)

---

## Seamless Background-to-Foreground Handoff

**Status:** emerging
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
While background agents can handle long-running, complex tasks autonomously, they might not achieve 100% correctness or perfectly match the user's nuanced intent. If an agent completes 90% of a task in the background but the remaining 10% requires human finesse, a clunky handoff process can negate the benefits of automation.

## Solution
Design the agent system to allow for a seamless transition from background (autonomous) agent work to foreground (human-in-the-loop or direct human control) work. This means:

1.  The background agent performs its task (e.g., generating a PR).
2.  The user reviews the agent's work.
3.  If the work is not entirely satisfactory (e.g., 90% correct), the user can easily "take control" or bring the task into their active foreground environment.
4.  The user can then utilize the same (or related) interactive AI tools and direct editing capabilities used in the foreground to refine, correct, or complete the remaining parts of the task.
5.  The context from the background agent's work should ideally be available to inform the foreground interaction.

This pattern ensures that developers can leverage the power of autonomous background processing while retaining the ability to easily intervene and apply their expertise for the final touches, without losing context or efficiency.

## Example
```mermaid
flowchart TD
    A[User: Refactor X in background] --> B[Background Agent: Works on X]
    B --> C[Agent Proposes PR for X]
    C --> D{User Reviews PR}
    D -->|90% Correct| E[User: Take over & refine]
    E --> F[User uses Foreground Agent Tools & IDE to complete X]
    F --> G[Finalized PR]
    D -->|100% Correct| G
```

## References
- Aman Sanger (Cursor) at 0:06:52: "...if it's only 90% of the way there, you want to go in and then take control and and do the rest of it. And then you want to use, you know, the features of Cursor in order to do that. So really being able to quickly move between the background and the foreground is really important."

---

## Self-Critique Evaluator Loop

**Status:** emerging
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2408.02666

## Problem
Human preference labels are costly and quickly become outdated as base models improve.

## Solution
Train a **self-taught evaluator** that bootstraps from synthetic data:

1. Generate multiple candidate outputs for an instruction.  
2. Ask the model to judge and explain which is better (reasoning trace).  
3. Fine-tune that judge on its own traces; iterate.  
4. Use the judge as a reward model or quality gate for the main agent.  
5. Periodically refresh with new synthetic debates to stay ahead of model drift.

## Pros & Cons
- **Pros:** near-human eval accuracy without labels; scales with compute.  
- **Cons:** risk of evaluator-model collusion; needs adversarial tests.

## References
- Wang et al., *Self-Taught Evaluators*

---

## Self-Discover: LLM Self-Composed Reasoning Structures

**Status:** emerging
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2402.03620

## Problem

Different reasoning tasks require different thinking strategies. While techniques like Chain-of-Thought (CoT) work well for some problems, they may be suboptimal for others. Current approaches typically use fixed reasoning patterns regardless of the specific problem at hand, leading to inefficient problem-solving and suboptimal performance on diverse tasks.

## Solution

Self-Discover enables LLMs to automatically discover and compose task-specific reasoning structures. The process involves:

1. **Task Analysis**: The LLM analyzes the problem to understand its requirements
2. **Strategy Selection**: From a set of atomic reasoning modules (like "break into steps", "think critically", "use examples"), the LLM selects relevant ones
3. **Structure Composition**: The selected modules are composed into a coherent reasoning structure tailored to the specific task
4. **Execution**: The problem is solved using the self-discovered structure

This approach allows the model to adapt its reasoning strategy to match the problem's unique characteristics, leading to significant performance improvements.

## Example

```python
class SelfDiscoverAgent:
    def __init__(self, llm):
        self.llm = llm
        self.reasoning_modules = [
            "Break the problem into smaller steps",
            "Think about similar problems you've seen",
            "Consider edge cases and exceptions",
            "Work backwards from the desired outcome",
            "Use concrete examples to test understanding",
            "Identify key constraints and requirements",
            "Consider multiple perspectives",
            "Check for logical consistency",
            "Simplify the problem first",
            "Look for patterns"
        ]
    
    def discover_reasoning_structure(self, task):
        # Step 1: Select relevant reasoning modules
        selection_prompt = f"""
        Task: {task}
        
        Available reasoning modules:
        {self.format_modules(self.reasoning_modules)}
        
        Select 3-5 most relevant modules for solving this task.
        Explain why each selected module is important for this problem.
        """
        selected_modules = self.llm.generate(selection_prompt)
        
        # Step 2: Adapt modules to the task
        adaptation_prompt = f"""
        Task: {task}
        Selected modules: {selected_modules}
        
        Adapt these generic modules into specific reasoning steps 
        tailored to this exact task. Make them concrete and actionable.
        """
        adapted_modules = self.llm.generate(adaptation_prompt)
        
        # Step 3: Compose into reasoning structure
        composition_prompt = f"""
        Task: {task}
        Adapted reasoning steps: {adapted_modules}
        
        Organize these into a coherent reasoning structure.
        Define the order of operations and how steps connect.
        Create a step-by-step reasoning plan.
        """
        reasoning_structure = self.llm.generate(composition_prompt)
        
        return reasoning_structure
    
    def solve_with_structure(self, task, reasoning_structure):
        solve_prompt = f"""
        Task: {task}
        
        Use this reasoning structure to solve the problem:
        {reasoning_structure}
        
        Follow each step carefully and show your work.
        """
        return self.llm.generate(solve_prompt)
    
    def self_discover_solve(self, task):
        # Discover optimal reasoning structure
        structure = self.discover_reasoning_structure(task)
        
        # Solve using discovered structure
        solution = self.solve_with_structure(task, structure)
        
        return {
            'reasoning_structure': structure,
            'solution': solution
        }
```

```mermaid
flowchart TD
    A[Input Task] --> B[Analyze Task Requirements]
    B --> C[Select Relevant Reasoning Modules]
    C --> D[Adapt Modules to Specific Task]
    D --> E[Compose Reasoning Structure]
    E --> F[Execute with Structure]
    F --> G[Solution]
    
    H[Module Library] --> C
    
    style E fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style H fill:#fff3e0,stroke:#e65100,stroke-width:2px
```

## Benefits

- **Task-Specific Optimization**: Reasoning approach matches problem requirements
- **Performance Gains**: Up to 32% improvement over Chain-of-Thought on challenging benchmarks
- **Interpretability**: Clear reasoning structure shows how the problem was approached
- **Transferability**: Discovered structures can be reused for similar problems

## Trade-offs

**Pros:**
- Significant performance improvements on diverse reasoning tasks
- More efficient than trying all reasoning strategies
- Creates reusable reasoning templates
- Adapts to novel problem types

**Cons:**
- Additional overhead for structure discovery phase
- Requires a diverse set of reasoning modules
- May over-engineer simple problems
- Structure quality depends on task analysis accuracy

## References

- [Self-Discover: Large Language Models Self-Compose Reasoning Structures (2024)](https://arxiv.org/abs/2402.03620)
- [Google DeepMind Research Blog](https://deepmind.google/research/)

---

## Self-Rewriting Meta-Prompt Loop

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://noahgoodman.substack.com/p/meta-prompt-a-simple-self-improving

## Problem
Static system prompts become stale or overly brittle as an agent encounters new tasks and edge-cases. Manually editing them is slow and error-prone.

## Solution
Let the agent **rewrite its own system prompt** after each interaction:

1. **Reflect** on the latest dialogue or episode.  
2. Draft improvements to the instructions (add heuristics, refine tool advice, retire bad rules).  
3. **Validate** the draft (internal sanity-check or external gate).  
4. Replace the old system prompt with the revised version; persist in version control.  
5. Use the new prompt on the next episode, closing the self-improvement loop.

```python
# pseudo-code
dialogue = run_episode()
delta = LLM("Reflect on dialogue and propose prompt edits", dialogue)
if passes_guardrails(delta):
    system_prompt += delta
    save(system_prompt)
```

## Trade-offs

**Pros:** rapid adaptation; no human in the loop for minor tweaks.
**Cons:** risk of drift or jailbreak—needs a strong guardrail step.

## References

* Goodman, *Meta-Prompt: A Simple Self-Improving Language Agent*. ([noahgoodman.substack.com](https://noahgoodman.substack.com/p/meta-prompt-a-simple-self-improving))

---

## Shell Command Contextualization

**Status:** established
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/claude-code

## Problem
When an AI agent interacts with a local development environment, it often needs to execute shell commands (e.g., run linters, check git status, list files) and then use the output of these commands as context for its subsequent reasoning or actions. Manually copying and pasting command output into the prompt is tedious and error-prone.

## Solution
Provide a dedicated mechanism within the agent's interface (e.g., a special prefix like `!` or a specific command mode) that allows the user to directly issue a shell command to be executed in the local environment. Crucially, both the command itself and its full output (stdout and stderr) are automatically captured and injected into the agent's current conversational or working context.

This ensures that the agent is immediately aware of the command that was run and its results, allowing it to seamlessly incorporate this information into its ongoing tasks without requiring manual data transfer by the user.

## Example (shell integration flow)
```mermaid
sequenceDiagram
    participant User
    participant Interface
    participant Shell
    participant Agent

    User->>Interface: !ls -la
    Interface->>Shell: Execute: ls -la
    Shell-->>Interface: Command output
    Interface->>Agent: Inject command + output
    Agent->>Agent: Process context
    Agent-->>User: Response with shell context
```

## Example
-   In Claude Code, typing `!ls -la` would execute `ls -la` locally, and both the command `!ls -la` and its output would be added to Claude's context.

## References
-   Based on the `!` (Exclamation mark) keybinding for Bash mode in "Mastering Claude Code: Boris Cherny's Guide & Cheatsheet," section V.

[Source](https://www.nibzard.com/claude-code)

---

## Skill Library Evolution

**Status:** established
**Category:** Learning & Adaptation
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.anthropic.com/engineering/code-execution-with-mcp

## Problem

Agents frequently solve similar problems across different sessions or workflows. Without a mechanism to preserve and reuse working code, agents must rediscover solutions each time, wasting tokens and time. Organizations want agents to build up capability over time rather than starting from scratch every session.

## Solution

Agents persist working code implementations as reusable functions in a `skills/` directory. Over time, these implementations evolve into well-documented, tested "skills" that become higher-level capabilities the agent can leverage.

**Evolution path:**

```mermaid
graph LR
    A[Ad-hoc Code] --> B[Save Working Solution]
    B --> C[Reusable Function]
    C --> D[Documented Skill]
    D --> E[Agent Capability]
```

**Basic pattern:**

```python
# Session 1: Agent writes code to solve a problem
def analyze_sentiment(text):
    # Implementation discovered through experimentation
    response = llm.complete(f"Analyze sentiment: {text}")
    return parse_sentiment(response)

# Agent saves working solution
with open("skills/analyze_sentiment.py", "w") as f:
    f.write(inspect.getsource(analyze_sentiment))
```

**Later session: Agent discovers and uses existing skill**

```python
# Agent explores skills directory
skills = os.listdir("skills/")
# Finds: ['analyze_sentiment.py', 'extract_entities.py', ...]

# Imports and uses existing skill
from skills.analyze_sentiment import analyze_sentiment

result = analyze_sentiment("Customer feedback here...")
```

**Evolved skill with documentation:**

```python
# skills/analyze_sentiment.py
"""
SKILL: Sentiment Analysis

Analyzes text sentiment using LLM completion and structured parsing.

Args:
    text (str): Text to analyze
    granularity (str): 'binary' or 'fine-grained' (default: 'binary')

Returns:
    dict: {'sentiment': str, 'confidence': float, 'aspects': list}

Example:
    >>> analyze_sentiment("Great product, fast shipping!")
    {'sentiment': 'positive', 'confidence': 0.92, 'aspects': ['product', 'shipping']}

Tested: 2024-01-15
Success rate: 94% on validation set
"""
def analyze_sentiment(text, granularity='binary'):
    # Refined implementation
    pass
```

**Progressive disclosure with on-demand loading (Imprint approach):**

Instead of loading all skills into context, inject skill descriptions into system prompt and provide a `load_skills` tool for full content:

```yaml
# skills/pdf-processing/SKILL.md
---
name: pdf-processing
description: Extract text and tables from PDF documents
metadata:
  author: example-org
  version: "1.0"
---
```

```python
# System prompt injection
AVAILABLE_SKILLS = """
Available skills (use load_skills tool to read full content):
- pdf-processing: Extract text and tables from PDF documents
- slack-formatting: Format messages for Slack with proper mrkdwn
- large-file-handling: Handle files exceeding context window
"""

# Tool for on-demand loading
def load_skills(skill_names):
    """Load full skill content into context."""
    for name in skill_names:
        path = f"skills/{name}/SKILL.md"
        # Read and inject full content
```

**Benefits of progressive disclosure:**
- Reduces conflicting or unnecessary context
- Minimizes formatting inconsistencies (e.g., Markdown vs Slack mrkdwn)
- In-context learning examples stay focused on relevant tools

**Lazy-loading MCP tools via skills (Amp approach):**

MCP servers often expose many tools that consume significant context. Bind MCP servers to skills with selective tool loading:

```json
// skills/chrome-automation/mcp.json
{
  "chrome-devtools": {
    "command": "npx",
    "args": ["-y", "chrome-devtools-mcp@latest"],
    "includeTools": [
      "navigate_page",
      "take_screenshot",
      "new_page",
      "list_pages"
    ]
  }
}
```

**Token savings example:**
- chrome-devtools MCP: 26 tools = 17k tokens
- Lazy-loaded subset: 4 tools = 1.5k tokens (91% reduction)

The agent sees only the skill description initially. When invoked, only the specified tools are loaded into context.

## How to use it

**Implementation phases:**

1. **Ad-hoc → Saved**

   - Agent writes code to solve immediate problem
   - If solution works, save to `skills/` directory
   - Use descriptive names: `skills/pdf_to_markdown.py`

2. **Saved → Reusable**

   - Refactor for generalization (parameterize hard-coded values)
   - Add basic error handling
   - Create simple function signature

3. **Reusable → Documented**

   - Add docstrings with purpose, parameters, returns, examples
   - Include any prerequisites or dependencies
   - Note when last tested or validated

4. **Documented → Capability**

   - Agent can discover skills through directory listing
   - Skills become part of agent's effective capability set
   - Skills are composed into higher-level workflows

**Skill organization:**

```
skills/
├── README.md                 # Index of available skills
├── data_processing/
│   ├── csv_to_json.py
│   └── filter_outliers.py
├── api_integration/
│   ├── github_pr_summary.py
│   └── slack_notify.py
├── text_analysis/
│   ├── sentiment.py
│   └── extract_entities.py
└── tests/
    └── test_sentiment.py     # Validation tests for skills
```

**Discovery pattern:**

```python
# Agent explores available skills
def discover_skills():
    """List available skills with descriptions."""
    skills = []
    for root, dirs, files in os.walk("skills/"):
        for file in files:
            if file.endswith(".py") and file != "__init__.py":
                path = os.path.join(root, file)
                # Extract docstring
                with open(path) as f:
                    content = f.read()
                    docstring = extract_docstring(content)
                skills.append({
                    'path': path,
                    'name': file[:-3],
                    'description': docstring.split('\n')[0] if docstring else ''
                })
    return skills
```

## Trade-offs

**Pros:**

- Builds agent capability over time
- Reduces redundant problem-solving across sessions
- Creates organizational knowledge base in code form
- Skills can be tested, versioned, and improved
- Enables composition of higher-level capabilities
- Reduces token consumption (reuse vs. rewrite)

**Cons:**

- Requires discipline to save and organize skills
- Skills can become stale or outdated
- Need maintenance and testing infrastructure
- Namespace conflicts if skills grow large
- Agents must be prompted to check skills before writing new code
- Quality varies (not all saved code is good code)

**Maintenance requirements:**

- Regular review of skill quality and relevance
- Testing framework for skill validation
- Deprecation policy for outdated skills
- Documentation standards for new skills
- Version control to track skill evolution

**Success factors:**

- Clear naming conventions
- Good documentation from the start
- Encourage skill reuse through prompting
- Periodic skill library review and curation
- Examples and test cases for each skill

## References

* Anthropic Engineering: Code Execution with MCP (2024)
* [Building an internal agent: Adding support for Agent Skills](https://lethain.com/agents-skills/) - Will Larson (Imprint, 2025)
* [Efficient MCP Tool Loading](https://ampcode.com/news/lazy-load-mcp-with-skills) - Amp (Nicolay, 2025)
* Related: Compounding Engineering Pattern, CLI-First Skill Design

---

## Spec-As-Test Feedback Loop

**Status:** proposed
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** http://jorypestorious.com/blog/ai-engineer-spec/

## Problem
Even in spec-first projects, implementations can drift as code evolves and the spec changes (or vice-versa). Silent divergence erodes trust.

## Solution
Generate **executable assertions** directly from the spec (e.g., unit or integration tests) and let the agent:

- Watch for any spec or code commit.  
- Auto-regenerate test suite from latest spec snapshot.  
- Run tests; if failures appear, open an *agent-authored* PR that either:
    
- updates code to match spec, or
    - flags unclear spec segments for human review.

This creates a continuous feedback loop ensuring specification and implementation remain synchronized.

## Trade-offs
- **Pros:** catches drift early, keeps spec & impl in lock-step.
- **Cons:** heavy CI usage; false positives if spec wording is too loose.

## References
- Natural extension of the "specification-driven development" concept surfaced in the page metadata.

---

## Specification-Driven Agent Development

**Status:** proposed
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** http://jorypestorious.com/blog/ai-engineer-spec/

## Problem
Hand-crafted prompts or loose user stories leave room for ambiguity; agents can wander, over-interpret, or produce code that conflicts with stakeholder intent.

## Solution
Adopt a **spec-first workflow** in which a formal specification file (e.g., Markdown, OpenAPI, JSON Schema) is the agent's *primary* input and source of truth.

- **Parse spec** → agent builds an explicit task graph.
- **Scaffold** project structure & stub code straight from the spec.
- **Enforce** that every generated artifact links back to a spec clause.
- **Iterate** only by editing the spec, *not* by re-prompting ad-hoc.

```pseudo
if new_feature_requested:
    write_spec(update)
    agent.sync_with(spec)
```

## How to use it
Give the agent a well-structured spec file, then run `claude spec run`.
Pitfalls: coarse or under-specified requirements still propagate errors.

## Trade-offs

- **Pros:** repeatable, audit-friendly, easy diffing.
- **Cons:** up-front spec writing effort; initial ramp-up for teams new to spec formats.

## References
- Talk teaser in the World's Fair meta-description about "shift to specification-driven development."

---

## Spectrum of Control / Blended Initiative

**Status:** validated-in-production
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=BGgsoIgbT_Y

## Problem
AI agents for tasks like coding can offer various levels of assistance, from simple completions to complex, multi-step operations. A one-size-fits-all approach to agent autonomy doesn't cater to the diverse needs of users or the varying complexity of tasks. Users need to fluidly shift between direct control and delegating tasks to the agent.

## Solution
Design the human-agent interaction to support a spectrum of control, allowing users to choose the level of agent autonomy appropriate for the current task or their familiarity with the codebase. This involves providing multiple modes or features for interaction:

-   **Low Autonomy (High Human Control):** Simple, inline assistance like tab-completion for code, where the human is primarily driving and the AI augments their input.
-   **Medium Autonomy:** Agent assistance for more contained tasks, like editing a selected region of code or an entire file based on a specific instruction (e.g., "Command K" functionality). The human defines the scope and the high-level goal.
-   **High Autonomy:** Agent takes on larger, multi-file tasks or complex refactorings, potentially involving multiple steps, with less direct human guidance on each step (e.g., an "Agent" feature).
-   **Very High Autonomy (Asynchronous):** Background agents that can take on entire complex tasks like implementing a feature or fixing a set of bugs and creating a pull request, operating largely independently.

Users can seamlessly switch between these modes depending on their needs, allowing for a "blended initiative" where both human and AI contribute effectively.

## Example
```mermaid
flowchart LR
    subgraph "Human Control"
        A[Tab Completion]
    end
    subgraph "Shared Control"
        B[Command K - Edit Region/File]
    end
    subgraph "Agent Control"
        C[Agent Feature - Multi-File Edits]
    end
    subgraph "Autonomous Agent"
        D[Background Agent - Entire PRs]
    end

    A --> B
    B --> C
    C --> D
    D --> A
```

## References
- Aman Sanger (Cursor) extensively discusses this spectrum at 0:05:16-0:06:44, detailing different features like tab completion, Command K, Agent for multi-file edits, and Background Agent for entire PRs, describing it as "almost a spectrum."

---

## Stop Hook Auto-Continue Pattern

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

Agents complete their turn and return control to the user even when the task isn't truly done. Common scenarios:

- Code compiles but tests fail
- Changes made but quality checks haven't passed
- Feature implemented but integration tests broken
- Migrations run but verification steps not completed

Without intervention, the user must manually check and re-prompt the agent, creating friction.

## Solution

Use **stop hooks** to programmatically check success criteria after each agent turn. If criteria aren't met, automatically continue the agent's execution until the task is genuinely complete.

**Stop hook**: A script that runs when the agent finishes a turn. It can inspect state and decide whether to return control to the user or keep the agent running.

```pseudo
define_stop_hook() {
    # Runs after every agent turn completion

    test_result = run_tests()

    if test_result.failed:
        agent.continue_with_prompt(
            "Tests failed with: {test_result.errors}. Fix these issues."
        )
    else:
        agent.stop()  # Return control to user
}
```

**Combined with dangerous mode**: In containerized/sandboxed environments, this enables fully autonomous operation until success.

## How to use it

**Basic implementation:**

1. Define success criteria (tests pass, build succeeds, linter clean, etc.)
2. Create stop hook that checks these criteria
3. If criteria fail, inject feedback and continue agent execution
4. If criteria pass, return control to user

**Claude Code SDK example:**

```javascript
// Stop hook configuration
{
  "hooks": {
    "on_stop": {
      "command": "./scripts/check_success.sh",
      "auto_continue_on_failure": true
    }
  }
}
```

**Power user pattern (from transcript):**

> "You can define a stop hook that's like, if the tests don't pass, keep going. Essentially make the model keep going until the thing is done."

**Advanced usage with programmatic SDK:**

Combine with dangerous mode in containers for autonomous operation:

- Agent makes changes
- Stop hook checks tests
- If failing, agent continues autonomously
- Loops until tests pass or timeout
- Result: "Deterministic outcomes from non-deterministic processes"

## Trade-offs

**Pros:**

- **True task completion**: Don't stop until actually done
- **Reduced human intervention**: No manual re-prompting needed
- **Systematic quality**: Encoded success criteria, not human judgment
- **Autonomous operation**: Combined with SDK, enables fully hands-off tasks
- **Prevents premature completion**: Agent can't declare victory too early

**Cons:**

- **Runaway costs**: Agent might loop indefinitely if criteria impossible
- **Requires good criteria**: Bad success checks lead to infinite loops
- **Container overhead**: Safest in sandboxed environments
- **Debugging challenges**: Harder to inspect mid-execution state
- **Timeout management**: Need sensible limits to prevent infinite execution

**Safety considerations:**

- Use timeouts to bound execution
- Monitor token usage during loops
- Test hooks in safe environments first
- Start with simple criteria before complex checks
- Log all auto-continue decisions for debugging

## References

* Boris Cherny: "You can define a stop hook that's like, if the tests don't pass, keep going. Essentially you can just make the model keep going until the thing is done."
* Boris Cherny: "This is insane when you combine it with the SDK and this kind of programmatic usage. This is a stochastic thing, it's non-deterministic, but with scaffolding you can get these deterministic outcomes."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Structured Output Specification

**Status:** established
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://vercel.com/blog/what-we-learned-building-agents-at-vercel

## Problem

Free-form agent outputs are difficult to validate, parse, and integrate with downstream systems. When agents return unstructured text, you face:

- Unpredictable output formats requiring complex parsing
- Difficult validation and error handling
- Brittle integration with automated workflows
- Inconsistent categorization and classification
- Manual post-processing to extract structured data

This makes it nearly impossible to build reliable multi-step workflows where one agent's output feeds into another system or agent.

## Solution

Constrain agent outputs using deterministic schemas that enforce structured, machine-readable results. Instead of allowing free-form text responses, specify exact output formats using type systems, JSON schemas, or framework-specific structured output APIs.

**Core approach:**

**Define explicit output schemas:**

- Use TypeScript interfaces, JSON Schema, or Pydantic models
- Specify required fields, types, and constraints
- Define enumerations for categorical outputs
- Document field semantics and validation rules

**Leverage framework structured output APIs:**

- OpenAI's structured outputs with JSON schema
- Anthropic's tool use for structured results
- Vercel AI SDK's `generateObject` function
- LangChain's output parsers

**Validate at generation time:**

- Framework ensures LLM adheres to schema
- Type errors caught before reaching application code
- Guaranteed parseable outputs

**Example implementation:**

```typescript
import { generateObject } from 'ai';
import { z } from 'zod';

// Define strict output schema
const LeadQualificationSchema = z.object({
  qualification: z.enum(['qualified', 'unqualified', 'needs_review']),
  confidence: z.number().min(0).max(1),
  companySize: z.enum(['enterprise', 'mid-market', 'smb', 'unknown']),
  estimatedBudget: z.string().optional(),
  nextSteps: z.array(z.string()),
  reasoning: z.string()
});

// Agent returns structured, validated output
const result = await generateObject({
  model: openai('gpt-4'),
  schema: LeadQualificationSchema,
  prompt: `Analyze this lead: ${leadData}`
});

// TypeScript knows exact structure
if (result.object.qualification === 'qualified') {
  await sendToSalesTeam(result.object);
}
```

**Integration benefits:**

```mermaid
graph LR
    A[Agent Input] --> B[LLM + Schema]
    B --> C[Validated Structured Output]
    C --> D[Downstream System]
    C --> E[Database Storage]
    C --> F[Next Agent Phase]

    style C fill:#90EE90
```

## How to use it

**When to apply:**

- Multi-phase agent workflows requiring structured handoffs
- Classification and categorization tasks
- Data extraction and transformation
- Integration with databases or APIs
- Compliance and audit requirements
- Quality assurance and validation

**Implementation steps:**

**1. Identify output requirements:**

- What decisions does the agent make?
- What data must be extracted?
- What downstream systems consume this output?

**2. Design schema:**

```python
from pydantic import BaseModel, Field
from typing import Literal

class AbuseAnalysis(BaseModel):
    content_type: Literal['spam', 'abuse', 'legitimate', 'unclear']
    severity: Literal['critical', 'high', 'medium', 'low']
    recommended_action: Literal['remove', 'warn', 'ignore', 'escalate']
    confidence_score: float = Field(ge=0, le=1)
    evidence: list[str]
    requires_human_review: bool
```

**3. Integrate with agent framework:**

```python
result = client.generate(
    model="gpt-4",
    response_format=AbuseAnalysis,
    messages=[{"role": "user", "content": abuse_report}]
)

# result is guaranteed to match schema
if result.requires_human_review:
    await send_to_human(result)
else:
    await auto_execute(result.recommended_action)
```

**4. Handle validation failures:**

- Retry with clarified prompt
- Fallback to human review
- Log schema violations for prompt improvement

**Prerequisites:**

- Agent framework with structured output support
- Clear understanding of downstream data requirements
- Schema validation library (Zod, Pydantic, JSON Schema)

## Trade-offs

**Pros:**

- **Reliability:** Guaranteed parseable outputs eliminate parsing errors
- **Type safety:** Compile-time checking in typed languages
- **Integration:** Seamless connection to databases, APIs, workflows
- **Validation:** Built-in constraint enforcement
- **Maintainability:** Explicit contracts between system components
- **Testability:** Easy to verify output correctness

**Cons:**

- **Rigidity:** Schema changes require coordinated updates
- **Complexity:** Requires upfront schema design effort
- **Expressiveness limits:** May constrain useful free-form outputs
- **Framework dependency:** Relies on LLM provider schema support
- **Over-specification:** Too strict schemas may cause generation failures
- **Evolution friction:** Adapting schemas as requirements change

**Mitigation strategies:**

- Include optional `additional_context` field for free-form notes
- Version schemas and support graceful degradation
- Use union types for evolving classifications
- Balance structure with flexibility (required vs optional fields)

## References

- [Vercel: What We Learned Building Agents](https://vercel.com/blog/what-we-learned-building-agents-at-vercel) - Lead qualification using structured categorization
- [OpenAI Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) - JSON schema enforcement
- [Vercel AI SDK generateObject](https://sdk.vercel.ai/docs/reference/ai-sdk-core/generate-object) - TypeScript-native structured generation
- [Anthropic Tool Use](https://docs.anthropic.com/claude/docs/tool-use) - Structured outputs via tool calling
- Related patterns: [Discrete Phase Separation](discrete-phase-separation.md), [Human-in-the-Loop Approval Framework](human-in-loop-approval-framework.md)

---

## Sub-Agent Spawning

**Status:** validated-in-production
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/ampcode

## Problem

Large multi-file tasks blow out the main agent's context window and reasoning budget. You need a way to delegate work to specialized agents with isolated contexts and tools.

## Solution

Let the main agent **spawn focused sub-agents**, each with its own fresh context, to work in parallel on shardable subtasks. Aggregate their results when done.

**Implementation approaches:**

### 1. Declarative YAML Configuration

Define subagent types in configuration files with their own system prompts, allowed tools, and context windows:

```yaml
# subagents/planning.yaml
name: planning
system_prompt: "Break down complex tasks into steps..."
tools:
  - list_files
  - read_file
  # or inherit: all (from parent agent)

# subagents/think.yaml
name: think
system_prompt: "Analyze and refine reasoning..."
tools:
  - read_file
  - search
```

Agents invoke subagents via a dedicated tool:

```pseudo
subagent(agent_name, prompt, files)
```

This allows:
- **Virtual file isolation**: Subagent only sees files explicitly passed to it
- **Tool scoping**: Subagents can inherit all parent tools or use a subset
- **Specialized system prompts**: Each subagent type has predefined behavior

### 2. Dynamic Spawning

Spawn subagents on-demand for parallel task execution:

```pseudo
# Main agent creates todo list
files = glob("**/*.md")
batches = chunk(files, 9)

# Spawn subagents for each batch
for batch in batches:
    spawn_subagent(
        task="Update YAML front-matter in these files",
        files=batch,
        context=instructions
    )
```

Recent developments show that improved agent [state externalization capabilities](proactive-agent-state-externalization.md) may make subagent delegation more practical by helping agents better identify which tasks are suitable for delegation and how to communicate necessary context to subagents.

## Example (YAML front-matter refactor)
```mermaid
sequenceDiagram
  MainAgent->>GlobTool: "*.md"
  MainAgent->>TaskTool: spawn 4 sub-agents with 9 files each
  loop per SubAgent
      SubAgent->>Files: update front-matter
      SubAgent->>Git: commit
  end
  MainAgent->>Git: merge branches ➜ single PR
```

## How to use it

**Use cases for subagents:**

1. **Context window management**: Process large files in subagents without polluting main context
   - Upload files to subagent
   - Extract specific data
   - Return summary to main agent

2. **Concurrent work**: Run multiple subagents in parallel, join on completion
   - Reduces clock-time for I/O-bound workflows
   - Network API calls can happen simultaneously

3. **Code-driven LLM invocation**: Hand off control to LLM for specific determination
   - Code workflow calls subagent
   - Subagent makes LLM-powered decision
   - Control returns to code with result

4. **Security isolation**: Separate tools/contexts in mutually isolated subagents
   - External resource retrieval isolated from internal access
   - Reduced blast radius for sensitive operations

**Declarative subagent setup:**

```yaml
# agents.yaml
subagents:
  planning:
    file: subagents/planning.yaml
    allowed_in:
      - main_agent
      - research_agent

  think:
    file: subagents/think.yaml
    allowed_in:
      - main_agent
```

**Virtual file passing:**

```pseudo
# Main agent
result = subagent(
    agent_name="planning",
    prompt="Analyze these files and create migration plan",
    files=["file1.ts", "file2.ts", "file3.ts"]
)
# Only these 3 files visible to planning subagent
```

**Recursive architecture insight:**

Some implementations treat every agent as a subagent, enabling flexible composition and consistent behavior across the system.

## Advanced usage: Swarm migrations

For massive parallelization (10+ subagents), see the **Swarm Migration Pattern** which extends this concept for large-scale code migrations.

**High-volume use case at Anthropic:**

Users spending $1000+/month on Claude Code are typically running swarm migrations:

- Main agent creates comprehensive todo list
- Spawns 10+ parallel subagents
- Each handles batch of migration targets (e.g., 10 files)
- Common for framework migrations, lint rule rollouts, API updates
- Achieves 10x+ speedup vs. sequential execution

**Quote from Boris Cherny (Anthropic):**

> "There's an increasing number of people internally at Anthropic using a lot of credits every month. Spending over a thousand bucks. The common use case is code migration... The main agent makes a big to-do list for everything and map reduces over a bunch of subagents. You instruct Claude like start 10 agents and then just go 10 at a time and just migrate all the stuff over."

## Trade-offs

**Pros:**

- **Context isolation**: Each subagent has clean context window
- **Parallelization**: Reduce workflow latency through concurrent execution
- **Specialization**: Different subagent types for different tasks (planning, thinking, analysis)
- **Virtual files**: Precise control over what each subagent can see
- **Tool scoping**: Limit subagent capabilities for security/simplicity
- **Declarative config**: Reusable subagent definitions via YAML

**Cons:**

- **Overhead**: Spawning and coordinating subagents adds complexity
- **Cost**: Running multiple agents simultaneously increases token usage
- **Coordination**: Main agent must track and aggregate subagent results
- **Not always necessary**: Author notes "frequently thought we needed subagents, then found more natural alternative"
- **Latency visibility**: User-facing latency is "invisible feature" until it becomes problematic

**When subagents matter most:**

- Context window management (large file processing)
- I/O-bound workflows (network API calls)
- Code-driven workflows needing LLM delegation
- Massive parallelization needs (10+ concurrent agents)

## References

* Raising An Agent - Episode 6: Claude 4 Sonnet edits 36 blog posts via four sub-agents.
* Boris Cherny (Anthropic) on swarm migrations for framework changes and lint rules
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)
* [Cognition AI: Devin & Claude Sonnet 4.5](https://cognition.ai/blog/devin-sonnet-4-5-lessons-and-challenges) - discusses how improved model judgment about state externalization may make subagent delegation more practical
* [Building Companies with Claude Code](https://claude.com/blog/building-companies-with-claude-code) - Ambral's "robust research engine" uses dedicated sub-agents specialized for different data types, enabling parallel research across system areas
* [Building an internal agent: Subagent support](https://lethain.com/agents-subagents/) - Will Larson on YAML-configured subagents with virtual file isolation and code-driven LLM invocation

[Source](https://www.nibzard.com/ampcode)

---

## Subagent Compilation Checker

**Status:** emerging
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=Xkwok_XXQgw

## Problem

Large coding tasks often involve multiple independent components (e.g., microservices, libraries). Having the **main agent** handle compilation and error checking for every component in-context:

- **Blows Up Context Length:** Including entire build logs or bytecode in the prompt is impractical.
- **Slows Down Inference:** Sending full build commands and parsing verbose output in-context uses excessive tokens.

Additionally, when the agent's single "compile-and-run" step fails, it's hard to pinpoint which submodule caused the error without a more granular approach.

## Solution

Spawn **specialized "Compilation Subagents"** to independently build and verify each code submodule, reporting back only:

**1. Error Summary:** File paths, line numbers, and error messages.
**2. Binary Artifacts (if needed):** Reference IDs (e.g., paths to compiled object files) rather than raw binaries.

**Workflow:**
- **Main Agent Request:** "Compile module `auth-service`."
- **Spawn `CompileSubagent(auth-service)`**
  - Subagent runs `mvn clean install` or `go build ./auth-service`.
  - Returns a structured error list or location of compiled artifact.
- **Main Agent:** Updates its context with the **concise error report** (e.g., `[{file: "auth_controller.go", line: 85, error: "undefined: UserModel"}]`).

## Example

```mermaid
sequenceDiagram
    MainAgent->>CompileSubagent: "Compile service A"
    CompileSubagent->>BuildEnv: go build ./serviceA
    alt Build Success
        CompileSubagent-->>MainAgent: {status: "success", artifact: "serviceA.bin"}
    else Build Failure
        CompileSubagent-->>MainAgent: [{file: "fileA.go", line: 10, error: "Syntax error"}]
    end
    MainAgent->>Context: Inject concise error summary
```

## How to use it

- **Subagent Definition:** Each subagent is a lightweight container or process with the appropriate runtime (e.g., JVM for Java code, Node for JavaScript).
- **Integration in RL Loop:** Treat each subagent invocation as a **tool call** within the RL environment.
- **Error-Driven Reward:** If the error list is non-empty, assign a negative reward proportional to the number of errors (e.g., `reward = −len(error_list)`), to encourage the agent to fix compile errors quickly.

## Trade-offs

- **Pros:**
  - **Modular Isolation:** The main agent never needs to load entire build logs into its context.
  - **Parallel Builds:** Multiple subagents can compile different modules in parallel, speeding up end-to-end workflow.
- **Cons/Considerations:**
  - **Infrastructure Overhead:** Requires a mechanism to spin up and tear down multiple build environments.
  - **Subagent Synchronization:** If one module depends on another's build artifact, coordination policies must ensure the correct build order.

## References

- Inspired by "Subagent Spawning" for code-related subtasks in the Open Source Agent RL talk (May 2025).
- Will Brown's note on decoupling long I/O-bound steps from the main model's inference to avoid context explosion.

---

## Swarm Migration Pattern

**Status:** validated-in-production
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

Large-scale code migrations are time-consuming when done sequentially:

- **Framework upgrades** (e.g., testing library A → testing library B)
- **Lint rule rollouts** across hundreds of files
- **API migrations** when dependencies change
- **Code modernization** (e.g., class components → hooks)
- **Refactoring patterns** across the codebase

Humans doing these manually is tedious; single agents doing them sequentially is slow.

## Solution

Use a **swarm architecture** where the main agent orchestrates 10+ parallel subagents working simultaneously on independent chunks of the migration.

**Pattern:**

1. **Main agent creates migration plan**: Enumerate all files/targets needing migration
2. **Create todo list**: Break work into parallelizable chunks
3. **Spawn subagent swarm**: Start 10+ agents concurrently, each taking N items
4. **Map-reduce execution**: Each subagent migrates its chunk independently
5. **Verification**: Main agent validates results, may spawn additional agents for fixes
6. **Consolidation**: Combine results (single PR or coordinated merge)

```mermaid
graph TD
    A[Main Agent] --> B[Scan Codebase]
    B --> C[Create Todo List<br/>100 files need migration]
    C --> D[Spawn 10 Subagents]
    D --> E1[Subagent 1<br/>Files 1-10]
    D --> E2[Subagent 2<br/>Files 11-20]
    D --> E3[...]
    D --> E10[Subagent 10<br/>Files 91-100]
    E1 --> F[Main Agent Verifies]
    E2 --> F
    E3 --> F
    E10 --> F
    F --> G[Consolidated PR]
```

## How to use it

**Implementation approach:**

```pseudo
# Main agent orchestration
main_agent.prompt = """
1. Find all files matching pattern (e.g., *.test.js using old framework)
2. Create todo list with file paths
3. Divide into batches of 10 files each
4. For each batch, spawn subagent with instructions:
   
- Migrate these specific files
   - Follow migration guide at docs/migration.md
   - Run tests after each change
   - Commit if tests pass
5. Monitor all subagents
6. Verify all todos completed
"""

# Spawn swarm
for batch in batches:
    spawn_subagent(
        task=f"Migrate {batch.files} from Framework A to B",
        context=migration_guide,
        auto_commit=True
    )
```

**Real-world usage at Anthropic:**

> "There's an increasing number of people internally at Anthropic that are using a lot of credits every month. Spending like over a thousand bucks every month. And this percent of people was growing actually pretty fast. The common use case is code migration... The main agent makes a big to-do list for everything and then just kind of map reduces over a bunch of subagents. You instruct Claude like, yeah, start 10 agents and then just go 10 at a time and just migrate all the stuff over." —Boris Cherny

**Common migration types:**

- **Lint rule enforcement**: Apply new ESLint/Biome rules across files
- **Framework migration**: Jest → Vitest, Mocha → Jest, etc.
- **API updates**: Update to new library versions
- **Code modernization**: var → const/let, callbacks → async/await
- **Import path changes**: Relative → absolute paths

## Trade-offs

**Pros:**

- **Massive parallelization**: 10x+ speedup vs. sequential migration
- **Easy verification**: Each subagent handles tractable chunk
- **Fault isolation**: One subagent failing doesn't break others
- **Cost-effective for scale**: $1000 for migrations that would take weeks manually
- **Reproducible**: Same migration applied consistently across all files

**Cons:**

- **High token cost**: Running 10+ agents simultaneously
- **Coordination complexity**: Main agent must track all subagents
- **Merge conflicts**: Parallel changes might conflict
- **Requires independence**: Only works if migration targets are separable
- **Verification burden**: Need to validate 10+ agent outputs

**Prerequisites:**

- **Atomic migrations**: Each file can be migrated independently
- **Clear specification**: Migration rules must be unambiguous
- **Good test coverage**: Automated verification of correctness
- **Sandbox environment**: Safe to run many agents simultaneously

**Optimization tips:**

- **Batch size tuning**: Start with 10 files per agent, adjust based on complexity
- **Staged rollout**: Migrate 10% first, verify, then do the rest
- **Failure handling**: Have main agent retry failed batches with refined instructions
- **Resource limits**: Cap parallel agents to avoid overwhelming infrastructure

## References

* Boris Cherny: "There's an increasing number of people internally at Anthropic using a lot of credits every month. Spending over a thousand bucks. The common use case is code migration. Framework A to framework B. The main agent makes a big to-do list for everything and map reduces over a bunch of subagents. Start 10 agents and go 10 at a time and migrate all the stuff over."
* Boris Cherny: "Lint rules... there's some kind of lint rule you're rolling out, there's no auto fixer because static analysis can't really—it's too simplistic for it. Framework migrations... we just migrated from one testing framework to a different one. That's a pretty common one where it's super easy to verify the output."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Team-Shared Agent Configuration as Code

**Status:** best-practice
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it

## Problem

When each engineer configures their AI agent independently:

- **Inconsistent behavior**: Agents work differently for different team members
- **Permission friction**: Everyone gets prompted for the same safe commands
- **Duplicated effort**: Each person solves the same configuration problems
- **Knowledge silos**: Good configurations don't spread across the team
- **Onboarding overhead**: New team members start from scratch
- **Security gaps**: No standardized rules about what agents can/can't touch

## Solution

**Check agent configuration into version control** as part of the repository. Treat `settings.json` (or equivalent) as code—reviewable, shareable, and versioned alongside your project.

**Key configuration elements:**

1. **Pre-allowed commands**: Tools that don't need permission prompts
2. **Blocked files/directories**: What the agent must never touch
3. **Default subagents**: Team-standard specialized agents
4. **Slash commands**: Shared workflows everyone can use
5. **Hooks**: Standardized automation triggers

```json
// .claude/settings.json (checked into repo)
{
  "permissions": {
    "pre_allowed": [
      "git add",
      "git commit",
      "git push",
      "npm test",
      "npm run lint"
    ],
    "blocked_paths": [
      ".env",
      "secrets/",
      "*.key",
      "credentials.json"
    ]
  },
  "subagents": {
    "security-review": "./agents/security.md",
    "migration-helper": "./agents/migration.md"
  },
  "hooks": {
    "pre_commit": "./hooks/run_tests.sh"
  }
}
```

## How to use it

**Implementation steps:**

### 1. Create shared config file

Start with common settings all team members need:

- Commands everyone runs (git, test runners, linters)
- Sensitive paths no one should modify
- Standard workflows as slash commands

### 2. Version control it

```bash
git add .claude/settings.json
git commit -m "Add shared agent configuration"
git push
```

### 3. Team adoption

New team members automatically get the configuration when they clone:

```bash
git clone repo
cd repo
# Agent reads .claude/settings.json automatically
```

### 4. Iterate as a team

- PRs can update agent configuration
- Code review applies to agent settings too
- Changes propagate via normal git pull

**Benefits observed in enterprise deployments (from transcript):**

> "Companies that have really big deployments of Claude Code... have settings.json that you check into the code base... you can use this to pre-allow certain commands so you don't get permission prompted every time. And also to block certain commands... and share it with the whole team so everyone gets to use it." —Boris Cherny

## Trade-offs

**Pros:**

- **Consistent team experience**: Everyone's agent behaves the same way
- **Faster onboarding**: New members inherit team knowledge immediately
- **Reduced friction**: Pre-allowed commands eliminate repetitive prompts
- **Security standardization**: Uniform rules about sensitive files
- **Collaborative improvement**: Team can improve config together via PRs
- **Auditable**: Version history shows why configurations changed

**Cons:**

- **Less individual flexibility**: Can't customize as freely
- **Potential conflicts**: Personal preferences vs. team standards
- **Config sprawl**: Settings file can become complex
- **Override complexity**: Need escape hatch for individual customization
- **Secrets exposure risk**: Must ensure no credentials in committed config

**Best practices:**

- **Separate local overrides**: Support `.claude/settings.local.json` (gitignored)
- **Document configuration**: Explain why things are pre-allowed/blocked
- **Regular review**: Audit config quarterly as tools/threats evolve
- **Gradual adoption**: Start minimal, expand based on team pain points
- **Template repositories**: Create starter configs for common project types

## References

* Boris Cherny: "Companies that have really big deployments of Claude Code... having settings.json that you check into the code base is really important because you can use this to pre-allow certain commands so you don't get permission prompted every time. And also to block certain commands... and this way as an engineer I don't get prompted and I can check this in and share it with the whole team so everyone gets to use it."
* [AI & I Podcast: How to Use Claude Code Like the People Who Built It](https://every.to/podcast/transcript-how-to-use-claude-code-like-the-people-who-built-it)

---

## Three-Stage Perception Architecture

**Status:** proposed
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.oreilly.com/library/view/software-architecture-patterns/9781491971437/

## Problem

Complex AI agents often struggle with unstructured inputs and need a systematic way to process information before taking action. Without a clear separation of concerns, agents can become monolithic and difficult to debug, extend, or optimize. Additionally, mixing perception, processing, and action logic makes it hard to swap out components or scale different parts of the system independently.

## Solution

Implement a three-stage pipeline architecture that cleanly separates an agent's workflow into distinct phases:

1. **Perception Stage**: Handles all input gathering and normalization
   - Receives raw inputs (text, images, audio, structured data)
   - Performs initial processing (OCR, speech-to-text, format conversion)
   - Normalizes data into a common internal representation

2. **Processing Stage**: Performs reasoning and decision-making
   - Analyzes normalized inputs using appropriate models
   - Applies business logic and reasoning
   - Makes decisions about what actions to take
   - Can involve multiple sub-agents or reasoning steps

3. **Action Stage**: Executes decisions in the environment
   - Translates decisions into concrete actions
   - Interfaces with external systems and APIs
   - Handles error recovery and retries
   - Reports results back to the system

## Example

```python
class ThreeStageAgent:
    def __init__(self):
        self.perception = PerceptionPipeline()
        self.processor = ProcessingPipeline()
        self.action = ActionPipeline()
    
    async def run(self, raw_input):
        # Stage 1: Perception
        perceived_data = await self.perception.process(raw_input)
        
        # Stage 2: Processing
        decisions = await self.processor.analyze(perceived_data)
        
        # Stage 3: Action
        results = await self.action.execute(decisions)
        
        return results

class PerceptionPipeline:
    def __init__(self):
        self.handlers = {
            'text': TextHandler(),
            'image': ImageHandler(),
            'audio': AudioHandler(),
            'structured': StructuredDataHandler()
        }
    
    async def process(self, raw_input):
        input_type = self.detect_input_type(raw_input)
        handler = self.handlers[input_type]
        
        # Normalize to common format
        normalized = await handler.normalize(raw_input)
        
        # Extract features
        features = await handler.extract_features(normalized)
        
        return {
            'type': input_type,
            'normalized': normalized,
            'features': features,
            'metadata': handler.get_metadata(raw_input)
        }

class ProcessingPipeline:
    def __init__(self):
        self.reasoning_engine = ReasoningEngine()
        self.decision_maker = DecisionMaker()
    
    async def analyze(self, perceived_data):
        # Apply reasoning based on input type and features
        analysis = await self.reasoning_engine.reason(perceived_data)
        
        # Make decisions based on analysis
        decisions = await self.decision_maker.decide(
            analysis,
            context=self.get_context()
        )
        
        # Validate decisions
        validated = await self.validate_decisions(decisions)
        
        return validated

class ActionPipeline:
    def __init__(self):
        self.executors = {
            'api_call': APIExecutor(),
            'database': DatabaseExecutor(),
            'file_system': FileSystemExecutor(),
            'notification': NotificationExecutor()
        }
    
    async def execute(self, decisions):
        results = []
        
        for decision in decisions:
            executor = self.executors[decision.action_type]
            
            try:
                result = await executor.execute(decision)
                results.append({
                    'decision': decision,
                    'status': 'success',
                    'result': result
                })
            except Exception as e:
                # Handle errors gracefully
                recovery_result = await self.attempt_recovery(decision, e)
                results.append(recovery_result)
        
        return results
```

```mermaid
flowchart LR
    subgraph "Perception Stage"
        A[Raw Input] --> B[Type Detection]
        B --> C[Normalization]
        C --> D[Feature Extraction]
        D --> E[Structured Data]
    end
    
    subgraph "Processing Stage"
        E --> F[Reasoning Engine]
        F --> G[Analysis]
        G --> H[Decision Making]
        H --> I[Validation]
        I --> J[Action Plan]
    end
    
    subgraph "Action Stage"
        J --> K[Executor Selection]
        K --> L[Action Execution]
        L --> M[Error Handling]
        M --> N[Results]
    end
    
    N --> O[Feedback Loop]
    O --> F
    
    style A fill:#ffebee,stroke:#c62828,stroke-width:2px
    style J fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style N fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
```

## Benefits

- **Modularity**: Each stage can be developed, tested, and scaled independently
- **Flexibility**: Easy to swap implementations for different stages
- **Debugging**: Clear boundaries make it easier to identify where issues occur
- **Reusability**: Stages can be shared across different agent types
- **Scalability**: Different stages can be scaled based on their computational needs

## Trade-offs

**Pros:**
- Clean separation of concerns
- Easier to maintain and extend
- Better error isolation
- Enables specialized optimization per stage
- Facilitates team collaboration (different teams per stage)

**Cons:**
- Additional complexity for simple tasks
- Potential latency from stage transitions
- Requires careful interface design between stages
- May introduce overhead for data transformation between stages

## References

- [Software Architecture Patterns](https://www.oreilly.com/library/view/software-architecture-patterns/9781491971437/)
- [Pipeline Pattern in ML Systems](https://ml-ops.org/content/mlops-principles)

---

## Tool Capability Compartmentalization

**Status:** emerging
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://simonwillison.net/2025/Jun/16/lethal-trifecta/

## Problem
Model Context Protocol (MCP) encourages "mix-and-match" tools—often combining private-data readers, web fetchers, and writers in a single callable unit. This amplifies the lethality of prompt-injection chains.

## Solution
Adopt **capability compartmentalization** at the tool layer:

- Split monolithic tools into *reader*, *processor*, and *writer* micro-tools.  
- Require explicit, per-call user consent when composing tools across capability classes.  
- Run each class in an isolated subprocess with scoped API keys and file permissions.

```yaml
# tool-manifest.yml
email_reader:
  capabilities: [private_data, untrusted_input]
  permissions:
    fs: read-only:/mail
    net: none

issue_creator:
  capabilities: [external_comm]
  permissions:
    net: allowlist:github.com
```

## How to use it

* Generate the manifest automatically from CI.
* Your agent runner consults the manifest before constructing action plans.
* Flag any attempt to chain tools that would recreate the lethal trifecta.

## Trade-offs

**Pros:** Fine-grained; plays well with modular architectures.
**Cons:** More tooling overhead; risk of permission creep over time.

## References

* Willison's warning that "one MCP mixed all three patterns in a single tool."

---

## Tool Search Lazy Loading

**Status:** emerging
**Category:** Context & Memory
**Authors:** Niko
**Source:** https://x.com/trq212/status/2011523109871108570

## Problem

As the Model Context Protocol (MCP) has grown, MCP servers may expose 50+ tools that consume significant context space. Documented setups with 7+ servers have been documented consuming 67k+ tokens just for tool descriptions. This creates a fundamental scalability issue:

* **Context bloat**: Preloading all tool descriptions consumes tokens that could be used for the actual task
* **Latency**: More tools means more processing overhead on every request
* **Discovery challenges**: Agents must scan through many irrelevant tools to find relevant ones
* **Memory pressure**: Large tool catalogs can exceed practical context limits

## Solution

Implement Tool Search: a lazy-loading mechanism where tools are dynamically loaded into context via search only when needed, rather than preloaded on initialization.

The pattern works by:

1. **Threshold detection**: Monitor when tool descriptions would exceed a context threshold (e.g., 10% of context window)
2. **Search interface**: Provide a ToolSearchTool that allows agents to search tool metadata and selectively load tools
3. **Server instructions**: Leverage MCP server instruction fields to guide the agent on when to search for specific tools
4. **Agentic search**: Use intelligent search rather than basic RAG to find relevant tools

```mermaid
graph TD
    A[Agent Request] --> B{Would tools exceed 10% context?}
    B -->|No| C[Preload All Tools]
    B -->|Yes| D[Enable Tool Search Mode]
    D --> E[Load Tool Metadata Only]
    E --> F[Agent Determines Tool Need]
    F --> G[Search via ToolSearchTool]
    G --> H[Load Specific Tool on Demand]
    H --> I[Execute Tool]
    C --> I
```

**Implementation approach:**

```pseudo
function initialize_mcp_servers(servers) {
    total_tool_tokens = calculate_tool_tokens(servers)

    if (total_tool_tokens > CONTEXT_THRESHOLD) {
        // Lazy loading mode
        tool_registry = load_tool_metadata_only(servers)
        return ToolSearchTool(tool_registry)
    } else {
        // Traditional preload mode
        return preload_all_tools(servers)
    }
}

function tool_search(query: string, tool_registry) {
    // Agentic search - not basic RAG
    relevant_tools = agentically_search(tool_registry, query)
    return load_tool_definitions(relevant_tools)
}
```

## How to use it

**For MCP server creators:**

* **Enhance server instructions**: The "server instructions" field becomes more critical with tool search enabled. It helps the agent know when to search for your tools.
* **Descriptive metadata**: Include rich descriptions and tags to improve searchability
* **Logical grouping**: Organize related tools to make discovery more intuitive

**For MCP client creators:**

* **Implement ToolSearchTool**: Provide a search interface for tool discovery
* **Use agentic search**: Implement intelligent search rather than basic vector RAG
* **Set appropriate thresholds**: Choose context thresholds based on your use case (Claude Code uses 10%)
* **Provide opt-out**: Allow users to disable search if they prefer preloading

**Usage scenarios:**

* Development environments with many specialized tools (file operations, git, database access, API clients)
* Multi-server setups where each server provides domain-specific capabilities
* Agents that only need a subset of available tools for any given task

## Trade-offs

* **Pros:**
  * Dramatically reduces baseline context usage (67k+ tokens to just metadata)
  * Enables scaling to 100+ tools without context issues
  * Faster cold-start times when tools aren't needed
  * Better tool discovery through intentional search
  * Allows more MCP servers to be enabled simultaneously

* **Cons:**
  * Adds latency when tools need to be dynamically loaded
  * Requires search infrastructure and metadata management
  * May miss serendipitous tool discovery that happens when browsing full catalogs
  * Server instructions become more critical and require careful authoring
  * Additional complexity in client implementation

## References

* [Original announcement tweet](https://x.com/trq212/status/2011523109871108570) by Thariq (@trq212)
* [MCP Documentation](https://modelcontextprotocol.io/) for implementation details
* GitHub issue references on lazy loading for MCP servers

## Variations

**Toggle-based forcing**: Allow users to force enable tool search even under the 10% threshold, for consistency or testing purposes.

**Caching strategies**: Implement LRU caching for recently used tools to minimize reload overhead.

**Progressive loading**: Load commonly-used tools upfront while keeping long-tail tools in search-only mode.

**Programmatic composition**: Future direction allowing tools to be composed with each other via code (experimented with but deferred for tool search priority).

## Pitfalls

* **Poor server instructions**: Without clear guidance on when to search, agents may miss relevant tools
* **Inadequate search**: Basic RAG may not capture tool capabilities effectively; agentic search is preferred
* **Metadata neglect**: Failing to maintain rich, searchable tool descriptions defeats the purpose
* **Threshold tuning**: Setting the context threshold too low or too high can negate benefits or create unnecessary overhead

---

## Tool Use Incentivization via Reward Shaping

**Status:** emerging
**Category:** Feedback Loops
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.youtube.com/watch?v=Xkwok_XXQgw

## Problem

Coding agents often underutilize specialized tools (e.g., compilers, linters, test runners) when left to optimize only for final task success. They default to "thinking" tokens—generating internal chain-of-thought—instead of invoking external tools, which can slow down development and lead to suboptimal code outputs.

- Models like R1 "use their think tokens" almost exclusively rather than calling tools unless explicitly rewarded for tool use.
- Without intermediate incentives, the agent has no incentive to write code, compile, or run tests until the very end.

## Solution

Provide **dense, shaped rewards** for every intermediate tool invocation that contributes toward final code correctness. Key components:

**1. Define Tool-Specific Reward Signals**
- **Compile Reward:** +1 if code compiles without errors.
- **Lint Reward:** +0.5 if linter returns zero issues.
- **Test Reward:** +2 if test suite passes a new test case.
- **Documentation Reward:** +0.2 for adding or correcting docstrings.

**2. Episode-Level Aggregation**
- Sum intermediate rewards to form a cumulative "coding progress" score.
- Combine with final reward (e.g., full test suite pass or PR merge) to guide policy updates.

**3. Policy Update Mechanism**
- Use Proximal Policy Optimization (PPO) or Advantage Actor-Critic (A2C) with these shaped rewards.
- During each RL rollout, track `(state, action, tool_result, local_reward)` tuples.

```python
# Pseudo-code: at each RL step, after tool call:
if action == "compile":
    local_reward = 1 if compile_success else -0.5
elif action == "run_tests":
    local_reward = 2 if new_tests_passed else 0
# ... other tool rewards ...
trajectory.append((state, action, tool_output, local_reward))
```

## How to use it

- **Instrumentation:** Wrap tool calls (e.g., `compile()`, `run_linter()`, `pytest`) with functions that return a binary or graded success signal.
- **Hyperparameter Tuning:** Adjust reward magnitudes so that the agent does not "overfit" to one tool (e.g., getting lint rewards repeatedly without actual functionality).
- **Curriculum Design:** Start with simpler tasks (e.g., "fix one failing test") to collect early positive signals and gradually scale to multi-file refactors.

## Trade-offs

- **Pros:**
  - **Denser Feedback:** Guides the agent step by step, reducing reliance on sparse, final success signals.
  - **Tool Adoption:** Encourages the agent to learn how and when to invoke compilers and test runners.
- **Cons/Considerations:**
  - **Reward Engineering Overhead:** Requires careful design and maintenance of reward functions for each tool.
  - **Potential Overfitting:** The agent may game intermediate rewards (e.g., repeatedly running lint without changing code).

## References

- Will Brown's discussion on how "if you set these models up to use tools, they just won't" unless incentivized.
- Concepts from "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment" (Prime Intellect paper previewed in talk).

---

## Tool Use Steering via Prompting

**Status:** best-practice
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/claude-code

## Problem
AI agents equipped with multiple tools (e.g., shell access, file system operations, web search, custom CLIs) need clear guidance on when, why, and how to use these tools effectively. Simply having tools available doesn't guarantee they will be used appropriately for the task at hand, especially for tools unfamiliar to the base model or specific to a team's workflow.

## Solution
Guide the agent's tool selection and execution through explicit natural language instructions within the prompt. This includes:

-   **Direct Tool Invocation:** Telling the agent which tool to use for a specific part of a task (e.g., "Use the file search tool to find...", "Run a bash command to...").
-   **Teaching Tool Usage:** Instructing the agent on how to learn about or use a new or custom tool, including how to discover its options (e.g., "Use our `barley` CLI to check logs. You can use `-h` to see how to use it.").
-   **Implicit Tool Suggestion:** Using phrases or shorthands that the agent learns to associate with specific tool sequences (e.g., "commit, push, pr" for a Git workflow).
-   **Encouraging Deeper Reasoning for Tool Use:** Adding phrases like "*think hard*" to prompt more careful consideration before acting, potentially leading to better tool choices or sequences.

This pattern emphasizes the user's role in actively shaping the agent's behavior with respect to its available tools, rather than relying solely on autonomous tool selection.

## Example (tool guidance flow)
```mermaid
flowchart TD
    A[User Task] --> B[Available Tools]
    A --> C[Explicit Guidance]
    C --> D[Direct Tool Invocation]
    C --> E[Teaching Tool Usage]
    C --> F[Implicit Tool Suggestion]
    C --> G[Deeper Reasoning Prompts]

    B --> H[Agent Tool Selection]
    D --> H
    E --> H
    F --> H
    G --> H

    H --> I[Tool Execution]
    I --> J[Task Completion]
```

## References
- Based on examples and tips in "Mastering Claude Code: Boris Cherny's Guide & Cheatsheet," section III, particularly "Steering Claude to Use Tools" and "Tip #3: Teach Claude to use *your* team's tools."

[Source](https://www.nibzard.com/claude-code)

---

## Tree-of-Thought Reasoning

**Status:** established
**Category:** Orchestration & Control
**Authors:** Nikola Balic (@nibzard)
**Source:** https://arxiv.org/abs/2305.10601

## Problem
Linear chain-of-thought reasoning can get stuck on complex problems, missing alternative approaches or failing to backtrack.

## Solution
Explore a search tree of intermediate "thoughts" instead of a single chain. The agent expands multiple possible steps and evaluates partial solutions before committing to a path.

```pseudo
queue = [root_problem]
while queue:
    thought = queue.pop()
    for step in expand(thought):
        score = evaluate(step)
        queue.push((score, step))
select_best(queue)
```

## How to use it
Apply when tasks benefit from exploring many potential strategies—puzzles, code generation, or planning. Use heuristics or a value function to prune unpromising branches.

## Trade-offs
* **Pros:** Covers more possibilities; improves reliability on hard tasks.
* **Cons:** Higher compute cost; needs a good scoring method to guide the search.

## References
* [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)

---

## Variance-Based RL Sample Selection

**Status:** emerging
**Category:** Learning & Adaptation
**Authors:** Nikola Balic (@nibzard)
**Source:** https://youtu.be/1s_7RMG4O4U

## Problem

Not all training samples are equally valuable for reinforcement learning:

- **Zero-variance samples**: Model gets same score every time (always correct or always wrong) → no learning signal
- **Wasted compute**: Training on samples where the model has no uncertainty wastes expensive RL exploration
- **Poor data utilization**: With limited training budgets, you want to maximize learning from each sample
- **Unclear training potential**: Hard to know if your dataset will support effective RL training

When Theo ran baseline evaluations on the FinQA benchmark, he discovered that ~85% of samples had zero variance (model always got them right or always wrong), meaning only ~15% of samples could actually contribute to learning.

## Solution

**Run multiple baseline evaluations per sample to identify variance, then prioritize high-variance samples for training.**

**The Variance Plot Methodology:**

1. **Baseline Evaluation**: Run your base model 3-5 times on each sample
2. **Visualize Variance**: Plot results to identify which samples have variance
3. **Categorize Samples**:
   
- **Always correct** (variance = 0): Model already knows this
   - **Always incorrect** (variance = 0): Model can't learn this (too hard or needs different approach)
   - **Sometimes correct** (variance > 0): **Prime candidates for RL**
4. **Focus Training**: Prioritize or exclusively use high-variance samples

**Understanding the Variance Plot:**

```
Score
1.0 ●━━━━━━━●━━━━━━●━━━━━━━●    ← Always correct (no learning)
    ┃       ┃      ┃       ┃
0.5 ┃   ●━━━●━━━●  ┃   ●━━━●━━━●    ← High variance (learn here!)
    ┃   ┃   ▼       ┃   ┃
0.0 ●━━━●━━━━━━●━━━●━━━━━━●━━━━━━●    ← Always wrong (no learning)
    └───┴───┴───┴───┴───┴───┴───→
        Sample Index

    ● = Best score (red cross in plots)
    ━ = Mean score (thick blue bar)
    ▼ = Variance range (thin blue bar)
```

**Implementation:**

```python
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

class VarianceAnalyzer:
    """
    Analyze baseline variance to identify high-value training samples
    """
    def __init__(self, agent, dataset, n_runs=3):
        self.agent = agent
        self.dataset = dataset
        self.n_runs = n_runs
        self.results = defaultdict(list)

    def run_baseline_evals(self):
        """
        Run agent multiple times on each sample
        """
        print(f"Running {self.n_runs} evaluations per sample...")

        for sample_idx, sample in enumerate(self.dataset):
            for run_idx in range(self.n_runs):
                score = self.agent.evaluate(sample)
                self.results[sample_idx].append(score)

            if sample_idx % 10 == 0:
                print(f"Completed {sample_idx}/{len(self.dataset)} samples")

        return self.results

    def compute_variance_metrics(self):
        """
        Calculate variance statistics for each sample
        """
        metrics = []

        for sample_idx in sorted(self.results.keys()):
            scores = self.results[sample_idx]

            metrics.append({
                'sample_idx': sample_idx,
                'mean_score': np.mean(scores),
                'best_score': np.max(scores),
                'worst_score': np.min(scores),
                'variance': np.var(scores),
                'std_dev': np.std(scores),
                'scores': scores
            })

        return metrics

    def plot_variance(self, metrics, title="Baseline Variance Analysis"):
        """
        Create variance visualization (like Theo's plots)
        """
        sample_indices = [m['sample_idx'] for m in metrics]
        mean_scores = [m['mean_score'] for m in metrics]
        best_scores = [m['best_score'] for m in metrics]
        std_devs = [m['std_dev'] for m in metrics]

        plt.figure(figsize=(14, 6))

        # Plot mean scores with error bars (variance)
        plt.errorbar(
            sample_indices,
            mean_scores,
            yerr=std_devs,
            fmt='o',
            linewidth=2,
            markersize=3,
            label='Mean ± Std Dev',
            color='cornflowerblue',
            elinewidth=1
        )

        # Overlay best scores
        plt.scatter(
            sample_indices,
            best_scores,
            marker='x',
            s=50,
            color='red',
            label='Best Score',
            alpha=0.7
        )

        plt.xlabel('Sample Index')
        plt.ylabel('Score')
        plt.title(title)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        return plt

    def identify_high_variance_samples(self, metrics, variance_threshold=0.01):
        """
        Filter samples with meaningful variance
        """
        high_variance = [
            m for m in metrics
            if m['variance'] > variance_threshold
            and 0 < m['mean_score'] < 1.0  # Not always right or wrong
        ]

        print(f"\nVariance Analysis:")
        print(f"  Total samples: {len(metrics)}")
        print(f"  High variance samples: {len(high_variance)} "
              f"({100*len(high_variance)/len(metrics):.1f}%)")
        print(f"  Always correct: {sum(1 for m in metrics if m['best_score'] == 1.0 and m['variance'] == 0)}")
        print(f"  Always incorrect: {sum(1 for m in metrics if m['best_score'] == 0.0)}")

        return high_variance

    def compute_improvement_potential(self, metrics):
        """
        Calculate how much performance could improve if model
        always achieves best-of-N performance
        """
        current_avg = np.mean([m['mean_score'] for m in metrics])
        best_of_n_avg = np.mean([m['best_score'] for m in metrics])

        potential_gain = best_of_n_avg - current_avg

        print(f"\nImprovement Potential:")
        print(f"  Current average: {current_avg:.3f}")
        print(f"  Best-of-{self.n_runs} average: {best_of_n_avg:.3f}")
        print(f"  Potential gain: {potential_gain:.3f} "
              f"({100*potential_gain/current_avg:.1f}% relative improvement)")

        return {
            'current': current_avg,
            'best_of_n': best_of_n_avg,
            'potential_gain': potential_gain
        }


# Usage example
analyzer = VarianceAnalyzer(
    agent=my_agent,
    dataset=validation_set,
    n_runs=3
)

# Run baseline evaluations
results = analyzer.run_baseline_evals()

# Analyze variance
metrics = analyzer.compute_variance_metrics()

# Visualize
analyzer.plot_variance(metrics)

# Identify high-value samples
high_var_samples = analyzer.identify_high_variance_samples(metrics)

# Calculate improvement potential
potential = analyzer.compute_improvement_potential(metrics)

# Use high-variance samples for training
training_data = [
    dataset[m['sample_idx']]
    for m in high_var_samples
]
```

## How to use it

**Step 1: Baseline Evaluation (Before Training)**

Run your base model 3-5 times on each sample in your training and validation sets:

```python
# Run multiple times per sample
for sample in dataset:
    for run in range(3):
        score = agent.evaluate(sample)
        record_score(sample.id, score)
```

**Step 2: Create Variance Plot**

Visualize to understand your data:

- **X-axis**: Sample index
- **Y-axis**: Score (0-1)
- **Red crosses**: Best score achieved across runs
- **Blue bars**: Mean score (thick) and variance (thin)

**Step 3: Interpret Results**

Good indicators for RL:

- **15-30% high variance samples**: Enough learning opportunities
- **Best-of-N >> Mean**: Model has potential to improve with RL
- **Variance distributed across dataset**: Not concentrated in few samples

Warning signs:

- **<10% high variance**: Dataset may be too easy or too hard
- **Best-of-N ≈ Mean**: Model is very consistent (low improvement potential)
- **All variance in tail**: Most samples don't offer learning signal

**Step 4: Set Compute Multiplier**

The compute multiplier controls exploration during training:

- **Low variance (10-15%)**: Use compute multiplier 2-4 for more exploration
- **Medium variance (15-30%)**: Use compute multiplier 1-2
- **High variance (>30%)**: Compute multiplier 1 may suffice

**Step 5: Monitor During Training**

Track how variance evolves:

- Early training: Variance should decrease as model learns
- Plateau: Variance may increase as model explores new strategies
- Convergence: Variance should stabilize at lower level

## Real-World Example: FinQA Benchmark

**Task**: Answer financial questions using tool-based search (not given context)

**Baseline Analysis:**

- Dataset: 100 validation samples, 1000 training samples
- Runs per sample: 3
- Base model: GPT-4o

**Results:**

```
Variance Analysis:
  Total samples: 100
  High variance samples: 15 (15%)
  Always correct: 40 samples
  Always incorrect: 45 samples

Improvement Potential:
  Current average: 0.59
  Best-of-3 average: 0.73
  Potential gain: +0.14 (24% relative improvement)
```

**Interpretation:**

- Only 15% of samples had variance → training will focus learning on those
- 24% potential improvement if model learns to consistently hit best-of-3 performance
- Good candidate for RL despite low variance percentage (quality over quantity)

**Training Results:**

After 10 steps of agent RFT:

- Validation reward: 0.59 → 0.63 (+7%)
- Tool calls per rollout: 6.9 → 4.2 (-39%)
- Latency: ~10% reduction

The model improved toward the best-of-3 ceiling while also becoming more efficient.

## Trade-offs

**Pros:**

- **Data efficiency**: Focus training on samples that actually contribute to learning
- **Predictive**: Estimate improvement potential before expensive training
- **Diagnostic**: Understand if your task is suitable for RL
- **Guides hyperparameters**: Informs compute multiplier and training duration decisions

**Cons:**

- **Upfront cost**: Requires 3-5x baseline evaluations before training
- **Small samples**: With few samples (<50), variance estimates may be noisy
- **Doesn't guarantee success**: High variance is necessary but not sufficient
- **Dynamic variance**: Variance changes during training, so initial analysis may not hold

## References

- [OpenAI Build Hour: Agent RFT - Variance Analysis Demo (November 2025)](https://youtu.be/1s_7RMG4O4U)
- [Prior RFT Build Hour with Prashant](https://www.youtube.com/openai-build-hours)
- Related patterns: Agent Reinforcement Fine-Tuning, Inference-Time Scaling

---

## Verbose Reasoning Transparency

**Status:** best-practice
**Category:** UX & Collaboration
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/claude-code

## Problem
AI agents, especially those using complex models or multiple tools, can sometimes behave like "black boxes." Users may not understand why an agent made a particular decision, chose a specific tool, or generated a certain output. This lack of transparency can hinder debugging, trust, and the ability to effectively guide the agent.

## Solution
Implement a feature that allows users to inspect the agent's internal "thought process" or reasoning steps on demand. This could be triggered by a keybinding (e.g., `Ctrl+R` in Claude Code) or a command.

When activated, the verbose output might reveal:

-   The agent's interpretation of the user's prompt.
-   Alternative actions or tools it considered.
-   The specific tool(s) it selected and why (if available).
-   Intermediate steps or sub-tasks it performed.
-   Confidence scores or internal states.
-   Raw outputs from tools before they are processed or summarized.

This transparency helps users understand the agent's decision-making process, identify issues if the agent is stuck or producing incorrect results, and learn how to prompt more effectively.

## Example (transparency activation)
```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant UI as Interface

    User->>Agent: Complex task request
    Agent->>Agent: Process internally
    Agent-->>User: Standard output

    User->>UI: Ctrl+R (or verbose command)
    UI->>Agent: Request verbose details
    Agent-->>UI: Internal reasoning steps
    Agent-->>UI: Tool selection rationale
    Agent-->>UI: Confidence scores
    Agent-->>UI: Raw tool outputs
    UI-->>User: Detailed transparency view
```

## References
-   Based on the `Ctrl+R` keybinding for showing verbose output in "Mastering Claude Code: Boris Cherny's Guide & Cheatsheet," section V.

[Source](https://www.nibzard.com/claude-code)

---

## Versioned Constitution Governance

**Status:** emerging
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://substack.com/home/post/p-161422949?utm_campaign=post&utm_medium=web

## Problem
When an agent rewrites its own "constitution," it may accidentally violate safety or regress on alignment objectives if changes aren't reviewed.

## Solution
Store the constitution in a **version-controlled, signed repository**:

- YAML/TOML rules live in Git.  
- Each commit is signed (e.g., Sigstore); CI runs automated policy checks.  
- Only commits signed by approved reviewers or automated tests are merged.  
- The agent can *propose* changes, but a gatekeeper merges them.

## How to use it
- Require `git commit -S` or similar.  
- Run diff-based linting to flag deletions of critical rules.  
- Expose constitution `HEAD` as read-only context in every agent episode.

## References
- Hiveism, *Self-Alignment by Constitutional AI*
- Anthropic, *Constitutional AI* white-paper

---

## Virtual Machine Operator Agent

**Status:** established
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://www.nibzard.com/silent-revolution

## Problem
AI agents need to perform complex tasks beyond simple code generation or text manipulation. They require the ability to interact with a full computer environment to execute code, manage system resources, install software, and operate various applications.

## Solution
Equip the AI agent with access to a dedicated virtual machine (VM) environment. The agent is trained or designed to understand how to operate within this VM, treating it as its direct workspace. This allows the agent to:

- Execute arbitrary code and scripts.
- Install and manage software packages.
- Read from and write to the file system.
- Utilize other command-line tools and applications available within the VM.

This pattern transforms the agent from a specialized tool into a more general-purpose digital operator.

## Example (flow)
```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant VM as Virtual Machine

    User->>Agent: Complex Task Request
    Agent->>VM: Execute Code/Scripts
    Agent->>VM: Install Packages
    Agent->>VM: File System Operations
    Agent->>VM: Use CLI Tools/Apps
    VM-->>Agent: Execution Results
    Agent->>Agent: Process & Analyze Results
    Agent-->>User: Task Completion Report
```

## References
- Based on Amjad Masad's description of advanced computer use agents: "People think of computer use as something like an operator, but actually it is more like you give the model a virtual machine, and it knows how to execute code on it, install packages, write scripts, use apps, do as much as possible with the computer." (Quote from the "How AI Agents Are Reshaping Creation" blog post).

[Source](https://www.nibzard.com/silent-revolution)

---

## Visual AI Multimodal Integration

**Status:** emerging
**Category:** Tool Use & Environment
**Authors:** Nikola Balic (@nibzard)
**Source:** https://openai.com/research/gpt-4v-system-card

## Problem

Many real-world tasks require understanding and processing visual information alongside text. Traditional text-only agents miss critical information present in images, videos, diagrams, and visual interfaces. This limitation prevents agents from helping with tasks like analyzing screenshots, debugging UI issues, understanding charts, processing security footage, or working with visual documentation.

## Solution

Integrate large multimodal models (LMMs) into agent architectures to enable visual understanding capabilities. This pattern involves:

1. **Visual Input Handling**: Accept images, videos, or screenshots as input alongside text
2. **Visual Analysis**: Use multimodal models to extract information, identify objects, read text, understand spatial relationships
3. **Cross-Modal Reasoning**: Combine visual and textual information for comprehensive understanding
4. **Visual-Guided Actions**: Take actions based on visual understanding (clicking UI elements, describing scenes, counting objects)

The integration can be implemented through specialized visual processing agents or by upgrading existing agents with multimodal capabilities.

## Example

```python
class VisualAIAgent:
    def __init__(self, multimodal_llm, text_llm=None):
        self.mm_llm = multimodal_llm
        self.text_llm = text_llm or multimodal_llm
        
    async def process_visual_task(self, task_description, visual_inputs):
        # Analyze each visual input
        visual_analyses = []
        for visual in visual_inputs:
            analysis = await self.analyze_visual(visual, task_description)
            visual_analyses.append(analysis)
        
        # Combine visual analyses with task
        combined_context = self.merge_visual_context(
            task_description, 
            visual_analyses
        )
        
        # Generate solution using combined understanding
        return await self.solve_with_visual_context(combined_context)
    
    async def analyze_visual(self, visual_input, context):
        prompt = f"""
        Task context: {context}
        
        Analyze this {visual_input.type} and extract:
        1. Relevant objects and their positions
        2. Any text present (OCR)
        3. Colors, patterns, or visual indicators
        4. Spatial relationships
        5. Anything relevant to the task
        
        Provide structured analysis:
        """
        
        return await self.mm_llm.analyze(
            prompt=prompt,
            image=visual_input.data
        )
    
    async def solve_with_visual_context(self, context):
        return await self.text_llm.generate(f"""
        Based on the visual analysis and task requirements:
        {context}
        
        Provide a comprehensive solution that incorporates 
        the visual information.
        """)

# Specialized visual agents for specific domains
class UIDebugAgent(VisualAIAgent):
    async def debug_ui_issue(self, screenshot, issue_description):
        ui_analysis = await self.analyze_visual(
            screenshot, 
            f"UI debugging: {issue_description}"
        )
        
        return await self.mm_llm.generate(f"""
        UI Analysis: {ui_analysis}
        Issue: {issue_description}
        
        Identify:
        1. Potential UI problems visible in the screenshot
        2. Specific elements that might cause the issue
        3. Recommendations for fixes
        4. Exact coordinates or selectors if applicable
        """)

class VideoAnalysisAgent(VisualAIAgent):
    async def analyze_video_segment(self, video_path, query):
        # Process video in chunks
        key_frames = await self.extract_key_frames(video_path)
        
        frame_analyses = []
        for frame in key_frames:
            analysis = await self.analyze_visual(frame, query)
            frame_analyses.append({
                'timestamp': frame.timestamp,
                'analysis': analysis
            })
        
        # Temporal reasoning across frames
        return await self.temporal_reasoning(frame_analyses, query)
```

```mermaid
flowchart TD
    A[User Query + Visual Input] --> B{Input Type}
    
    B -->|Image| C[Image Analysis]
    B -->|Video| D[Video Processing]
    B -->|Screenshot| E[UI Analysis]
    
    C --> F[Object Detection]
    C --> G[OCR/Text Extraction]
    C --> H[Spatial Understanding]
    
    D --> I[Key Frame Extraction]
    I --> J[Frame-by-Frame Analysis]
    J --> K[Temporal Reasoning]
    
    E --> L[Element Identification]
    E --> M[Layout Analysis]
    
    F --> N[Multimodal Integration]
    G --> N
    H --> N
    K --> N
    L --> N
    M --> N
    
    N --> O[Combined Understanding]
    O --> P[Task Solution]
    
    style B fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style N fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
```

## Use Cases

- **UI/UX Debugging**: Analyze screenshots to identify visual bugs or usability issues
- **Document Processing**: Extract information from charts, diagrams, and visual documents
- **Video Analysis**: Count objects, identify events, or generate timestamps in videos
- **Security Monitoring**: Analyze security footage for specific activities or anomalies
- **Medical Imaging**: Assist in analyzing medical images (with appropriate disclaimers)
- **E-commerce**: Analyze product images for categorization or quality control

## Trade-offs

**Pros:**
- Enables entirely new categories of tasks
- More natural interaction (users can show rather than describe)
- Better accuracy for visual tasks
- Can handle complex multimodal reasoning

**Cons:**
- Higher computational costs for visual processing
- Larger model requirements
- Potential privacy concerns with visual data
- May require specialized infrastructure for video processing
- Quality depends on visual model capabilities

## References

- [Andrew Ng on Visual AI and Agentic Workflows (2024)](https://www.deeplearning.ai/the-batch/)
- [GPT-4V(ision) System Card](https://openai.com/research/gpt-4v-system-card)
- [Claude 3 Vision Capabilities](https://www.anthropic.com/claude)
- [Google Gemini Multimodal Features](https://deepmind.google/technologies/gemini/)

---

## Workflow Evals with Mocked Tools

**Status:** emerging
**Category:** Reliability & Eval
**Authors:** Nikola Balic (@nibzard)
**Source:** https://lethain.com/agents-evals/

## Problem

Unit tests, linters, and typecheckers validate individual components but don't test agent workflows end-to-end. It's easy to create prompts that don't work well despite all underlying pieces being correct.

You need to validate that prompts and tools work together effectively as a system.

## Solution

Implement **workflow evals (simulations)** that test complete agent workflows with mocked tools.

**Core components (Sierra-inspired approach):**

1. **Dual tool implementations**: Every tool has both `true` and `mock` versions

   ```python
   # True implementation - calls real APIs
   def search_knowledge_base_true(query: str) -> str:
       return kb_api.search(query)

   # Mock implementation - returns static/test data
   def search_knowledge_base_mock(query: str) -> str:
       return TEST_KB_RESULTS.get(query, DEFAULT_RESULT)
   ```

2. **Simulation configuration**: Each eval defines:
   - **Initial prompt**: What the agent receives
   - **Metadata**: Situation context available to harness
   - **Evaluation criteria**: Success/failure determination

   ```yaml
   evals:
     - name: slack_reaction_jira_workflow
       initial_prompt: "Add a smiley reaction to the JIRA ticket in this Slack message"
       metadata:
         situation: "slack_message_with_jira_link"
       expected_tools:
         - slack_get_message
         - jira_get_ticket
         - slack_add_reaction
       evaluation_criteria:
         objective:
           - tools_called: ["slack_get_message", "jira_get_ticket", "slack_add_reaction"]
           - tools_not_called: ["slack_send_message"]
         subjective:
           - agent_judge: "Response was helpful and accurate"
   ```

3. **Dual evaluation criteria**:

   **Objective criteria**:
   - Which tools were called
   - Which tools were NOT called
   - Tags/states added to conversation (if applicable)

   **Subjective criteria**:
   - Agent-as-judge assessments (e.g., "Was response friendly?")
   - LLM evaluations of qualitative outcomes

4. **CI/CD integration**: Run evals automatically on every PR

   ```pseudo
   # GitHub Actions workflow
   on: pull_request
   steps:
     - run: python scripts/run_agent_evals.py
       # Posts results as PR comment
   ```

```pseudo
# Eval execution flow
1. Load eval configuration
2. Swap in mock implementations for all tools
3. Run agent with initial prompt + metadata
4. Track which tools agent calls
5. Evaluate against objective criteria (tool usage)
6. Run agent-as-judge for subjective criteria
7. Report pass/fail with details
```

## How to use it

**Best for:**

- Agent workflows where tools have side effects (APIs, databases)
- CI/CD pipelines requiring workflow validation
- Prompt engineering and optimization
- Regression testing for agent behavior changes

**Implementation approach:**

**1. Create mock layer for tools:**

```python
class MockToolRegistry:
    def __init__(self, mode: str = "mock"):
        self.mode = mode

    def get_tool(self, tool_name: str):
        if self.mode == "mock":
            return self.mocks[tool_name]
        return self.real_tools[tool_name]

    # Register mock implementations
    mocks = {
        "slack_send_message": mock_slack_send_message,
        "jira_create_ticket": mock_jira_create_ticket,
        # ...
    }
```

**2. Define eval cases:**

```python
evals = [
    {
        "name": "login_support_flow",
        "prompt": "User can't log in, help them",
        "expected_tools": ["user_lookup", "password_reset"],
        "forbidden_tools": ["account_delete"],
        "subjective_criteria": "Response was empathetic and helpful"
    },
    # ... more evals
]
```

**3. Run and evaluate:**

```python
def run_eval(eval_config):
    # Run agent with mocked tools
    result = agent.run(
        prompt=eval_config["prompt"],
        tools=mock_registry
    )

    # Check objective criteria
    tools_called = result.tools_used
    passed = all(t in tools_called for t in eval_config["expected_tools"])
    passed &= all(t not in tools_called for t in eval_config["forbidden_tools"])

    # Check subjective criteria
    if passed:
        judge_prompt = f"""
        Evaluate this agent response: {result.response}
        Criteria: {eval_config['subjective_criteria']}
        Pass/fail?
        """
        passed = llm_evaluator(judge_prompt) == "PASS"

    return {"passed": passed, "details": result}
```

**4. Integrate with CI/CD:**

```yaml
# .github/workflows/agent_evals.yml
name: Agent Evals
on: pull_request
jobs:
  evals:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: python scripts/run_evals.py --format github
      - uses: actions/github-script@v6
        with:
          script: |
            const results = require('./eval_results.json');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: formatResults(results)
            });
```

**Handling non-determinism:**

The article notes evals are "not nearly as well as I hoped" due to non-determinism:

- **Strong signal**: All pass or all fail
- **Weak signal**: Mixed results
- **Mitigation**: Retry failed evals (e.g., "at least once in three tries")

## Trade-offs

**Pros:**

- **End-to-end validation**: Tests prompts + tools together as a system
- **Fast feedback**: Catch regressions before they reach production
- **Safe testing**: Mocked tools avoid side effects during testing
- **Clear criteria**: Both objective (tool calls) and subjective (quality) measures
- **CI/CD integration**: Automated validation on every PR

**Cons:**

- **Non-deterministic**: LLM variability makes flaky tests common
- **Mock maintenance**: Need to keep mocks synced with real tool behavior
- **Prompt-driven fragility**: Prompt-dependent workflows (vs code-driven) more flaky
- **Not blocking-ready**: Hard to use as CI gate due to variability
- **Tuning overhead**: Need continuous adjustment of prompts and mock responses
- **Limited signal**: Mixed pass/fail results provide ambiguous guidance

**Operational challenges:**

> "This is working well, but not nearly as well as I had hoped... there's very strong signal when they all fail, and strong signal when they all pass, but most runs are in between."

> "Our reliance on prompt-driven workflows rather than code-driven workflows introduces a lot of non-determinism, which I don't have a way to solve without... prompt and mock tuning."

**Improvement strategies:**

1. **Retry logic**: "At least once in three tries" to reduce flakiness
2. **Tune prompts**: Make eval prompts more precise and deterministic
3. **Tune mocks**: Improve mock responses to be more realistic
4. **Code over prompts**: Move complex workflows from prompt-driven to code-driven
5. **Directional vs blocking**: Use for context rather than CI gates

## References

* [Building an internal agent: Evals to validate workflows](https://lethain.com/agents-evals/) - Will Larson (2025)
* Sierra platform: Simulations approach for agent testing
* Related: [Stop Hook Auto-Continue Pattern](stop-hook-auto-continue-pattern.md) - Post-execution testing
* Related: [Agent Reinforcement Fine-Tuning](agent-reinforcement-fine-tuning.md) - Training on agent workflows
