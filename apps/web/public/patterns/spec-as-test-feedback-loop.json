{
  "title": "Spec-As-Test Feedback Loop",
  "status": "proposed",
  "authors": [
    "Nikola Balic (@nibzard)"
  ],
  "based_on": [
    "Jory Pestorious"
  ],
  "category": "Feedback Loops",
  "source": "http://jorypestorious.com/blog/ai-engineer-spec/",
  "tags": [
    "validation",
    "drift-detection",
    "continuous-testing"
  ],
  "id": "spec-as-test-feedback-loop",
  "slug": "spec-as-test-feedback-loop",
  "updated_at": "2026-01-05",
  "body": "\n## Problem\nEven in spec-first projects, implementations can drift as code evolves and the spec changes (or vice-versa). Silent divergence erodes trust.\n\n## Solution\nGenerate **executable assertions** directly from the spec (e.g., unit or integration tests) and let the agent:\n\n- Watch for any spec or code commit.  \n- Auto-regenerate test suite from latest spec snapshot.  \n- Run tests; if failures appear, open an *agent-authored* PR that either:\n    \n- updates code to match spec, or\n    - flags unclear spec segments for human review.\n\nThis creates a continuous feedback loop ensuring specification and implementation remain synchronized.\n\n## Trade-offs\n- **Pros:** catches drift early, keeps spec & impl in lock-step.\n- **Cons:** heavy CI usage; false positives if spec wording is too loose.\n\n## References\n- Natural extension of the \"specification-driven development\" concept surfaced in the page metadata."
}