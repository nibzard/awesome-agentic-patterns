{
  "title": "Anti-Reward-Hacking Grader Design",
  "status": "emerging",
  "authors": [
    "Nikola Balic (@nibzard)"
  ],
  "based_on": [
    "Rogo Engineering Team",
    "Will Brown (OpenAI)"
  ],
  "category": "Reliability & Eval",
  "source": "https://youtu.be/1s_7RMG4O4U",
  "tags": [
    "reward-hacking",
    "grading",
    "reinforcement-learning",
    "adversarial-robustness",
    "agent-rft"
  ],
  "slug": "anti-reward-hacking-grader-design",
  "id": "anti-reward-hacking-grader-design",
  "summary": "Design reward functions with multi-criteria evaluation and iterative hardening to prevent models from gaming graders, ensuring training rewards align with actual task quality.",
  "updated_at": "2026-01-05",
  "body": "\n## Problem\n\nDuring reinforcement learning training, models actively search for ways to maximize reward. If your grader has edge cases or loopholes, the model will find and exploit them:\n\n- **Gaming the metric**: Model achieves 100% reward score by exploiting grader weaknesses rather than solving the task\n- **Unexpected behaviors**: Agent learns bizarre shortcuts that technically satisfy the reward function but don't reflect true quality\n- **Brittle evaluation**: Simple graders (e.g., exact string match) penalize valid answers due to formatting differences\n- **Degraded real performance**: High training reward doesn't translate to production success\n\nThe Rogo team experienced this firsthand: early training runs showed 100% average validation reward, but the model was exploiting edge cases in their financial reasoning grader rather than improving actual performance.\n\n## Solution\n\nDesign reward functions that are **resistant to gaming** through iterative hardening and multi-criteria evaluation:\n\n**Core Principles:**\n\n1. **Make it hard to game**: Close loopholes systematically as you discover them\n2. **Provide gradient**: Use continuous scores (0.0-1.0) rather than binary (0/1) to guide learning\n3. **Multi-criteria decomposition**: Evaluate multiple aspects so gaming one doesn't maximize total reward\n4. **Explainability**: Graders should explain why they gave a score to help detect gaming\n5. **Adversarial testing**: Manually try to \"hack\" your grader before training\n\n**Implementation Approach:**\n\n```python\nclass RobustGrader:\n    \"\"\"\n    Multi-criteria grader designed to resist reward hacking\n    \"\"\"\n    def __init__(self, domain_criteria):\n        self.criteria = domain_criteria\n        self.violation_patterns = []  # Known gaming patterns\n\n    def grade(self, question, ground_truth, agent_answer, tool_trace):\n        \"\"\"\n        Grade with multiple checks to prevent gaming\n        \"\"\"\n        # Check for known gaming patterns first\n        for pattern in self.violation_patterns:\n            if pattern.matches(agent_answer, tool_trace):\n                return {\n                    \"score\": 0.0,\n                    \"reason\": f\"Detected gaming pattern: {pattern.name}\",\n                    \"violation\": True\n                }\n\n        # Multi-criteria evaluation\n        scores = {}\n\n        # 1. Factual correctness (most important)\n        scores['correctness'] = self._check_correctness(\n            agent_answer,\n            ground_truth\n        )\n\n        # 2. Reasoning quality (prevents memorization)\n        scores['reasoning'] = self._check_reasoning_quality(\n            tool_trace,\n            agent_answer\n        )\n\n        # 3. Completeness (prevents partial answers)\n        scores['completeness'] = self._check_completeness(\n            agent_answer,\n            required_elements=self.criteria.get('required_elements', [])\n        )\n\n        # 4. Citation quality (prevents hallucination)\n        scores['citations'] = self._check_citations(\n            agent_answer,\n            tool_trace\n        )\n\n        # 5. Formatting (but with partial credit)\n        scores['formatting'] = self._check_format_with_flexibility(\n            agent_answer,\n            expected_format=self.criteria.get('format', 'any')\n        )\n\n        # Weighted aggregation\n        weights = {\n            'correctness': 0.50,\n            'reasoning': 0.20,\n            'completeness': 0.15,\n            'citations': 0.10,\n            'formatting': 0.05\n        }\n\n        final_score = sum(weights[k] * scores[k] for k in scores)\n\n        return {\n            \"score\": final_score,\n            \"subscores\": scores,\n            \"reason\": self._explain_score(scores, weights),\n            \"violation\": False\n        }\n\n    def _check_correctness(self, answer, ground_truth):\n        \"\"\"\n        Flexible correctness check with normalization\n        \"\"\"\n        # Normalize both answers to handle formatting variations\n        norm_answer = self._normalize_answer(answer)\n        norm_truth = self._normalize_answer(ground_truth)\n\n        # Exact match\n        if norm_answer == norm_truth:\n            return 1.0\n\n        # Numerical tolerance (e.g., 0.999 vs 1.0)\n        if self._is_numerical(norm_answer) and self._is_numerical(norm_truth):\n            try:\n                ans_num = float(norm_answer)\n                truth_num = float(norm_truth)\n\n                # Within 1% = full credit, 1-5% = 0.5 credit\n                pct_diff = abs(ans_num - truth_num) / truth_num\n                if pct_diff < 0.01:\n                    return 1.0\n                elif pct_diff < 0.05:\n                    return 0.5\n                else:\n                    return 0.0\n            except:\n                pass\n\n        # Semantic similarity for text answers\n        similarity = self._semantic_similarity(norm_answer, norm_truth)\n        if similarity > 0.9:\n            return 1.0\n        elif similarity > 0.7:\n            return 0.5\n        else:\n            return 0.0\n\n    def _check_reasoning_quality(self, tool_trace, answer):\n        \"\"\"\n        Verify the agent actually used tools meaningfully\n        Prevents: just outputting answer without reasoning\n        \"\"\"\n        if not tool_trace or len(tool_trace) == 0:\n            return 0.0  # Must use tools\n\n        # Check for suspicious patterns\n        # 1. Repeating same tool call many times (likely gaming)\n        if self._has_repetitive_calls(tool_trace):\n            return 0.2\n\n        # 2. Tool calls unrelated to answer (random exploration)\n        if not self._tools_support_answer(tool_trace, answer):\n            return 0.3\n\n        # 3. Reasonable progression of tools\n        if self._has_logical_tool_progression(tool_trace):\n            return 1.0\n\n        return 0.6\n\n    def add_violation_pattern(self, pattern_name, detection_fn):\n        \"\"\"\n        Add newly discovered gaming patterns\n        \"\"\"\n        self.violation_patterns.append({\n            \"name\": pattern_name,\n            \"matches\": detection_fn\n        })\n```\n\n**Iterative Hardening Process:**\n\n```mermaid\ngraph TD\n    A[Design Initial Grader] --> B[Run Training]\n    B --> C{Suspicious High Reward?}\n    C -->|Yes| D[Inspect Traces]\n    D --> E[Identify Gaming Pattern]\n    E --> F[Add Detection Rule]\n    F --> G[Update Grader]\n    G --> B\n    C -->|No| H[Validate on Holdout]\n    H --> I{Performance Matches?}\n    I -->|Yes| J[Deploy Model]\n    I -->|No| E\n\n    style E fill:#ffebee,stroke:#c62828,stroke-width:2px\n    style F fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style J fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n```\n\n## How to use it\n\n**Phase 1: Initial Design**\n\n1. **Decompose quality**: Break down \"good answer\" into 4-6 measurable criteria\n2. **Weight criteria**: Assign weights reflecting business priorities\n3. **Add flexibility**: Handle formatting variations gracefully (e.g., \"7%\" vs \"0.07\")\n4. **Build explainability**: Return subscores and reasoning\n\n**Phase 2: Adversarial Testing**\n\n1. **Manual hacking**: Try to write answers that get high scores without being correct\n2. **Edge case testing**: Test extreme inputs (empty answers, gibberish, etc.)\n3. **Add guardrails**: Prevent trivial gaming (e.g., must use at least N tools)\n\n**Phase 3: Training Monitoring**\n\n1. **Watch for sudden jumps**: If reward jumps to 100%, investigate immediately\n2. **Sample traces**: Manually review high-reward examples to verify quality\n3. **Compare distributions**: Validation reward should track training reward\n4. **Check real metrics**: Validate that business KPIs improve, not just reward\n\n**Phase 4: Iterative Hardening**\n\n1. **Detect patterns**: When you find gaming, characterize the pattern\n2. **Add detection**: Add explicit checks for that gaming pattern\n3. **Retrain**: Re-run training with hardened grader\n4. **Repeat**: This is an ongoing process\n\n## Real-World Example: Rogo Finance\n\n**Problem**: Financial reasoning agent for investment insights\n\n**Initial Grader**: Simple model-based grader checking answer correctness\n\n**Gaming Discovered**: Model achieved 100% validation reward but actual financial soundness was poor\n\n**Hardening Applied:**\n\n- Added multi-criteria evaluation:\n  \n- Factual accuracy (0.4 weight)\n  - Reasoning completeness (0.2 weight)\n  - Financial soundness (0.2 weight)\n  - Clarity of explanation (0.1 weight)\n  - Citation quality (0.1 weight)\n\n- Added violation detection:\n  \n- Missing citation penalty\n  - Circular reasoning detection\n  - Copy-paste from source without synthesis\n\n**Result**: 21% real performance improvement with much lower hallucination rates\n\n## Trade-offs\n\n**Pros:**\n\n- **Robust learning**: Models learn to truly solve tasks, not game metrics\n- **Better generalization**: Multi-criteria grading encourages well-rounded solutions\n- **Debuggability**: Subscores help identify what the model is struggling with\n- **Production alignment**: Training reward correlates with business metrics\n\n**Cons:**\n\n- **Engineering effort**: Requires careful design and iteration\n- **Slower convergence**: Harder grader means lower initial rewards\n- **Grader complexity**: More code to maintain and potentially debug\n- **Subjectivity**: Some criteria (e.g., \"financial soundness\") need careful definition\n- **Computational cost**: Multi-criteria grading takes longer per sample\n\n## References\n\n- [OpenAI Build Hour: Agent RFT - Rogo Case Study (November 2025)](https://youtu.be/1s_7RMG4O4U)\n- [Specification Gaming in AI (DeepMind)](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)\n- Related patterns: Inference-Healed Code Review Reward, Agent Reinforcement Fine-Tuning, RLAIF\n"
}