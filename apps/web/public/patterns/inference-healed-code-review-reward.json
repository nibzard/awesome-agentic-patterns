{
  "title": "Inference-Healed Code Review Reward",
  "status": "proposed",
  "authors": [
    "Nikola Balic (@nibzard)"
  ],
  "based_on": [
    "Anonymous Speaker (Open Source Agent RL Talk)",
    "Will Brown (Prime Intellect Talk)"
  ],
  "category": "Feedback Loops",
  "source": "https://www.youtube.com/watch?v=Xkwok_XXQgw",
  "tags": [
    "reward-modeling",
    "code-review",
    "inference-healing",
    "quality-assessment"
  ],
  "slug": "inference-healed-code-review-reward",
  "id": "inference-healed-code-review-reward",
  "summary": "TODO: Add a concise summary for \"Inference-Healed Code Review Reward\" describing the pattern's purpose and key benefits.",
  "updated_at": "2026-01-05",
  "body": "\n## Problem\n\nSimple reward functions that only check for \"all tests passed\" fail to capture nuanced code quality issues (e.g., performance regressions, style violations, missing edge-case handling). A single binary signal at the end cannot guide the agent to produce maintainable, high-quality code.\n\n- Verifying **only** final correctness misses suboptimal commits (e.g., a patch that removes error handling but still passes tests).\n- Reward models that produce a single scalar lack **explainability** to tell the agent which aspect of the code needs improvement.\n\n## Solution\n\nUse an **inference-healed reward model**—a code-review critic that:\n\n**1. Decomposes Code Quality into Subcriteria**\n- **Correctness:** Does the code pass all existing and newly added tests?\n- **Style:** Are linters (e.g., ESLint, pylint) satisfied (zero or minimal warnings)?\n- **Performance:** Are there clear performance regressions gauged by simple benchmarks?\n- **Security:** Does a static analyzer (e.g., Bandit, SonarQube) flag no critical issues?\n\n**2. Runs Internal Chain-of-Thought (CoT) Reasoning**\n- If uncertain about a subcriterion (e.g., performance), the critic runs a short CoT inside itself:\n  ```text\n  \"Step: performance check. Baseline runtime: 50ms. New code runtime: 65ms. \n  Regression > 20%. Score: 0.4.\"  \n  ```\n- This \"inference healing\" allows the reward model to **explain** each sub-score.\n\n**3. Aggregates Subscores**\n- Each subcriterion returns a float ∈ [0, 1].\n- A weighted sum (e.g., 0.4 × correctness + 0.2 × style + 0.2 × performance + 0.2 × security) yields the final code-review score.\n\n**4. Generates Human-Readable Feedback**\n- Alongside a numerical score, return a short analysis:\n  ```json\n  {\n    \"correctness\": 1.0,\n    \"style\": 0.8,\n    \"performance\": 0.4,\n    \"security\": 0.6,\n    \"comments\": \"Performance regression due to O(n²) loop.\"\n  }\n  ```\n\n## Example\n\n```python\n# Pseudo-code for one code-review reward invocation\nsubscores = {\n    \"correctness\": test_critic.score(patch),\n    \"style\": linter_critic.score(patch),\n    \"performance\": perf_critic.score(patch),\n    \"security\": security_critic.score(patch),\n}\nfinal_score = sum(weight[k]*subscores[k] for k in subscores)\nreturn final_score, subscores, comments\n```\n\n## How to use it\n\n- **Critic Dataset Collection:** Gather examples of good vs. bad code patches, labeled along each subcriterion.\n- **Critic Training:** Fine-tune a small LLM (e.g., 1–2 B parameters) to produce sub-scores and CoT justifications.\n- **Integration into RL Loop:** Replace or augment the existing binary \"tests-passed\" reward with `inference_healed_reward(patch)`.\n- **Human-in-the-Loop Checkpoints:** If a patch is borderline (e.g., final_score ∈ [0.5, 0.7]), route it for manual code review to generate better labels for future training.\n\n## Trade-offs\n\n- **Pros:**\n  - **Explainable Feedback:** The agent knows *why* a patch scored poorly, allowing targeted improvements.\n  - **Higher Code Quality:** Incorporates non-functional criteria (performance, security), leading to more robust code.\n- **Cons/Considerations:**\n  - **Compute Overhead:** Each reward invocation may involve running tests, linters, benchmarks, and a static analysis, adding latency.\n  - **Critic Maintenance:** As coding standards or security rules evolve, retrain or update the critic models and rubrics.\n\n## References\n\n- Derived from \"inference healing\" in reward modeling, as discussed in the Open Source Agent RL talk (May 2025) and by Will Brown (Prime Intellect).\n- Similar principles in \"Criterion-Led Reward Models\" (DeepMind blog, April 2025).\n"
}