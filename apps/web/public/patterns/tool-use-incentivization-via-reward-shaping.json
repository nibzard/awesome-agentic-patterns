{
  "title": "Tool Use Incentivization via Reward Shaping",
  "status": "emerging",
  "authors": [
    "Nikola Balic (@nibzard)"
  ],
  "based_on": [
    "Will Brown (Prime Intellect Talk)"
  ],
  "category": "Feedback Loops",
  "source": "https://www.youtube.com/watch?v=Xkwok_XXQgw",
  "tags": [
    "tool-use",
    "reward-shaping",
    "coding-agent",
    "RL"
  ],
  "id": "tool-use-incentivization-via-reward-shaping",
  "slug": "tool-use-incentivization-via-reward-shaping",
  "updated_at": "2026-01-05",
  "body": "\n## Problem\n\nCoding agents often underutilize specialized tools (e.g., compilers, linters, test runners) when left to optimize only for final task success. They default to \"thinking\" tokens—generating internal chain-of-thought—instead of invoking external tools, which can slow down development and lead to suboptimal code outputs.\n\n- Models like R1 \"use their think tokens\" almost exclusively rather than calling tools unless explicitly rewarded for tool use.\n- Without intermediate incentives, the agent has no incentive to write code, compile, or run tests until the very end.\n\n## Solution\n\nProvide **dense, shaped rewards** for every intermediate tool invocation that contributes toward final code correctness. Key components:\n\n**1. Define Tool-Specific Reward Signals**\n- **Compile Reward:** +1 if code compiles without errors.\n- **Lint Reward:** +0.5 if linter returns zero issues.\n- **Test Reward:** +2 if test suite passes a new test case.\n- **Documentation Reward:** +0.2 for adding or correcting docstrings.\n\n**2. Episode-Level Aggregation**\n- Sum intermediate rewards to form a cumulative \"coding progress\" score.\n- Combine with final reward (e.g., full test suite pass or PR merge) to guide policy updates.\n\n**3. Policy Update Mechanism**\n- Use Proximal Policy Optimization (PPO) or Advantage Actor-Critic (A2C) with these shaped rewards.\n- During each RL rollout, track `(state, action, tool_result, local_reward)` tuples.\n\n```python\n# Pseudo-code: at each RL step, after tool call:\nif action == \"compile\":\n    local_reward = 1 if compile_success else -0.5\nelif action == \"run_tests\":\n    local_reward = 2 if new_tests_passed else 0\n# ... other tool rewards ...\ntrajectory.append((state, action, tool_output, local_reward))\n```\n\n## How to use it\n\n- **Instrumentation:** Wrap tool calls (e.g., `compile()`, `run_linter()`, `pytest`) with functions that return a binary or graded success signal.\n- **Hyperparameter Tuning:** Adjust reward magnitudes so that the agent does not \"overfit\" to one tool (e.g., getting lint rewards repeatedly without actual functionality).\n- **Curriculum Design:** Start with simpler tasks (e.g., \"fix one failing test\") to collect early positive signals and gradually scale to multi-file refactors.\n\n## Trade-offs\n\n- **Pros:**\n  - **Denser Feedback:** Guides the agent step by step, reducing reliance on sparse, final success signals.\n  - **Tool Adoption:** Encourages the agent to learn how and when to invoke compilers and test runners.\n- **Cons/Considerations:**\n  - **Reward Engineering Overhead:** Requires careful design and maintenance of reward functions for each tool.\n  - **Potential Overfitting:** The agent may game intermediate rewards (e.g., repeatedly running lint without changing code).\n\n## References\n\n- Will Brown's discussion on how \"if you set these models up to use tools, they just won't\" unless incentivized.\n- Concepts from \"Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment\" (Prime Intellect paper previewed in talk)."
}