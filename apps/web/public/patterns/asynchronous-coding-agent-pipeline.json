{
  "title": "Asynchronous Coding Agent Pipeline",
  "status": "proposed",
  "authors": [
    "Nikola Balic (@nibzard)"
  ],
  "based_on": [
    "Will Brown (Prime Intellect Talk)"
  ],
  "category": "Reliability & Eval",
  "source": "https://www.youtube.com/watch?v=Xkwok_XXQgw",
  "tags": [
    "asynchronous",
    "pipeline",
    "code-agent",
    "parallelism"
  ],
  "id": "asynchronous-coding-agent-pipeline",
  "slug": "asynchronous-coding-agent-pipeline",
  "updated_at": "2026-01-05",
  "body": "\n## Problem\n\nSynchronous execution of coding tasks—where the agent must wait for compilation, testing, linting, or static analysis—creates **compute bubbles** and **idle resources**. When a coding agent issues a tool call (e.g., `run_tests()`), it blocks further reasoning until that tool returns, leading to underutilized GPUs/TPUs and slower RL rollouts.\n\n- RL agents must push hard on **async RL** \"so everything is happening in parallel without blowing up bubbles\".\n- For coding agents, each I/O-bound tool call (compilation, test runs) can take seconds to minutes.\n\n## Solution\n\nDecouple the **inference**, **tool execution**, and **learning** into **parallel, asynchronous components**, communicating via message queues:\n\n**1. Inference Workers (GPU)**\n- Continuously sample from the latest policy.\n- Output \"actions\" that are either low-compute (e.g., \"suggest next line\") or external tool calls (e.g., \"CompileSubagent(serviceA)\").\n\n**2. Tool Executors (CPU / Container Hosts)**\n- Listen to a queue of tool call requests (`compile`, `run_tests`, `lint`).\n- Run each tool in an isolated environment, then push the results (success/failure, logs) back to the **Inference Workers**.\n\n**3. Reward Modeling Units (GPU/CPU)**\n- Consume completed trajectories (series of `(state, action, tool_output)`), compute turn-level or final rewards (e.g., via `inference_healed_reward`).\n- Push `(trajectory_id, reward)` to the **Learner**.\n\n**4. Learner / Parameter Server (GPU)**\n- Periodically aggregates gradients from recent trajectories, updates policy weights, and publishes new checkpoints.\n\n**5. Replay & Buffer System**\n- **Experience Replay:** Stores recent `(state, action, reward)` tuples, allowing the Learner to sample minibatches.\n- **Priority Queues:** If certain coding episodes show high variance (e.g., intermittent compile successes), re-evaluate them with updated reward models.\n\n## Example\n\n```mermaid\nflowchart LR\n    subgraph InferenceCluster\n        A[Inference Worker] -->|\"Compile serviceA\"| B[Tool Queue]\n        B -->|request| C[CompileSubagent]\n        C -->|result (succ/fail)| A\n        A -->|trajectory data| D[Replay Buffer]\n    end\n    subgraph TrainingCluster\n        D -->|batch| E[Learner (Policy Update)]\n        E -->|new checkpoint| A\n    end\n    subgraph RewardCluster\n        F[RewardModel Worker] -->|consume trajectories| D\n    end\n```\n\n## How to use it\n\n- **Message Broker:** Use Redis streams or RabbitMQ topics to queue tool calls (`compile_requests`, `test_requests`).\n- **Autoscaling Policies:** Monitor queue lengths: if `compile_requests` > threshold, spin up additional `CompileSubagent` containers.\n- **Failure Handling:** If a tool executor crashes or a network error occurs, send a \"retry\" or \"skip\" message; mark that trajectory as \"stale\" if too many retries.\n- **Checkpoint Frequency:** Decide at what interval the Learner should publish new policy weights (e.g., every 1,000 episodes) to avoid excessive network traffic.\n\n## Trade-offs\n\n- **Pros:**\n  - **High Utilization:** GPUs remain busy running inference or learning while CPU-bound tasks run in parallel.\n  - **Scalable Compute:** Can independently scale inference, tool execution, and reward modeling.\n- **Cons/Considerations:**\n  - **Complex System Maintenance:** Requires robust monitoring, logging, and alerting across multiple services.\n  - **Staleness Management:** Policies may train on slightly outdated data; hyperparameters must account for acceptable staleness windows (e.g., 5–20 minutes).\n\n## References\n\n- Will Brown's emphasis on \"everything being async and overlapped\" to hide latencies in multi-hour RL tasks.\n- \"IMPALA: Scalable Distributed Deep-RL\" for a precedent in actor-learner pipelines."
}